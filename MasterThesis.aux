\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand{\transparent@use}[1]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{abbrvnat}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Abstract}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{andersonSpeciesProblemIris1936a,fisherUseMultipleMeasurements1936}
\citation{LargeElectronPositronCollider}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Introduction}{3}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:introduction}{{2}{3}{Introduction}{chapter.2}{}}
\citation{veenNeuralNetworkZoo2016}
\citation{abu-mostafaLearningData2012}
\citation{AdvancedTopicsMachine}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Machine Learning Theory}{5}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:ML_theory}{{3}{5}{Machine Learning Theory}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Statistical Learning Theory}{5}{section.3.1}}
\citation{vapnikPrinciplesRiskMinimization1991}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Supervised Learning}{6}{section.3.2}}
\newlabel{sec:ml:supervised_learning}{{3.2}{6}{Supervised Learning}{section.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Overview of the learning problem.}}{6}{section.3.2}}
\newlabel{fig:ml:learning_problem}{{3.1}{6}{Supervised Learning}{section.3.2}{}}
\newlabel{eq:L}{{3.1}{6}{Supervised Learning}{equation.3.2.1}{}}
\newlabel{eq:L_hat}{{3.2}{6}{Supervised Learning}{equation.3.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Generalization Bound}{7}{section.3.3}}
\newlabel{sec:generalization_bound}{{3.3}{7}{Generalization Bound}{section.3.3}{}}
\newlabel{eq:hoeffding_onesided_a}{{3.6}{7}{The one-sided Hoeffding's inequalities}{equation.3.3.6}{}}
\newlabel{eq:hoeffding_onesided_b}{{3.7}{7}{The one-sided Hoeffding's inequalities}{equation.3.3.7}{}}
\newlabel{lemma:hoeffding}{{3}{7}{The two-sided Hoeffding's inequality}{lemma.3}{}}
\newlabel{eq:hoeffding_inequality}{{3.8}{7}{The two-sided Hoeffding's inequality}{equation.3.3.8}{}}
\newlabel{eq:hoeffding_inequality_generalization_error}{{3.9}{8}{Generalization Bound}{equation.3.3.9}{}}
\newlabel{theorem:hoeffding_single}{{1}{8}{Hoeffding's inequality for a single hypothesis}{theorem.1}{}}
\newlabel{eq:hoeffding_inequality_generalization_error_delta}{{3.11}{8}{Hoeffding's inequality for a single hypothesis}{equation.3.3.11}{}}
\newlabel{eq:hoeffding_inequality_single_PAC}{{3.12}{8}{Generalization Bound}{equation.3.3.12}{}}
\newlabel{theorem:hoeffding_finite}{{2}{8}{Hoeffding's inequality for a finite set of hypotheses candidates}{theorem.2}{}}
\newlabel{eq:hoeffding_inequality_theorem_multiple}{{3.13}{8}{Hoeffding's inequality for a finite set of hypotheses candidates}{equation.3.3.13}{}}
\citation{abu-mostafaLearningData2012}
\newlabel{eq:hoeffding_inequality_multi_PAC}{{3.14}{9}{Generalization Bound}{equation.3.3.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Generalization Bound for infinite hypotheses}{9}{subsection.3.3.1}}
\newlabel{subsec:generalization_bound_infinite}{{3.3.1}{9}{Generalization Bound for infinite hypotheses}{subsection.3.3.1}{}}
\citation{tikhonovStabilityInverseProblems1943}
\newlabel{theorem:VC_generalization_bound}{{3}{10}{VC Generalization Bound}{theorem.3}{}}
\newlabel{eq:VC_bound}{{3.15}{10}{VC Generalization Bound}{equation.3.3.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Approximation-Estimation tradeoff}}{10}{equation.3.3.16}}
\newlabel{fig:ml:empirical_risk}{{3.2}{10}{Generalization Bound for infinite hypotheses}{equation.3.3.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Avoiding overfitting}{10}{section.3.4}}
\newlabel{sec:ml:overfitting}{{3.4}{10}{Avoiding overfitting}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Model Regularization}{10}{subsection.3.4.1}}
\newlabel{subsec:regularization}{{3.4.1}{10}{Model Regularization}{subsection.3.4.1}{}}
\citation{hastieElementsStatisticalLearning2009}
\newlabel{eq:l2_norm}{{3.20}{11}{Model Regularization}{equation.3.4.20}{}}
\newlabel{eq:l2_norm_linear}{{3.21}{11}{Model Regularization}{equation.3.4.21}{}}
\newlabel{eq:l2_norm_non_lagrangian}{{3.22}{11}{Model Regularization}{equation.3.4.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Regularization Effect}}{11}{equation.3.4.22}}
\newlabel{fig:ml:regularization_ridge}{{3.3}{11}{Model Regularization}{equation.3.4.22}{}}
\citation{tibshiraniRegressionShrinkageSelection1996}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Regularization Effect of $L_2$ }}{12}{equation.3.4.22}}
\newlabel{fig:ml:regularization_effect_ridge}{{3.4}{12}{Model Regularization}{equation.3.4.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Regularization Effect of $L_1$}}{12}{equation.3.4.22}}
\newlabel{fig:ml:regularization_effect_lasso}{{3.5}{12}{Model Regularization}{equation.3.4.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Cross Validation}{12}{subsection.3.4.2}}
\newlabel{subsec:cross_validation}{{3.4.2}{12}{Cross Validation}{subsection.3.4.2}{}}
\citation{hastieElementsStatisticalLearning2009}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces $k$-Fold Cross Validation}}{13}{subsection.3.4.2}}
\newlabel{fig:ml:cross_val_kfold}{{3.6}{13}{Cross Validation}{subsection.3.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces $k$-Fold Cross Validation for Time Series Data}}{13}{subsection.3.4.2}}
\newlabel{fig:ml:cross_val_kfold_time}{{3.7}{13}{Cross Validation}{subsection.3.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Early Stopping}{14}{subsection.3.4.3}}
\newlabel{subsec:early_stopping}{{3.4.3}{14}{Early Stopping}{subsection.3.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Loss functions}{14}{section.3.5}}
\newlabel{sec:ml:loss_function}{{3.5}{14}{Loss functions}{section.3.5}{}}
\citation{barronGeneralAdaptiveRobust2017}
\citation{barronGeneralAdaptiveRobust2017}
\citation{barronGeneralAdaptiveRobust2017}
\citation{AllstateClaimsSeverity}
\oddpage@label{1}{15}
\citation{hastieElementsStatisticalLearning2009}
\gincltex@bb{figures/decision_tree/decision_tree.tex}{0}{0}{167.85095}{97.0583}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Comparison of different objective functions.}}{16}{equation.3.5.27}}
\newlabel{fig:ml:objective_funcs}{{3.8}{16}{Loss functions}{equation.3.5.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Comparison of different objective functions zoom in.}}{16}{equation.3.5.27}}
\newlabel{fig:ml:objective_funcs_zoom}{{3.9}{16}{Loss functions}{equation.3.5.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Evaluation Function}{16}{subsection.3.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Decision Trees}{16}{section.3.6}}
\newlabel{sec:ml:decision_trees}{{3.6}{16}{Decision Trees}{section.3.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Decision Tree Cuts In Feature Space}}{16}{section.3.6}}
\newlabel{fig:ml:decision_tree_feature_space}{{3.10}{16}{Decision Trees}{section.3.6}{}}
\oddpage@label{2}{16}
\citation{hastieElementsStatisticalLearning2009}
\citation{breimanRandomForests2001}
\citation{hastieElementsStatisticalLearning2009}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Decision Tree}}{17}{section.3.6}}
\newlabel{fig:ml:decision_tree}{{3.11}{17}{Decision Trees}{section.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Ensembles of Decision Trees}{17}{subsection.3.6.1}}
\newlabel{subsec:ml:multiple_decision_trees}{{3.6.1}{17}{Ensembles of Decision Trees}{subsection.3.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Random Forests}{17}{subsubsection*.4}}
\newlabel{subsubsec:ml:random_forest}{{3.6.1}{17}{Random Forests}{subsubsection*.4}{}}
\citation{hastieElementsStatisticalLearning2009}
\citation{JSSv059i10}
\citation{chenXGBoostScalableTree2016}
\citation{DmlcXgboost}
\citation{HEPMeetsML}
\citation{hastieElementsStatisticalLearning2009}
\newlabel{eq:ml:variance_of_correlated_variables}{{3.30}{18}{Variance of average of correlated i.d. variables}{equation.3.6.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{Boosted Decision Trees}{18}{subsubsection*.5}}
\newlabel{subsubsec:ml:boosted_decision_trees}{{3.6.1}{18}{Boosted Decision Trees}{subsubsection*.5}{}}
\citation{freundDesiciontheoreticGeneralizationOnline1995}
\citation{chenXGBoostScalableTree2016}
\citation{keLightGBMHighlyEfficient2017}
\newlabel{eq:ml:boosted_decision_trees_residual}{{3.32}{19}{Boosted Decision Trees}{equation.3.6.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Hyperparamater Optimization}{19}{section.3.7}}
\newlabel{sec:ml:hyperparameter_optimization}{{3.7}{19}{Hyperparamater Optimization}{section.3.7}{}}
\citation{bergstraRandomSearchHyperparameter2012}
\citation{bergstraRandomSearchHyperparameter2012}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Grid Search}{20}{subsection.3.7.1}}
\newlabel{subsec:ml:grid_search}{{3.7.1}{20}{Grid Search}{subsection.3.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Grid Search}}{20}{subsection.3.7.1}}
\newlabel{fig:ml:gridsearch}{{3.12}{20}{Grid Search}{subsection.3.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Random Search}{20}{subsection.3.7.2}}
\newlabel{subsec:ml:random_search}{{3.7.2}{20}{Random Search}{subsection.3.7.2}{}}
\newlabel{eq:ml:random_search}{{3.37}{20}{Random Search}{equation.3.7.37}{}}
\citation{bergstraRandomSearchHyperparameter2012}
\citation{bergstraRandomSearchHyperparameter2012}
\citation{bergstraRandomSearchHyperparameter2012}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Random Search}}{21}{equation.3.7.38}}
\newlabel{fig:ml:random_search}{{3.13}{21}{Random Search}{equation.3.7.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.3}Bayesian Optimization}{21}{subsection.3.7.3}}
\newlabel{subsec:ml:bayesian_optimization}{{3.7.3}{21}{Bayesian Optimization}{subsection.3.7.3}{}}
\citation{brochuTutorialBayesianOptimization2010}
\citation{brochuTutorialBayesianOptimization2010}
\citation{brochuTutorialBayesianOptimization2010}
\citation{scikit-learn}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Bayesian Optimization}}{22}{subsection.3.7.3}}
\newlabel{fig:ml:bayesian_optimization}{{3.14}{22}{Bayesian Optimization}{subsection.3.7.3}{}}
\citation{Lundberg:2017}
\citation{Lundberg:2017}
\citation{lundbergConsistentIndividualizedFeature2019}
\citation{Shapley1953}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Feature Importance}{23}{section.3.8}}
\newlabel{sec:ml:feature_importance}{{3.8}{23}{Feature Importance}{section.3.8}{}}
\newlabel{eq:ml:additive_feature_attribution_method}{{3.40}{23}{Feature Importance}{equation.3.8.40}{}}
\citation{Lundberg:2017}
\citation{lundbergConsistentIndividualizedFeature2019}
\citation{lundbergConsistentIndividualizedFeature2019}
\newlabel{axiom:ml:shap_consistency}{{3}{24}{Consistency}{axiom.3}{}}
\newlabel{eq:ml:shap_feature_importance}{{3.43}{24}{Feature Importance}{equation.3.8.43}{}}
\newlabel{eq:ml:shap_feature_importance_simplification}{{3.44}{24}{Feature Importance}{equation.3.8.44}{}}
\citation{L