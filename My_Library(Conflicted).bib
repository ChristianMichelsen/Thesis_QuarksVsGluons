
@article{carmichaelDataScienceVs2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1801.00371},
  title = {Data {{Science}} vs. {{Statistics}}: {{Two Cultures}}?},
  issn = {2520-8756, 2520-8764},
  url = {http://arxiv.org/abs/1801.00371},
  doi = {10.1007/s42081-018-0009-3},
  shorttitle = {Data {{Science}} vs. {{Statistics}}},
  abstract = {Data science is the business of learning from data, which is traditionally the business of statistics. Data science, however, is often understood as a broader, task-driven and computationally-oriented version of statistics. Both the term data science and the broader idea it conveys have origins in statistics and are a reaction to a narrower view of data analysis. Expanding upon the views of a number of statisticians, this paper encourages a big-tent view of data analysis. We examine how evolving approaches to modern data analysis relate to the existing discipline of statistics (e.g. exploratory analysis, machine learning, reproducibility, computation, communication and the role of theory). Finally, we discuss what these trends mean for the future of statistics by highlighting promising directions for communication, education and research.},
  journaltitle = {Japanese Journal of Statistics and Data Science},
  urldate = {2018-08-15},
  date = {2018-05-14},
  keywords = {80/20,machine learning,ml,data science},
  author = {Carmichael, Iain and Marron, J. S.},
  file = {/Users/michelsen/Zotero/storage/GDMFHSY2/Data Science vs. Statistics Two Cultures.pdf;/Users/michelsen/Zotero/storage/NYUIY2B4/Data Science vs. Statistics Two Cultures.html}
}

@article{domingosFewUsefulThings2012,
  langid = {english},
  title = {A Few Useful Things to Know about Machine Learning},
  volume = {55},
  issn = {00010782},
  url = {http://dl.acm.org/citation.cfm?doid=2347736.2347755},
  doi = {10.1145/2347736.2347755},
  abstract = {Machine learning algorithms can figure out how to perform important tasks by generalizing from examples. This is often feasible and cost-effective where manual programming is not. As more data becomes available, more ambitious problems can be tackled. As a result, machine learning is widely used in computer science and other fields. However, developing successful machine learning applications requires a substantial amount of “black art” that is hard to find in textbooks. This article summarizes twelve key lessons that machine learning researchers and practitioners have learned. These include pitfalls to avoid, important issues to focus on, and answers to common questions.},
  number = {10},
  journaltitle = {Communications of the ACM},
  urldate = {2018-08-15},
  date = {2012-10-01},
  pages = {78},
  keywords = {machine learning},
  author = {Domingos, Pedro},
  file = {/Users/michelsen/Zotero/storage/2PXM3C5E/Domingos - 2012 - A few useful things to know about machine learning.pdf}
}

@online{WhichAlgorithmTakes2017,
  title = {Which Algorithm Takes the Crown: {{Light GBM}} vs {{XGBOOST}}?},
  url = {https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/},
  shorttitle = {Which Algorithm Takes the Crown},
  abstract = {A comparison between LightGBM and XGBoost algorithms in machine learning. XGBoost works on lead based splitting of decision tree \& is faster, parallel},
  journaltitle = {Analytics Vidhya},
  urldate = {2018-08-15},
  date = {2017-06-12T10:37:59+05:30},
  keywords = {lightgbm,machine learning,ml,xgb,xgboost},
  file = {/Users/michelsen/Zotero/storage/TQ9CUTDG/which-algorithm-takes-the-crown-light-gbm-vs-xgboost.html}
}

@online{lundbergInterpretableMachineLearning2018,
  title = {Interpretable {{Machine Learning}} with {{XGBoost}}},
  url = {https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27},
  abstract = {This is a story about the danger of interpreting your machine learning model incorrectly, and the value of interpreting it correctly…},
  journaltitle = {Towards Data Science},
  urldate = {2018-08-15},
  date = {2018-04-17T16:27:30.464Z},
  keywords = {xgb,xgboost,cover,feature importance,gain,shap,weight},
  author = {Lundberg, Scott},
  file = {/Users/michelsen/Zotero/storage/27QGW4S9/interpretable-machine-learning-with-xgboost-9ec80d148d27.html}
}

@article{goodfellowGenerativeAdversarialNetworks2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.2661},
  primaryClass = {cs, stat},
  title = {Generative {{Adversarial Networks}}},
  url = {http://arxiv.org/abs/1406.2661},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  urldate = {2018-08-15},
  date = {2014-06-10},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,adversarial,gan,generative,Goodfellow,nn},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  file = {/Users/michelsen/Zotero/storage/J4KCAPVN/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf;/Users/michelsen/Zotero/storage/96DEXYE3/1406.html}
}

@online{BasicsClassifierEvaluation2015,
  langid = {american},
  title = {The {{Basics}} of {{Classifier Evaluation}}: {{Part}} 2},
  url = {https://www.svds.com/classifiers2/},
  shorttitle = {The {{Basics}} of {{Classifier Evaluation}}},
  abstract = {Before covering evaluation techniques such as ROC curves, profit curves, and lift curves, several points must be made.},
  journaltitle = {Silicon Valley Data Science},
  urldate = {2018-08-15},
  date = {2015-12-10T12:46:20-07:00},
  keywords = {calibration},
  file = {/Users/michelsen/Zotero/storage/DETNJGPX/classifiers2.html}
}

@online{DatasframeScalableMachine,
  title = {Datas-Frame – {{Scalable Machine Learning}} ({{Part}} 1)},
  url = {https://tomaugspurger.github.io/scalable-ml-01},
  urldate = {2018-08-15},
  keywords = {dask,pipeline},
  file = {/Users/michelsen/Zotero/storage/8UPXREL5/scalable-ml-01.html}
}

@online{britzUnderstandingConvolutionalNeural2015,
  langid = {american},
  title = {Understanding {{Convolutional Neural Networks}} for {{NLP}}},
  url = {http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/},
  abstract = {When we hear about Convolutional Neural Network (CNNs), we typically think of Computer Vision. CNNs were responsible for major breakthroughs in Image~Classification and are the core~of most Compute…},
  journaltitle = {WildML},
  urldate = {2018-08-15},
  date = {2015-11-07T13:56:42+00:00},
  keywords = {nn,cnn,convolution,nlp},
  author = {Britz, Denny},
  file = {/Users/michelsen/Zotero/storage/L2AMCBUA/understanding-convolutional-neural-networks-for-nlp.html}
}

@article{greenlandStatisticalTestsPvalues2016,
  langid = {english},
  title = {Statistical Tests, {{P}}-Values, Confidence Intervals, and Power: A Guide to Misinterpretations},
  volume = {31},
  issn = {0393-2990, 1573-7284},
  url = {https://link.springer.com/article/10.1007/s10654-016-0149-3},
  doi = {10.1007/s10654-016-0149-3},
  shorttitle = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power},
  abstract = {Misinterpretation and abuse of statistical tests, confidence intervals, and statistical power have been decried for decades, yet remain rampant. A key problem is that there are no interpretations of these concepts that are at once simple, intuitive, correct, and foolproof. Instead, correct use and interpretation of these statistics requires an attention to detail which seems to tax the patience of working scientists. This high cognitive demand has led to an epidemic of shortcut definitions and interpretations that are simply wrong, sometimes disastrously so—and yet these misinterpretations dominate much of the scientific literature. In light of this problem, we provide definitions and a discussion of basic statistics that are more general and critical than typically found in traditional introductory expositions. Our goal is to provide a resource for instructors, researchers, and consumers of statistics whose knowledge of statistical theory and technique may be limited but who wish to avoid and spot misinterpretations. We emphasize how violation of often unstated analysis protocols (such as selecting analyses for presentation based on the P values they produce) can lead to small P values even if the declared test hypothesis is correct, and can lead to large P values even if that hypothesis is incorrect. We then provide an explanatory list of 25 misinterpretations of P values, confidence intervals, and power. We conclude with guidelines for improving statistical interpretation and reporting.},
  number = {4},
  journaltitle = {European Journal of Epidemiology},
  shortjournal = {Eur J Epidemiol},
  urldate = {2018-08-15},
  date = {2016-04-01},
  pages = {337-350},
  keywords = {confidence,p-values,power,statistical tests},
  author = {Greenland, Sander and Senn, Stephen J. and Rothman, Kenneth J. and Carlin, John B. and Poole, Charles and Goodman, Steven N. and Altman, Douglas G.},
  file = {/Users/michelsen/Zotero/storage/8BB2X8FN/Greenland et al. - 2016 - Statistical tests, Emphasis Type=ItalicPEmph.pdf;/Users/michelsen/Zotero/storage/7VZE9I6S/s10654-016-0149-3.html}
}

@article{donoho50YearsData2017,
  langid = {english},
  title = {50 {{Years}} of {{Data Science}}},
  volume = {26},
  issn = {1061-8600, 1537-2715},
  url = {https://www.tandfonline.com/doi/full/10.1080/10618600.2017.1384734},
  doi = {10.1080/10618600.2017.1384734},
  abstract = {More than 50 years ago, John Tukey called for a reformation of academic statistics. In ‘The Future of Data Analysis’, he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or ‘data analysis’. Ten to twenty years ago, John Chambers, Bill Cleveland and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland even suggested the catchy name “Data Science” for his envisioned field.},
  number = {4},
  journaltitle = {Journal of Computational and Graphical Statistics},
  urldate = {2018-08-15},
  date = {2017-10-02},
  pages = {745-766},
  keywords = {machine learning,ml,data science,statistics,greater data science},
  author = {Donoho, David},
  file = {/Users/michelsen/Zotero/storage/DS2TMI7A/Donoho - 2017 - 50 Years of Data Science.pdf}
}

@online{karpathyYesYouShould2016,
  title = {Yes You Should Understand Backprop},
  url = {https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b},
  abstract = {When we offered CS231n (Deep Learning class) at Stanford, we intentionally designed the programming assignments to include explicit…},
  journaltitle = {Andrej Karpathy},
  urldate = {2018-08-15},
  date = {2016-12-19T19:50:08.605Z},
  keywords = {back propagation,backprop},
  author = {Karpathy, Andrej},
  file = {/Users/michelsen/Zotero/storage/XCHFTS85/yes-you-should-understand-backprop-e2f06eab496b.html}
}

@online{IntroductionPythonEnsembles2018,
  title = {Introduction to {{Python Ensembles}}},
  url = {https://www.dataquest.io/blog/introduction-to-ensembles/},
  abstract = {This post takes you through the basics of ensembles — what they are and why they work so well — and provides a hands-on tutorial for building basic ensembles.},
  journaltitle = {Dataquest},
  urldate = {2018-08-15},
  date = {2018-01-11T18:51:04.000Z},
  file = {/Users/michelsen/Zotero/storage/WZ56SXG5/introduction-to-ensembles.html}
}

@article{larkoskiGainingMutualInformation2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1408.3122},
  title = {Gaining ({{Mutual}}) {{Information}} about {{Quark}}/{{Gluon Discrimination}}},
  volume = {2014},
  issn = {1029-8479},
  url = {http://arxiv.org/abs/1408.3122},
  doi = {10.1007/JHEP11(2014)129},
  abstract = {Discriminating quark jets from gluon jets is an important but challenging problem in jet substructure. In this paper, we use the concept of mutual information to illuminate the physics of quark/gluon tagging. Ideal quark/gluon separation requires only one bit of truth information, so even if two discriminant variables are largely uncorrelated, they can still share the same "truth overlap". Mutual information can be used to diagnose such situations, and thus determine which discriminant variables are redundant and which can be combined to improve performance. Using both parton showers and analytic resummation, we study a two-parameter family of generalized angularities, which includes familiar infrared and collinear (IRC) safe observables like thrust and broadening, as well as IRC unsafe variants like \$p\_T\^D\$ and hadron multiplicity. At leading-logarithmic (LL) order, the bulk of these variables exhibit Casimir scaling, such that their truth overlap is a universal function of the color factor ratio \$C\_A/C\_F\$. Only at next-to-leading-logarithmic (NLL) order can one see a difference in quark/gluon performance. For the IRC safe angularities, we show that the quark/gluon performance can be improved by combining angularities with complementary angular exponents. Interestingly, LL order, NLL order, Pythia 8, and Herwig++ all exhibit similar correlations between observables, but there are significant differences in the predicted quark/gluon discrimination power. For the IRC unsafe angularities, we show that the mutual information can be calculated analytically with the help of a nonperturbative "weighted-energy function", providing evidence for the complementarity of safe and unsafe observables for quark/gluon discrimination.},
  number = {11},
  journaltitle = {Journal of High Energy Physics},
  urldate = {2018-08-15},
  date = {2014-11},
  keywords = {High Energy Physics - Experiment,High Energy Physics - Phenomenology,Nuclear Theory,generalized angularities,gluon,quark},
  author = {Larkoski, Andrew J. and Thaler, Jesse and Waalewijn, Wouter J.},
  file = {/Users/michelsen/Zotero/storage/GVMGBDCB/Larkoski et al. - 2014 - Gaining (Mutual) Information about QuarkGluon Dis.pdf;/Users/michelsen/Zotero/storage/PGTZEJDN/1408.html}
}

@article{thalerReportHouchesQuark,
  langid = {english},
  title = {Report of the {{Les Houches}}  {{Quark}}/{{Gluon Subgroup}}},
  number = {1},
  pages = {28},
  keywords = {gluon,quark,workshop},
  author = {Thaler, Jesse},
  file = {/Users/michelsen/Zotero/storage/CBJZ2N4R/Thaler - Report of the Les Houches  QuarkGluon Subgroup.pdf;/Users/michelsen/Zotero/storage/9ZGMEGWB/60556993-Report-of-the-les-houches-quark-gluon-subgroup.html}
}

@article{deryWeaklySupervisedClassification2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1702.00414},
  title = {Weakly {{Supervised Classification}} in {{High Energy Physics}}},
  volume = {2017},
  issn = {1029-8479},
  url = {http://arxiv.org/abs/1702.00414},
  doi = {10.1007/JHEP05(2017)145},
  abstract = {As machine learning algorithms become increasingly sophisticated to exploit subtle features of the data, they often become more dependent on simulations. This paper presents a new approach called weakly supervised classification in which class proportions are the only input into the machine learning algorithm. Using one of the most challenging binary classification tasks in high energy physics - quark versus gluon tagging - we show that weakly supervised classification can match the performance of fully supervised algorithms. Furthermore, by design, the new algorithm is insensitive to any mis-modeling of discriminating features in the data by the simulation. Weakly supervised classification is a general procedure that can be applied to a wide variety of learning problems to boost performance and robustness when detailed simulations are not reliable or not available.},
  number = {5},
  journaltitle = {Journal of High Energy Physics},
  urldate = {2018-08-15},
  date = {2017-05},
  keywords = {Statistics - Machine Learning,High Energy Physics - Phenomenology,gluon,quark,Physics - Data Analysis; Statistics and Probability,classification,weakly,weakly supervised},
  author = {Dery, Lucio Mwinmaarong and Nachman, Benjamin and Rubbo, Francesco and Schwartzman, Ariel},
  file = {/Users/michelsen/Zotero/storage/CKUNBFYU/Dery et al. - 2017 - Weakly Supervised Classification in High Energy Ph.pdf;/Users/michelsen/Zotero/storage/WIGAZGS9/1702.html}
}

@article{metodievClassificationLabelsLearning2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1708.02949},
  title = {Classification without Labels: {{Learning}} from Mixed Samples in High Energy Physics},
  volume = {2017},
  issn = {1029-8479},
  url = {http://arxiv.org/abs/1708.02949},
  doi = {10.1007/JHEP10(2017)174},
  shorttitle = {Classification without Labels},
  abstract = {Modern machine learning techniques can be used to construct powerful models for difficult collider physics problems. In many applications, however, these models are trained on imperfect simulations due to a lack of truth-level information in the data, which risks the model learning artifacts of the simulation. In this paper, we introduce the paradigm of classification without labels (CWoLa) in which a classifier is trained to distinguish statistical mixtures of classes, which are common in collider physics. Crucially, neither individual labels nor class proportions are required, yet we prove that the optimal classifier in the CWoLa paradigm is also the optimal classifier in the traditional fully-supervised case where all label information is available. After demonstrating the power of this method in an analytical toy example, we consider a realistic benchmark for collider physics: distinguishing quark- versus gluon-initiated jets using mixed quark/gluon training samples. More generally, CWoLa can be applied to any classification problem where labels or class proportions are unknown or simulations are unreliable, but statistical mixtures of the classes are available.},
  number = {10},
  journaltitle = {Journal of High Energy Physics},
  urldate = {2018-08-15},
  date = {2017-10},
  keywords = {Statistics - Machine Learning,High Energy Physics - Experiment,High Energy Physics - Phenomenology},
  author = {Metodiev, Eric M. and Nachman, Benjamin and Thaler, Jesse},
  file = {/Users/michelsen/Zotero/storage/HAK8MZBC/Metodiev et al. - 2017 - Classification without labels Learning from mixed.pdf;/Users/michelsen/Zotero/storage/KVLGKGYU/1708.html}
}

@article{chenXGBoostScalableTree2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.02754},
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  url = {http://arxiv.org/abs/1603.02754},
  doi = {10.1145/2939672.2939785},
  shorttitle = {{{XGBoost}}},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  journaltitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '16},
  urldate = {2018-08-22},
  date = {2016},
  pages = {785-794},
  keywords = {machine learning,xgb,xgboost,Computer Science - Machine Learning,ML},
  author = {Chen, Tianqi and Guestrin, Carlos},
  file = {/Users/michelsen/Zotero/storage/574WD7IM/Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf;/Users/michelsen/Zotero/storage/Z9LV69U7/1603.html}
}

@book{hastieElementsStatisticalLearning2009,
  langid = {english},
  location = {{New York}},
  title = {The {{Elements}} of {{Statistical Learning}}: {{Data Mining}}, {{Inference}}, and {{Prediction}}, {{Second Edition}}},
  edition = {2},
  isbn = {978-0-387-84857-0},
  url = {//www.springer.com/la/book/9780387848570},
  shorttitle = {The {{Elements}} of {{Statistical Learning}}},
  abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer-Verlag}},
  urldate = {2018-08-23},
  date = {2009},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  file = {/Users/michelsen/Zotero/storage/YZE7SQV8/Hastie et al. - 2009 - The Elements of Statistical Learning Data Mining,.pdf;/Users/michelsen/Zotero/storage/5ZXE2J89/9780387848570.html}
}

@article{danielovaOverwinteringMosquitoborneViruses1975,
  langid = {english},
  title = {Overwintering of Mosquito-Borne Viruses},
  volume = {53},
  issn = {0302-2137},
  number = {5},
  journaltitle = {Medical Biology},
  shortjournal = {Med. Biol.},
  date = {1975-10},
  pages = {282-287},
  keywords = {Aedes,Animals,Anopheles,Arboviruses,Culex,Culicidae,Encephalitis Virus; California,Female,Hibernation,Insect Vectors,Seasons,Temperature},
  author = {Danielová, V.},
  eprinttype = {pmid},
  eprint = {1603}
}

@article{maatenVisualizingDataUsing2008,
  title = {Visualizing {{Data}} Using T-{{SNE}}},
  volume = {9},
  issn = {ISSN 1533-7928},
  url = {http://www.jmlr.org/papers/v9/vandermaaten08a.html},
  issue = {Nov},
  journaltitle = {Journal of Machine Learning Research},
  urldate = {2018-08-30},
  date = {2008},
  pages = {2579-2605},
  keywords = {t-sne,tsne,dimensionality reduction},
  author = {van der Maaten, Laurens and Hinton, Geoffrey},
  file = {/Users/michelsen/Zotero/storage/E5EJCQZT/Maaten and Hinton - 2008 - Visualizing Data using t-SNE.pdf;/Users/michelsen/Zotero/storage/2XLZ75YM/vandermaaten08a.html}
}

@article{kuleshovAccurateUncertaintiesDeep2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.00263},
  primaryClass = {cs, stat},
  title = {Accurate {{Uncertainties}} for {{Deep Learning Using Calibrated Regression}}},
  url = {http://arxiv.org/abs/1807.00263},
  abstract = {Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty. However, because of model misspecification and the use of approximate inference, Bayesian uncertainty estimates are often inaccurate -- for example, a 90\% credible interval may not contain the true outcome 90\% of the time. Here, we propose a simple procedure for calibrating any regression algorithm; when applied to Bayesian and probabilistic models, it is guaranteed to produce calibrated uncertainty estimates given enough data. Our procedure is inspired by Platt scaling and extends previous work on classification. We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently outputs well-calibrated credible intervals while improving performance on time series forecasting and model-based reinforcement learning tasks.},
  urldate = {2018-08-30},
  date = {2018-06-30},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,calibrated,uncertainties,uncertainty},
  author = {Kuleshov, Volodymyr and Fenner, Nathan and Ermon, Stefano},
  file = {/Users/michelsen/Zotero/storage/3AFIFFLE/Kuleshov et al. - 2018 - Accurate Uncertainties for Deep Learning Using Cal.pdf;/Users/michelsen/Zotero/storage/IFM9MI3T/1807.html}
}

@article{sculleyWINNERCURSEPACE2018,
  langid = {english},
  title = {{{WINNER}}’{{S CURSE}}? {{ON PACE}}, {{PROGRESS}}, {{AND EMPIRICAL RIGOR}}},
  abstract = {The field of ML is distinguished both by rapid innovation and rapid dissemination of results. While the pace of progress has been extraordinary by any measure, in this paper we explore potential issues that we believe to be arising as a result. In particular, we observe that the rate of empirical advancement may not have been matched by consistent increase in the level of empirical rigor across the field as a whole. This short position paper highlights examples where progress has actually been slowed as a result, offers thoughts on incentive structures currently at play, and gives suggestions as seeds for discussions on productive change.},
  date = {2018},
  pages = {4},
  author = {Sculley, D and Snoek, Jasper and Rahimi, Ali and Wiltschko, Alex},
  file = {/Users/michelsen/Zotero/storage/DVFIAHN6/Sculley et al. - 2018 - ON PACE, PROGRESS, AND EMPIRICAL RIGOR.pdf}
}

@article{sculleyWinnerCursePace2018,
  title = {Winner's {{Curse}}? {{On Pace}}, {{Progress}}, and {{Empirical Rigor}}},
  url = {https://openreview.net/forum?id=rJWF0Fywf},
  shorttitle = {Winner's {{Curse}}?},
  abstract = {The field of ML is distinguished both by rapid innovation and rapid dissemination of results. While the pace of progress has been extraordinary by any measure, in this paper we explore potential...},
  urldate = {2018-08-30},
  date = {2018-02-12},
  author = {Sculley, D. and Snoek, Jasper and Wiltschko, Alex and Rahimi, Ali},
  file = {/Users/michelsen/Zotero/storage/WLPR9XAZ/Sculley et al. - 2018 - Winner's Curse On Pace, Progress, and Empirical R.pdf;/Users/michelsen/Zotero/storage/XYVVAWLE/forum.html}
}

@article{bottouOptimizationMethodsLargeScale2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.04838},
  primaryClass = {cs, math, stat},
  title = {Optimization {{Methods}} for {{Large}}-{{Scale Machine Learning}}},
  url = {http://arxiv.org/abs/1606.04838},
  abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
  urldate = {2018-08-30},
  date = {2016-06-15},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Mathematics - Optimization and Control},
  author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
  file = {/Users/michelsen/Zotero/storage/VGUM68YR/Bottou et al. - 2016 - Optimization Methods for Large-Scale Machine Learn.pdf;/Users/michelsen/Zotero/storage/SRMD8G5H/1606.html}
}

@online{StanfordEngineeringEverywhere,
  title = {Stanford {{Engineering Everywhere}} | {{CS229}} - {{Machine Learning}}},
  url = {https://see.stanford.edu/Course/CS229},
  urldate = {2018-08-30},
  keywords = {ml,CS229,lecture notes},
  file = {/Users/michelsen/Zotero/storage/E8V5WHLK/CS229.html}
}

@article{mehtaHighbiasLowvarianceIntroduction2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.08823},
  primaryClass = {cond-mat, physics:physics, stat},
  title = {A High-Bias, Low-Variance Introduction to {{Machine Learning}} for Physicists},
  url = {http://arxiv.org/abs/1803.08823},
  abstract = {Machine Learning (ML) is one of the most exciting and dynamic areas of modern research and application. The purpose of this review is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists. The review begins by covering fundamental concepts in ML and modern statistics such as the bias-variance tradeoff, overfitting, regularization, and generalization before moving on to more advanced topics in both supervised and unsupervised learning. Topics covered in the review include ensemble models, deep learning and neural networks, clustering and data visualization, energy-based models (including MaxEnt models and Restricted Boltzmann Machines), and variational methods. Throughout, we emphasize the many natural connections between ML and statistical physics. A notable aspect of the review is the use of Python notebooks to introduce modern ML/statistical packages to readers using physics-inspired datasets (the Ising Model and Monte-Carlo simulations of supersymmetric decays of proton-proton collisions). We conclude with an extended outlook discussing possible uses of machine learning for furthering our understanding of the physical world as well as open problems in ML where physicists maybe able to contribute. (Notebooks are available at https://physics.bu.edu/\textasciitilde{}pankajm/MLnotebooks.html )},
  urldate = {2018-09-02},
  date = {2018-03-23},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Condensed Matter - Statistical Mechanics,Physics - Computational Physics},
  author = {Mehta, Pankaj and Bukov, Marin and Wang, Ching-Hao and Day, Alexandre G. R. and Richardson, Clint and Fisher, Charles K. and Schwab, David J.},
  file = {/Users/michelsen/Zotero/storage/5TJ3RXMN/Mehta et al. - 2018 - A high-bias, low-variance introduction to Machine .pdf;/Users/michelsen/Zotero/storage/7CZKW4HD/1803.html}
}

@article{mcinnesUMAPUniformManifold2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.03426},
  primaryClass = {cs, stat},
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  url = {http://arxiv.org/abs/1802.03426},
  shorttitle = {{{UMAP}}},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP as described has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  urldate = {2018-09-02},
  date = {2018-02-09},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Computational Geometry,umap},
  author = {McInnes, Leland and Healy, John},
  file = {/Users/michelsen/Zotero/storage/46JKNJAK/McInnes and Healy - 2018 - UMAP Uniform Manifold Approximation and Projectio.pdf;/Users/michelsen/Zotero/storage/BGF8EJRD/1802.html}
}

@article{mulalicFinancialCrisisDiverging2017,
  langid = {english},
  title = {The {{Financial Crisis}} and {{Diverging House Prices}}: {{Evidence}} from the {{Copenhagen Metropolitan Area}}},
  issn = {1556-5068},
  url = {https://www.ssrn.com/abstract=3041272},
  doi = {10.2139/ssrn.3041272},
  shorttitle = {The {{Financial Crisis}} and {{Diverging House Prices}}},
  abstract = {This paper investigates the development of house prices in Copenhagen in the period 1994-2013, while paying special attention to the heterogeneous impact of the boom and bust periods along the dimensions of housing type (single vs multifamily housing), geography and quality. To allow for price developments that can differ by quality, we use a recently developed generalization of the conventional Muth model that assumes a constant unit price across the quality spectrum. It allows us to separately consider the development of house prices and quality in Copenhagen neighbourhoods. Moreover, we investigate the validity of the common assumption of a constant unit price and reject it decisively. We use detailed housing transaction data for the greater Copenhagen area. We show that the housing boom of the 2000’s and the bust that followed hit the lowest quality segments significantly harder than the high quality segments of the housing market.},
  journaltitle = {SSRN Electronic Journal},
  urldate = {2018-09-02},
  date = {2017},
  author = {Mulalic, Ismir and Rasmussen, Holger and Rouwendal, Jan and Woltmann, Hans Henrik},
  file = {/Users/michelsen/Zotero/storage/EVWLZAKS/Mulalic et al. - 2017 - The Financial Crisis and Diverging House Prices E.pdf}
}

@article{zhangSensitivityAnalysisPractitioners2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1510.03820},
  primaryClass = {cs},
  title = {A {{Sensitivity Analysis}} of (and {{Practitioners}}' {{Guide}} to) {{Convolutional Neural Networks}} for {{Sentence Classification}}},
  url = {http://arxiv.org/abs/1510.03820},
  abstract = {Convolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on the practically important task of sentence classification (kim 2014, kalchbrenner 2014, johnson 2014). However, these models require practitioners to specify an exact model architecture and set accompanying hyperparameters, including the filter region size, regularization parameters, and so on. It is currently unknown how sensitive model performance is to changes in these configurations for the task of sentence classification. We thus conduct a sensitivity analysis of one-layer CNNs to explore the effect of architecture components on model performance; our aim is to distinguish between important and comparatively inconsequential design decisions for sentence classification. We focus on one-layer CNNs (to the exclusion of more complex models) due to their comparative simplicity and strong empirical performance, which makes it a modern standard baseline method akin to Support Vector Machine (SVMs) and logistic regression. We derive practical advice from our extensive empirical results for those interested in getting the most out of CNNs for sentence classification in real world settings.},
  urldate = {2018-09-02},
  date = {2015-10-13},
  keywords = {Computer Science - Machine Learning,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,NLP},
  author = {Zhang, Ye and Wallace, Byron},
  file = {/Users/michelsen/Zotero/storage/E2R4CVFR/Zhang and Wallace - 2015 - A Sensitivity Analysis of (and Practitioners' Guid.pdf;/Users/michelsen/Zotero/storage/6L492HGT/1510.html}
}

@article{goldsteinPeekingBlackBox2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1309.6392},
  primaryClass = {stat},
  title = {Peeking {{Inside}} the {{Black Box}}: {{Visualizing Statistical Learning}} with {{Plots}} of {{Individual Conditional Expectation}}},
  url = {http://arxiv.org/abs/1309.6392},
  shorttitle = {Peeking {{Inside}} the {{Black Box}}},
  abstract = {This article presents Individual Conditional Expectation (ICE) plots, a tool for visualizing the model estimated by any supervised learning algorithm. Classical partial dependence plots (PDPs) help visualize the average partial relationship between the predicted response and one or more features. In the presence of substantial interaction effects, the partial response relationship can be heterogeneous. Thus, an average curve, such as the PDP, can obfuscate the complexity of the modeled relationship. Accordingly, ICE plots refine the partial dependence plot by graphing the functional relationship between the predicted response and the feature for individual observations. Specifically, ICE plots highlight the variation in the fitted values across the range of a covariate, suggesting where and to what extent heterogeneities might exist. In addition to providing a plotting suite for exploratory analysis, we include a visual test for additive structure in the data generating model. Through simulated examples and real data sets, we demonstrate how ICE plots can shed light on estimated models in ways PDPs cannot. Procedures outlined are available in the R package ICEbox.},
  urldate = {2018-09-02},
  date = {2013-09-24},
  keywords = {Statistics - Applications,ICE,partial dependence},
  author = {Goldstein, Alex and Kapelner, Adam and Bleich, Justin and Pitkin, Emil},
  file = {/Users/michelsen/Zotero/storage/YFTV884D/Goldstein et al. - 2013 - Peeking Inside the Black Box Visualizing Statisti.pdf;/Users/michelsen/Zotero/storage/GBTFNXT7/1309.html}
}

@article{friedmanPredictiveLearningRule2008,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0811.1679},
  title = {Predictive Learning via Rule Ensembles},
  volume = {2},
  issn = {1932-6157},
  url = {http://arxiv.org/abs/0811.1679},
  doi = {10.1214/07-AOAS148},
  abstract = {General regression and classification models are constructed as linear combinations of simple rules derived from the data. Each rule consists of a conjunction of a small number of simple statements concerning the values of individual input variables. These rule ensembles are shown to produce predictive accuracy comparable to the best methods. However, their principal advantage lies in interpretation. Because of its simple form, each rule is easy to understand, as is its influence on individual predictions, selected subsets of predictions, or globally over the entire space of joint input variable values. Similarly, the degree of relevance of the respective input variables can be assessed globally, locally in different regions of the input space, or at individual prediction points. Techniques are presented for automatically identifying those variables that are involved in interactions with other variables, the strength and degree of those interactions, as well as the identities of the other variables with which they interact. Graphical representations are used to visualize both main and interaction effects.},
  number = {3},
  journaltitle = {The Annals of Applied Statistics},
  urldate = {2018-09-02},
  date = {2008-09},
  pages = {916-954},
  keywords = {Statistics - Applications},
  author = {Friedman, Jerome H. and Popescu, Bogdan E.},
  file = {/Users/michelsen/Zotero/storage/APSBBPDF/Friedman and Popescu - 2008 - Predictive learning via rule ensembles.pdf;/Users/michelsen/Zotero/storage/ZJ93AZEV/0811.html}
}

@article{lundbergUnifiedApproachInterpreting2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.07874},
  primaryClass = {cs, stat},
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  url = {http://arxiv.org/abs/1705.07874},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  urldate = {2018-09-02},
  date = {2017-05-22},
  keywords = {shap,Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence,shapley},
  author = {Lundberg, Scott and Lee, Su-In},
  file = {/Users/michelsen/Zotero/storage/A6QEWJVN/Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf;/Users/michelsen/Zotero/storage/D5GW6ZJJ/1705.html}
}

@article{lundbergConsistentFeatureAttribution2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.06060},
  primaryClass = {cs, stat},
  title = {Consistent Feature Attribution for Tree Ensembles},
  url = {http://arxiv.org/abs/1706.06060},
  abstract = {Note that a newer expanded version of this paper is now available at: arXiv:1802.03888 It is critical in many applications to understand what features are important for a model, and why individual predictions were made. For tree ensemble methods these questions are usually answered by attributing importance values to input features, either globally or for a single prediction. Here we show that current feature attribution methods are inconsistent, which means changing the model to rely more on a given feature can actually decrease the importance assigned to that feature. To address this problem we develop fast exact solutions for SHAP (SHapley Additive exPlanation) values, which were recently shown to be the unique additive feature attribution method based on conditional expectations that is both consistent and locally accurate. We integrate these improvements into the latest version of XGBoost, demonstrate the inconsistencies of current methods, and show how using SHAP values results in significantly improved supervised clustering performance. Feature importance values are a key part of understanding widely used models such as gradient boosting trees and random forests, so improvements to them have broad practical implications.},
  urldate = {2018-09-02},
  date = {2017-06-19},
  keywords = {shap,Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence,shapley},
  author = {Lundberg, Scott M. and Lee, Su-In},
  file = {/Users/michelsen/Zotero/storage/HI367ET2/Lundberg and Lee - 2017 - Consistent feature attribution for tree ensembles.pdf;/Users/michelsen/Zotero/storage/FNXAQCAR/1706.html}
}

@article{taylorForecastingScale,
  langid = {english},
  title = {Forecasting at Scale},
  url = {https://peerj.com/preprints/3190},
  doi = {10.7287/peerj.preprints.3190v2},
  abstract = {Forecasting is a common data science task that helps organizations with capacity planning, goal setting, and anomaly detection. Despite its importance, there are serious challenges associated with producing reliable and high quality forecasts –especially when there are a variety of time series and analysts with expertise in time series modeling are relatively rare. To address these challenges, we describe a practical approach to forecasting “at scale” that combines configurable models with analyst-in-the-loop performance analysis. We propose a modular regression model with interpretable parameters that can be intuitively adjusted by analysts with domain knowledge about the time series. We describe performance analyses to compare and evaluate forecasting procedures, and automatically flag forecasts for manual review and adjustment. Tools that help analysts to use their expertise most effectively enable reliable, practical forecasting of business time series.},
  urldate = {2018-09-02},
  keywords = {facebook,forecast,prophet},
  author = {Taylor, Sean J and Letham, Benjamin},
  file = {/Users/michelsen/Zotero/storage/4TGS8CYS/Taylor and Letham - Forecasting at scale.pdf}
}

@article{kiapourRobustBayesianPrediction2011,
  title = {Robust {{Bayesian}} Prediction and Estimation under a Squared Log Error Loss Function},
  volume = {81},
  issn = {0167-7152},
  url = {http://www.sciencedirect.com/science/article/pii/S0167715211002331},
  doi = {10.1016/j.spl.2011.07.002},
  abstract = {Robust Bayesian analysis is concerned with the problem of making decisions about some future observation or an unknown parameter, when the prior distribution belongs to a class Γ instead of being specified exactly. In this paper, the problem of robust Bayesian prediction and estimation under a squared log error loss function is considered. We find the posterior regret Γ-minimax predictor and estimator in a general class of distributions. Furthermore, we construct the conditional Γ-minimax, most stable and least sensitive prediction and estimation in a gamma model. A prequential analysis is carried out by using a simulation study to compare these predictors.},
  number = {11},
  journaltitle = {Statistics \& Probability Letters},
  shortjournal = {Statistics \& Probability Letters},
  urldate = {2018-09-02},
  date = {2011-11-01},
  pages = {1717-1724},
  keywords = {Class of priors,Gamma distribution,Robust Bayesian prediction,Sensitivity analysis,Squared log error loss function},
  author = {Kiapour, A. and Nematollahi, N.},
  file = {/Users/michelsen/Zotero/storage/N2U5TEZI/Kiapour and Nematollahi - 2011 - Robust Bayesian prediction and estimation under a .pdf;/Users/michelsen/Zotero/storage/8AJFABLJ/S0167715211002331.html}
}

@article{reshefDetectingNovelAssociations2011,
  langid = {english},
  title = {Detecting {{Novel Associations}} in {{Large Data Sets}}},
  volume = {334},
  issn = {0036-8075, 1095-9203},
  url = {http://science.sciencemag.org/content/334/6062/1518},
  doi = {10.1126/science.1205438},
  abstract = {Identifying interesting relationships between pairs of variables in large data sets is increasingly important. Here, we present a measure of dependence for two-variable relationships: the maximal information coefficient (MIC). MIC captures a wide range of associations both functional and not, and for functional relationships provides a score that roughly equals the coefficient of determination (R2) of the data relative to the regression function. MIC belongs to a larger class of maximal information-based nonparametric exploration (MINE) statistics for identifying and classifying relationships. We apply MIC and MINE to data sets in global health, gene expression, major-league baseball, and the human gut microbiota and identify known and novel relationships.
A statistical method reveals relationships among variables in complex data sets.
A statistical method reveals relationships among variables in complex data sets.},
  number = {6062},
  journaltitle = {Science},
  urldate = {2018-09-02},
  date = {2011-12-16},
  pages = {1518-1524},
  keywords = {mic,mine},
  author = {Reshef, David N. and Reshef, Yakir A. and Finucane, Hilary K. and Grossman, Sharon R. and McVean, Gilean and Turnbaugh, Peter J. and Lander, Eric S. and Mitzenmacher, Michael and Sabeti, Pardis C.},
  file = {/Users/michelsen/Zotero/storage/H7DXTUMY/Reshef et al. - 2011 - Detecting Novel Associations in Large Data Sets.pdf;/Users/michelsen/Zotero/storage/QHRDSBPA/1518.html},
  eprinttype = {pmid},
  eprint = {22174245}
}

@incollection{keLightGBMHighlyEfficient2017,
  title = {{{LightGBM}}: {{A Highly Efficient Gradient Boosting Decision Tree}}},
  url = {http://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf},
  shorttitle = {{{LightGBM}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2018-09-02},
  date = {2017},
  pages = {3146--3154},
  keywords = {lightgbm},
  author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  file = {/Users/michelsen/Zotero/storage/ARLXYW3U/Ke et al. - 2017 - LightGBM A Highly Efficient Gradient Boosting Dec.pdf;/Users/michelsen/Zotero/storage/BQ9YBD8F/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.html}
}

@article{potdarComparativeStudyCategorical2017,
  langid = {english},
  title = {A {{Comparative Study}} of {{Categorical Variable Encoding Techniques}} for {{Neural Network Classifiers}}},
  volume = {175},
  issn = {09758887},
  url = {http://www.ijcaonline.org/archives/volume175/number4/potdar-2017-ijca-915495.pdf},
  doi = {10.5120/ijca2017915495},
  abstract = {In classification analysis, the dependent variable is frequently influenced not only by ratio scale variables, but also by qualitative (nominal scale) variables. Machine Learning algorithms accept only numerical inputs, hence, it is necessary to encode these categorical variables into numerical values using encoding techniques.},
  number = {4},
  journaltitle = {International Journal of Computer Applications},
  urldate = {2018-09-02},
  date = {2017-10-17},
  pages = {7-9},
  author = {Potdar, Kedar and S., Taher and D., Chinmay},
  file = {/Users/michelsen/Zotero/storage/A7MHX967/Potdar et al. - 2017 - A Comparative Study of Categorical Variable Encodi.pdf}
}

@article{speedCorrelation21stCentury2011,
  langid = {english},
  title = {A {{Correlation}} for the 21st {{Century}}},
  volume = {334},
  issn = {0036-8075, 1095-9203},
  url = {http://science.sciencemag.org/content/334/6062/1502},
  doi = {10.1126/science.1215894},
  abstract = {A novel statistical approach has been developed that can uncover nonlinear associations in large data sets.
A novel statistical approach has been developed that can uncover nonlinear associations in large data sets.},
  number = {6062},
  journaltitle = {Science},
  urldate = {2018-09-02},
  date = {2011-12-16},
  pages = {1502-1503},
  keywords = {mic},
  author = {Speed, Terry},
  file = {/Users/michelsen/Zotero/storage/D4VADSR5/Speed - 2011 - A Correlation for the 21st Century.pdf;/Users/michelsen/Zotero/storage/7TVYBPK9/1502.html},
  eprinttype = {pmid},
  eprint = {22174235}
}

@article{tibshiraniValeriePatrickHastie,
  langid = {english},
  title = {Valerie and {{Patrick Hastie}}},
  pages = {764},
  author = {Tibshirani, Sami and Friedman, Harry},
  file = {/Users/michelsen/Zotero/storage/8FHWQKSJ/Tibshirani and Friedman - Valerie and Patrick Hastie.pdf}
}

@book{abu-mostafaLearningDataShort2012,
  langid = {english},
  location = {{S.l.}},
  title = {Learning from Data: A Short Course},
  isbn = {978-1-60049-006-4},
  shorttitle = {Learning from Data},
  pagetotal = {201},
  publisher = {{AMLbook.com}},
  date = {2012},
  author = {Abu-Mostafa, Yaser S. and Magdon-Ismail, Malik and Lin, Hsuan-Tien},
  file = {/Users/michelsen/Zotero/storage/LG5LX7EV/Abu-Mostafa et al. - 2012 - Learning from data a short course.pdf},
  note = {OCLC: 808441289}
}

@article{lundbergConsistentIndividualizedFeature2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.03888},
  primaryClass = {cs, stat},
  title = {Consistent {{Individualized Feature Attribution}} for {{Tree Ensembles}} - {{SHAP}}},
  url = {http://arxiv.org/abs/1802.03888},
  abstract = {Interpreting predictions from tree ensemble methods such as gradient boosting machines and random forests is important, yet feature attribution for trees is often heuristic and not individualized for each prediction. Here we show that popular feature attribution methods are inconsistent, meaning they can lower a feature's assigned importance when the true impact of that feature actually increases. This is a fundamental problem that casts doubt on any comparison between features. To address it we turn to recent applications of game theory and develop fast exact tree solutions for SHAP (SHapley Additive exPlanation) values, which are the unique consistent and locally accurate attribution values. We then extend SHAP values to interaction effects and define SHAP interaction values. We propose a rich visualization of individualized feature attributions that improves over classic attribution summaries and partial dependence plots, and a unique "supervised" clustering (clustering based on feature attributions). We demonstrate better agreement with human intuition through a user study, exponential improvements in run time, improved clustering performance, and better identification of influential features. An implementation of our algorithm has also been merged into XGBoost and LightGBM, see http://github.com/slundberg/shap for details.},
  urldate = {2018-09-05},
  date = {2018-02-11},
  keywords = {shap,Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Lundberg, Scott M. and Erion, Gabriel G. and Lee, Su-In},
  file = {/Users/michelsen/Zotero/storage/DEF7F9RC/Lundberg et al. - 2018 - Consistent Individualized Feature Attribution for .pdf;/Users/michelsen/Zotero/storage/XAYBVZ9I/1802.html}
}

@article{nielsenTreeBoostingXGBoost2016,
  langid = {english},
  title = {Tree {{Boosting With XGBoost}} - {{Why Does XGBoost Win}} "{{Every}}" {{Machine Learning Competition}}?},
  abstract = {Tree boosting has empirically proven to be a highly effective approach to predictive
modeling. It has shown remarkable results for a vast array of problems. For many
years, MART has been the tree boosting method of choice. More recently, a tree
boosting method known as XGBoost has gained popularity by winning numerous
machine learning competitions.
In this thesis, we will investigate how XGBoost differs from the more traditional
MART. We will show that XGBoost employs a boosting algorithm which we will
term Newton boosting. This boosting algorithm will further be compared with the
gradient boosting algorithm that MART employs. Moreover, we will discuss the
regularization techniques that these methods offer and the effect these have on the
models.
In addition to this, we will attempt to answer the question of why XGBoost
seems to win so many competitions. To do this, we will provide some arguments
for why tree boosting, and in particular XGBoost, seems to be such a highly effective
and versatile approach to predictive modeling. The core argument is that
tree boosting can be seen to adaptively determine the local neighbourhoods of the
model. Tree boosting can thus be seen to take the bias-variance tradeoff into consideration
during model fitting. XGBoost further introduces some subtle improvements
which allows it to deal with the bias-variance tradeoff even more carefully.},
  date = {2016-12},
  pages = {110},
  keywords = {xgb,xgboost},
  author = {Nielsen, Didrik},
  file = {/Users/michelsen/Zotero/storage/I6PRLQTL/Nielsen - Tree Boosting With XGBoost.pdf}
}

@article{zhangBoostingEarlyStopping2005,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {math/0508276},
  title = {Boosting with Early Stopping: {{Convergence}} and Consistency},
  volume = {33},
  issn = {0090-5364},
  url = {http://arxiv.org/abs/math/0508276},
  doi = {10.1214/009053605000000255},
  shorttitle = {Boosting with Early Stopping},
  abstract = {Boosting is one of the most significant advances in machine learning for classification and regression. In its original and computationally flexible version, boosting seeks to minimize empirically a loss function in a greedy fashion. The resulting estimator takes an additive function form and is built iteratively by applying a base estimator (or learner) to updated samples depending on the previous iterations. An unusual regularization technique, early stopping, is employed based on CV or a test set. This paper studies numerical convergence, consistency and statistical rates of convergence of boosting with early stopping, when it is carried out over the linear span of a family of basis functions. For general loss functions, we prove the convergence of boosting's greedy optimization to the infinimum of the loss function over the linear span. Using the numerical convergence result, we find early-stopping strategies under which boosting is shown to be consistent based on i.i.d. samples, and we obtain bounds on the rates of convergence for boosting estimators. Simulation studies are also presented to illustrate the relevance of our theoretical results for providing insights to practical aspects of boosting. As a side product, these results also reveal the importance of restricting the greedy search step-sizes, as known in practice through the work of Friedman and others. Moreover, our results lead to a rigorous proof that for a linearly separable problem, AdaBoost with \textbackslash{}epsilon\textbackslash{}to0 step-size becomes an L\^1-margin maximizer when left to run to convergence.},
  number = {4},
  journaltitle = {The Annals of Statistics},
  urldate = {2019-01-02},
  date = {2005-08},
  pages = {1538-1579},
  keywords = {62G05; 62G08 (Primary),Mathematics - Statistics Theory,early stopping},
  author = {Zhang, Tong and Yu, Bin},
  file = {/Users/michelsen/Zotero/storage/WPQHM2ER/Zhang and Yu - 2005 - Boosting with early stopping Convergence and cons.pdf;/Users/michelsen/Zotero/storage/N3UQIVDZ/0508276.html}
}

@incollection{mullerRegularizationTechniquesImprove2012,
  langid = {english},
  location = {{Berlin, Heidelberg}},
  title = {Regularization {{Techniques}} to {{Improve Generalization}}},
  isbn = {978-3-642-35289-8},
  url = {https://doi.org/10.1007/978-3-642-35289-8_4},
  doi = {10.1007/978-3-642-35289-8_4},
  abstract = {PrefaceGood tricks for regularization are extremely important for improving the generalization ability of neural networks. The first and most commonly used trick is early stopping, which was originally described in [11].},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}: {{Second Edition}}},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2019-01-02},
  date = {2012},
  pages = {49-51},
  keywords = {early stopping},
  author = {Müller, Klaus-Robert},
  editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
  file = {/Users/michelsen/Zotero/storage/AG7QK3HV/Montavon et al. - 2012 - Neural networks tricks of the trade.pdf;/Users/michelsen/Zotero/storage/D7ZGSEKC/Müller - 2012 - Regularization Techniques to Improve Generalizatio.pdf}
}

@online{bilaniukEinsteinSummationNumpy2016,
  langid = {english},
  title = {Einstein {{Summation}} in {{Numpy}}},
  url = {https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/},
  abstract = {In Python’s Numpy library lives an extremely general, but little-known and used, function called einsum() that performs summation according to Einstein’s summation convention. In this t…},
  journaltitle = {Olexa Bilaniuk's IFT6266H16 Course Blog},
  urldate = {2019-01-02},
  date = {2016-02-04T22:51:02+00:00},
  keywords = {einstein summation,numpy,tensor,python},
  author = {Bilaniuk, Olexa},
  file = {/Users/michelsen/Zotero/storage/PZEQLUSB/einstein-summation-in-numpy.html}
}

@online{BasicIntroductionNumPy,
  title = {A Basic Introduction to {{NumPy}}'s Einsum – Ajcr – {{Haphazard}} Investigations},
  url = {http://ajcr.net/Basic-guide-to-einsum/},
  urldate = {2019-01-02},
  keywords = {einstein summation,numpy,einsum,python},
  file = {/Users/michelsen/Zotero/storage/57CETY7P/Basic-guide-to-einsum.html}
}

@article{stearnsMODESTADVICEGRADUATE,
  langid = {english},
  title = {{{SOME MODEST ADVICE FOR GRADUATE STUDENTS}}},
  pages = {9},
  author = {Stearns, Stephen C},
  file = {/Users/michelsen/Zotero/storage/IZ3Z828E/Stearns - SOME MODEST ADVICE FOR GRADUATE STUDENTS.pdf}
}

@online{WhysHowsLicensing,
  title = {The {{Whys}} and {{Hows}} of {{Licensing Scientific Code}}},
  url = {http://www.astrobetter.com/blog/2014/03/10/the-whys-and-hows-of-licensing-scientific-code/},
  urldate = {2019-01-02},
  keywords = {gpl,license},
  file = {/Users/michelsen/Zotero/storage/YPUFYZAC/the-whys-and-hows-of-licensing-scientific-code.html}
}

@article{hueyREPLYSTEARNSACYNICAL,
  langid = {english},
  title = {{{REPLY TO STEARNS}}: {{SOME ACYNICAL ADVICE FOR GRADUATE STUDENTS}}},
  pages = {6},
  author = {Huey, Raymond B},
  file = {/Users/michelsen/Zotero/storage/DDPSCUVQ/Huey - REPLY TO STEARNS SOME ACYNICAL ADVICE FOR GRADUAT.pdf}
}

@article{hueyBecomingBetterScientist2011,
  title = {On {{Becoming}} a {{Better Scientist}}},
  volume = {57},
  issn = {1565-9801},
  url = {https://doi.org/10.1560/IJEE.57.4.293},
  doi = {10.1560/IJEE.57.4.293},
  abstract = {Good scientific research yields insights that are important and general. But the process of learning to do good science is far from simple, and the inherent challenges are often more motivational than scientific. I review various ways that may help scientists (especially young ones) to do better research. Perhaps the most important is to spend time with people who are smart, productive, and enjoy what they are doing: motivation and success are infectious. Trying some risky projects, for which success is not guaranteed, can enhance motivation. Before tackling risky projects, however, seek advice from those with experience; but make your own decision. Always be as self-directed as possible (and as political): actively seek opportunities and don't wait for them to come to you. If you have to learn a skill that is challenging or unpleasant, try to convince yourself that you look forward to learning it. Similarly, develop a high tolerance for repetitive tasks, which are inevitable components of science. In particular, learn to communicate well both in writing and in speaking: treat communication as a vital apprenticeship to be learned. Conflict is inevitable in science, but collaboration with opponents can be a positive way to resolve and grow beyond conflict. Staying fresh becomes a challenge as scientists age, but changing fields, continuing to go to seminars and meetings, and interacting with students and new colleagues can minimize one's personal fossilization.},
  number = {4},
  journaltitle = {Israel Journal of Ecology \& Evolution},
  urldate = {2019-01-02},
  date = {2011-01-01},
  pages = {293-307},
  keywords = {communication,good science,motivation},
  author = {Huey, Raymond B.},
  file = {/Users/michelsen/Zotero/storage/F3QWSFTN/Huey - 2011 - On Becoming a Better Scientist.pdf;/Users/michelsen/Zotero/storage/DCBZALRN/IJEE.57.4.html}
}

@online{WhyAreExponential,
  title = {Why Are Exponential Families so Awesome? - {{Quora}}},
  url = {https://www.quora.com/Why-are-exponential-families-so-awesome},
  urldate = {2019-01-02},
  keywords = {exponential families},
  file = {/Users/michelsen/Zotero/storage/HQUJVKBJ/Why-are-exponential-families-so-awesome.html}
}

@online{neeMonteCarloPower2018,
  title = {Monte {{Carlo Power Analysis}}},
  url = {http://deliveroo.engineering/2018/12/07/monte-carlo-power-analysis.html},
  abstract = {Take advantage of computing power and empirical data to use Monte Carlo simulation to perform experiment power analysis.},
  journaltitle = {deliveroo.engineering},
  urldate = {2019-01-02},
  date = {2018-12-07T00:00:00+00:00},
  keywords = {monte carlo,power analysis},
  author = {Nee, Daniel},
  file = {/Users/michelsen/Zotero/storage/UDHRQGDY/monte-carlo-power-analysis.html}
}

@online{ciszkowskiModernFunctionsPython2018,
  title = {Modern {{Functions}} in {{Python}} 3},
  url = {https://tech.gadventures.com/modern-functions-in-python-3-80208c44ce47},
  abstract = {Python has thrived over the past few decades as the language which lets you work quickly and effectively. Like many modern companies, we…},
  journaltitle = {G Adventures Technology},
  urldate = {2019-01-02},
  date = {2018-08-28T18:28:08.178Z},
  author = {Ciszkowski, Bartek},
  file = {/Users/michelsen/Zotero/storage/43HK6J8M/modern-functions-in-python-3-80208c44ce47.html}
}

@online{rosebrockHowUseKeras2018,
  langid = {american},
  title = {How to Use {{Keras}} Fit and Fit\_generator (a Hands-on Tutorial)},
  url = {https://www.pyimagesearch.com/2018/12/24/how-to-use-keras-fit-and-fit_generator-a-hands-on-tutorial/},
  abstract = {In this tutorial you will learn how the Keras .fit and .fit\_generator functions work, including the differences between them. I'll then show you how to implement your own custom Keras generator function.},
  journaltitle = {PyImageSearch},
  urldate = {2019-01-02},
  date = {2018-12-24T10:00:47-05:00},
  keywords = {fit,fit_generator,keras,train_on_batch},
  author = {Rosebrock, Adrian},
  file = {/Users/michelsen/Zotero/storage/CLKZB4M8/how-to-use-keras-fit-and-fit_generator-a-hands-on-tutorial.html}
}

@online{ahemadVersionControlJupyter2018,
  title = {Version {{Control}} with {{Jupyter Notebooks}}},
  url = {https://towardsdatascience.com/version-control-with-jupyter-notebooks-f096f4d7035a},
  abstract = {Version Control is a vital part data science workflows. Between multiple experiments it is essential to know what changed and which…},
  journaltitle = {Towards Data Science},
  urldate = {2019-01-02},
  date = {2018-10-08T16:15:40.848Z},
  keywords = {git,jupyter,notebook,version control},
  author = {Ahemad, Faizan},
  file = {/Users/michelsen/Zotero/storage/BZRRWHNZ/version-control-with-jupyter-notebooks-f096f4d7035a.html}
}

@online{serifovicOptimizingJupyterNotebooks2018,
  title = {Optimizing {{Jupyter Notebooks}} — {{A Comprehensive Guide}}},
  url = {https://towardsdatascience.com/speed-up-jupyter-notebooks-20716cbe2025},
  abstract = {Finding bottlenecks and increasing your speed performance by magnitudes with some tips I came along over the past year.},
  journaltitle = {Towards Data Science},
  urldate = {2019-01-02},
  date = {2018-12-31T23:04:50.747Z},
  author = {Serifovic, Muriz},
  file = {/Users/michelsen/Zotero/storage/J59QN249/speed-up-jupyter-notebooks-20716cbe2025.html}
}

@online{DockerHassleData,
  title = {Docker {{Without}} the {{Hassle}} – {{Towards Data Science}}},
  url = {https://towardsdatascience.com/docker-without-the-hassle-b98447caedd8},
  urldate = {2019-01-02},
  keywords = {docker},
  file = {/Users/michelsen/Zotero/storage/A3GTL5YX/docker-without-the-hassle-b98447caedd8.html}
}

@online{DistributedTensorFlowUsing,
  title = {Distributed {{TensorFlow}} Using {{Horovod}} – {{Towards Data Science}}},
  url = {https://towardsdatascience.com/distributed-tensorflow-using-horovod-6d572f8790c4},
  urldate = {2019-01-02},
  file = {/Users/michelsen/Zotero/storage/GRUEHM86/distributed-tensorflow-using-horovod-6d572f8790c4.html}
}

@online{torres.aiDistributedTensorFlowUsing2018,
  title = {Distributed {{TensorFlow}} Using {{Horovod}}},
  url = {https://towardsdatascience.com/distributed-tensorflow-using-horovod-6d572f8790c4},
  abstract = {Reduce training time for deep neural networks by using many GPUs},
  journaltitle = {Towards Data Science},
  urldate = {2019-01-02},
  date = {2018-12-17T21:44:57.753Z},
  keywords = {horovod,tensorflow},
  author = {TORRES.AI, Jordi},
  file = {/Users/michelsen/Zotero/storage/L4THJA35/distributed-tensorflow-using-horovod-6d572f8790c4.html}
}

@online{FullHardwareGuide2018,
  langid = {american},
  title = {A {{Full Hardware Guide}} to {{Deep Learning}}},
  url = {http://timdettmers.com/2018/12/16/deep-learning-hardware-guide/},
  abstract = {In this guide I analyse hardware from CPU to SSD and their impact on performance for deep learning so that you can choose the hardware that you really need.},
  journaltitle = {Tim Dettmers},
  urldate = {2019-01-02},
  date = {2018-12-16T18:25:41+00:00},
  keywords = {hardware},
  file = {/Users/michelsen/Zotero/storage/596H9QCS/deep-learning-hardware-guide.html}
}

@article{mohamedStatisticalViewDeep,
  langid = {english},
  title = {A {{Statistical View}} of {{Deep Learning}}},
  pages = {31},
  author = {Mohamed, Shakir},
  file = {/Users/michelsen/Zotero/storage/9WHUGQFA/Mohamed - A Statistical View of Deep Learning.pdf}
}

@online{datapredAdvancedCrossvalidationTips,
  langid = {english},
  title = {Advanced Cross-Validation Tips for Time Series},
  url = {https://www.datapred.com/blog/advanced-cross-validation-tips},
  abstract = {Advanced tips and practical examples for coding proper machine learning backtests.},
  urldate = {2019-01-02},
  author = {Datapred},
  file = {/Users/michelsen/Zotero/storage/ZPVZQCAD/advanced-cross-validation-tips.html}
}

@online{breddelsVaexOutCore2018,
  title = {Vaex: {{Out}} of {{Core Dataframes}} for {{Python}} and {{Fast Visualization}}},
  url = {https://towardsdatascience.com/vaex-out-of-core-dataframes-for-python-and-fast-visualization-12c102db044a},
  shorttitle = {Vaex},
  abstract = {1 billion row datasets on your laptop},
  journaltitle = {Towards Data Science},
  urldate = {2019-01-02},
  date = {2018-12-13T08:27:44.372Z},
  author = {Breddels, Maarten},
  file = {/Users/michelsen/Zotero/storage/SDTZ3KAA/vaex-out-of-core-dataframes-for-python-and-fast-visualization-12c102db044a.html}
}

@online{sarkarHandsonMachineLearning2018,
  title = {Hands-on {{Machine Learning Model Interpretation}}},
  url = {https://towardsdatascience.com/explainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608},
  abstract = {A comprehensive guide to interpreting machine learning models},
  journaltitle = {Towards Data Science},
  urldate = {2019-01-02},
  date = {2018-12-13T22:29:43.545Z},
  author = {Sarkar, Dipanjan (DJ)}
}

@article{karpatnePhysicsguidedNeuralNetworks2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.11431},
  primaryClass = {physics, stat},
  title = {Physics-Guided {{Neural Networks}} ({{PGNN}}): {{An Application}} in {{Lake Temperature Modeling}}},
  url = {http://arxiv.org/abs/1710.11431},
  shorttitle = {Physics-Guided {{Neural Networks}} ({{PGNN}})},
  abstract = {This paper introduces a novel framework for combining scientific knowledge of physics-based models with neural networks to advance scientific discovery. This framework, termed as physics-guided neural network (PGNN), leverages the output of physics-based model simulations along with observational features to generate predictions using a neural network architecture. Further, this paper presents a novel framework for using physics-based loss functions in the learning objective of neural networks, to ensure that the model predictions not only show lower errors on the training set but are also scientifically consistent with the known physics on the unlabeled set. We illustrate the effectiveness of PGNN for the problem of lake temperature modeling, where physical relationships between the temperature, density, and depth of water are used to design a physics-based loss function. By using scientific knowledge to guide the construction and learning of neural networks, we are able to show that the proposed framework ensures better generalizability as well as scientific consistency of results.},
  urldate = {2019-01-02},
  date = {2017-10-31},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Physics - Data Analysis; Statistics and Probability,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Artificial Intelligence,physics},
  author = {Karpatne, Anuj and Watkins, William and Read, Jordan and Kumar, Vipin},
  file = {/Users/michelsen/Zotero/storage/AYY6R7DH/Karpatne et al. - 2017 - Physics-guided Neural Networks (PGNN) An Applicat.pdf;/Users/michelsen/Zotero/storage/XRVGGRS7/1710.html}
}

@article{kimNewMetricAbsolute2016,
  langid = {english},
  title = {A New Metric of Absolute Percentage Error for Intermittent Demand Forecasts},
  volume = {32},
  issn = {01692070},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207016000121},
  doi = {10.1016/j.ijforecast.2015.12.003},
  abstract = {The mean absolute percentage error (MAPE) is one of the most widely used measures of forecast accuracy, due to its advantages of scale-independency and interpretability. However, MAPE has the significant disadvantage that it produces infinite or undefined values for zero or close-to-zero actual values. In order to address this issue in MAPE, we propose a new measure of forecast accuracy called the mean arctangent absolute percentage error (MAAPE). MAAPE has been developed through looking at MAPE from a different angle. In essence, MAAPE is a slope as an angle, while MAPE is a slope as a ratio, considering a triangle with adjacent and opposite sides that are equal to an actual value and the difference between the actual and forecast values, respectively. MAAPE inherently preserves the philosophy of MAPE, overcoming the problem of division by zero by using bounded influences for outliers in a fundamental manner through considering the ratio as an angle instead of a slope. The theoretical properties of MAAPE are investigated, and the practical advantages are demonstrated using both simulated and real-life data.},
  number = {3},
  journaltitle = {International Journal of Forecasting},
  urldate = {2019-01-02},
  date = {2016-07},
  pages = {669-679},
  keywords = {mape},
  author = {Kim, Sungil and Kim, Heeyoung},
  file = {/Users/michelsen/Zotero/storage/RBVLCZ3J/Kim and Kim - 2016 - A new metric of absolute percentage error for inte.pdf}
}

@online{CompleteGuideParameter2016,
  title = {Complete {{Guide}} to {{Parameter Tuning}} in {{XGBoost}} (with Codes in {{Python}})},
  url = {https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/},
  abstract = {This article explains the parameter tuning in xgboost model in python and takes a practice problem for practice in data science and analytics},
  journaltitle = {Analytics Vidhya},
  urldate = {2019-01-03},
  date = {2016-03-01T22:36:16+05:30},
  keywords = {xgb. xgboost},
  file = {/Users/michelsen/Zotero/storage/P43NIZFY/complete-guide-parameter-tuning-xgboost-with-codes-python.html}
}

@article{chaiRootMeanSquare2014,
  langid = {english},
  title = {Root Mean Square Error ({{RMSE}}) or Mean Absolute Error ({{MAE}})? – {{Arguments}} against Avoiding {{RMSE}} in the Literature},
  volume = {7},
  issn = {1991-9603},
  url = {https://www.geosci-model-dev.net/7/1247/2014/},
  doi = {10.5194/gmd-7-1247-2014},
  shorttitle = {Root Mean Square Error ({{RMSE}}) or Mean Absolute Error ({{MAE}})?},
  abstract = {Both the root mean square error (RMSE) and the mean absolute error (MAE) are regularly employed in model evaluation studies. Willmott and Matsuura (2005) have suggested that the RMSE is not a good indicator of average model performance and might be a misleading indicator of average error, and thus the MAE would be a better metric for that purpose. While some concerns over using RMSE raised by Willmott and Matsuura (2005) and Willmott et al. (2009) are valid, the proposed avoidance of RMSE in favor of MAE is not the solution. Citing the aforementioned papers, many researchers chose MAE over RMSE to present their model evaluation statistics when presenting or adding the RMSE measures could be more beneficial. In this technical note, we demonstrate that the RMSE is not ambiguous in its meaning, contrary to what was claimed by Willmott et al. (2009). The RMSE is more appropriate to represent model performance than the MAE when the error distribution is expected to be Gaussian. In addition, we show that the RMSE satisfies the triangle inequality requirement for a distance metric, whereas Willmott et al. (2009) indicated that the sums-ofsquares-based statistics do not satisfy this rule. In the end, we discussed some circumstances where using the RMSE will be more beneficial. However, we do not contend that the RMSE is superior over the MAE. Instead, a combination of metrics, including but certainly not limited to RMSEs and MAEs, are often required to assess model performance.},
  number = {3},
  journaltitle = {Geoscientific Model Development},
  urldate = {2019-01-03},
  date = {2014-06-30},
  pages = {1247-1250},
  keywords = {mae,rmse},
  author = {Chai, T. and Draxler, R. R.},
  file = {/Users/michelsen/Zotero/storage/M3R4ZYGQ/Chai and Draxler - 2014 - Root mean square error (RMSE) or mean absolute err.pdf}
}

@article{ramageHiddenMarkovModels2007,
  langid = {english},
  title = {Hidden {{Markov Models Fundamentals}}},
  journaltitle = {CS229 Section Notes},
  date = {2007-12-01},
  pages = {13},
  keywords = {hidden,hmm,markov},
  author = {Ramage, Daniel},
  file = {/Users/michelsen/Zotero/storage/QJEW5BQC/Ramage - Hidden Markov Models Fundamentals.pdf}
}

@article{sculleyMachineLearningHighInterest,
  langid = {english},
  title = {Machine {{Learning}}: {{The High}}-{{Interest Credit Card}} of {{Technical Debt}}},
  abstract = {Machine learning offers a fantastically powerful toolkit for building complex systems quickly. This paper argues that it is dangerous to think of these quick wins as coming for free. Using the framework of technical debt, we note that it is remarkably easy to incur massive ongoing maintenance costs at the system level when applying machine learning. The goal of this paper is highlight several machine learning specific risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns.},
  pages = {9},
  keywords = {technical debt},
  author = {Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael},
  file = {/Users/michelsen/Zotero/storage/QLZRUW7D/Sculley et al. - Machine Learning The High-Interest Credit Card of.pdf}
}

@online{koehrsenConceptualExplanationBayesian2018,
  title = {A {{Conceptual Explanation}} of {{Bayesian Hyperparameter Optimization}} for {{Machine Learning}}},
  url = {https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f},
  abstract = {The concepts behind efficient hyperparameter tuning using Bayesian optimization},
  journaltitle = {Towards Data Science},
  urldate = {2019-01-03},
  date = {2018-06-24T13:25:59.216Z},
  keywords = {bayesian},
  author = {Koehrsen, Will},
  file = {/Users/michelsen/Zotero/storage/PUQ72MDG/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learni.html}
}

@article{bottouOptimizationMethodsLargeScale2016a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.04838},
  primaryClass = {cs, math, stat},
  title = {Optimization {{Methods}} for {{Large}}-{{Scale Machine Learning}}},
  url = {http://arxiv.org/abs/1606.04838},
  abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
  urldate = {2019-01-03},
  date = {2016-06-15},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Mathematics - Optimization and Control,optimization,gradient descent},
  author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
  file = {/Users/michelsen/Zotero/storage/MG8LF7UP/Bottou et al. - 2016 - Optimization Methods for Large-Scale Machine Learn.pdf;/Users/michelsen/Zotero/storage/TGLKZ2Q8/1606.html}
}

@article{sculleyPACEPROGRESSEMPIRICAL2018,
  langid = {english},
  title = {{{ON PACE}}, {{PROGRESS}}, {{AND EMPIRICAL RIGOR}}},
  abstract = {The field of ML is distinguished both by rapid innovation and rapid dissemination of results. While the pace of progress has been extraordinary by any measure, in this paper we explore potential issues that we believe to be arising as a result. In particular, we observe that the rate of empirical advancement may not have been matched by consistent increase in the level of empirical rigor across the field as a whole. This short position paper highlights examples where progress has actually been slowed as a result, offers thoughts on incentive structures currently at play, and gives suggestions as seeds for discussions on productive change.},
  date = {2018},
  pages = {4},
  author = {Sculley, D and Snoek, Jasper and Rahimi, Ali and Wiltschko, Alex},
  file = {/Users/michelsen/Zotero/storage/KLAGC22H/Sculley et al. - 2018 - ON PACE, PROGRESS, AND EMPIRICAL RIGOR.pdf}
}

@article{kolterConvexOptimizationOverview,
  langid = {english},
  title = {Convex {{Optimization Overview}}},
  pages = {14},
  author = {Kolter, Zico},
  file = {/Users/michelsen/Zotero/storage/BLNPFJXW/Kolter - Convex Optimization Overview.pdf}
}

@article{vapnikRethinkingStatisticalLearning2018,
  langid = {english},
  title = {Rethinking Statistical Learning Theory: Learning Using Statistical Invariants},
  issn = {0885-6125, 1573-0565},
  url = {http://link.springer.com/10.1007/s10994-018-5742-0},
  doi = {10.1007/s10994-018-5742-0},
  shorttitle = {Rethinking Statistical Learning Theory},
  abstract = {This paper introduces a new learning paradigm, called Learning Using Statistical Invariants (LUSI), which is different from the classical one. In a classical paradigm, the learning machine constructs a classification rule that minimizes the probability of expected error; it is datadriven model of learning. In the LUSI paradigm, in order to construct the desired classification function, a learning machine computes statistical invariants that are specific for the problem, and then minimizes the expected error in a way that preserves these invariants; it is thus both data- and invariant-driven learning. From a mathematical point of view, methods of the classical paradigm employ mechanisms of strong convergence of approximations to the desired function, whereas methods of the new paradigm employ both strong and weak convergence mechanisms. This can significantly increase the rate of convergence.},
  journaltitle = {Machine Learning},
  urldate = {2019-01-03},
  date = {2018-07-18},
  author = {Vapnik, Vladimir and Izmailov, Rauf},
  file = {/Users/michelsen/Zotero/storage/U8FH55U6/Vapnik and Izmailov - 2018 - Rethinking statistical learning theory learning u.pdf}
}

@article{schwabCommentWhyDoes2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.03541},
  primaryClass = {cond-mat, stat},
  title = {Comment on "{{Why}} Does Deep and Cheap Learning Work so Well?" [{{arXiv}}:1608.08225]},
  url = {http://arxiv.org/abs/1609.03541},
  shorttitle = {Comment on "{{Why}} Does Deep and Cheap Learning Work so Well?},
  abstract = {In a recent paper, "Why does deep and cheap learning work so well?", Lin and Tegmark claim to show that the mapping between deep belief networks and the variational renormalization group derived in [arXiv:1410.3831] is invalid, and present a "counterexample" that claims to show that this mapping does not hold. In this comment, we show that these claims are incorrect and stem from a misunderstanding of the variational RG procedure proposed by Kadanoff. We also explain why the "counterexample" of Lin and Tegmark is compatible with the mapping proposed in [arXiv:1410.3831].},
  urldate = {2019-01-03},
  date = {2016-09-12},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks},
  author = {Schwab, David J. and Mehta, Pankaj},
  file = {/Users/michelsen/Zotero/storage/YXKAJZ4L/Schwab and Mehta - 2016 - Comment on Why does deep and cheap learning work .pdf;/Users/michelsen/Zotero/storage/9LGXKQL7/1609.html}
}

@article{makComparativePerformanceBGISEQ5002017,
  title = {Comparative Performance of the {{BGISEQ}}-500 vs {{Illumina HiSeq2500}} Sequencing Platforms for Palaeogenomic Sequencing},
  volume = {6},
  issn = {2047-217X},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5570000/},
  doi = {10.1093/gigascience/gix049},
  abstract = {Ancient DNA research has been revolutionized following development of next-generation sequencing platforms. Although a number of such platforms have been applied to ancient DNA samples, the Illumina series are the dominant choice today, mainly because of high production capacities and short read production. Recently a potentially attractive alternative platform for palaeogenomic data generation has been developed, the BGISEQ-500, whose sequence output are comparable with the Illumina series. In this study, we modified the standard BGISEQ-500 library preparation specifically for use on degraded DNA, then directly compared the sequencing performance and data quality of the BGISEQ-500 to the Illumina HiSeq2500 platform on DNA extracted from 8 historic and ancient dog and wolf samples. The data generated were largely comparable between sequencing platforms, with no statistically significant difference observed for parameters including level (P = 0.371) and average sequence length (P = 0718) of endogenous nuclear DNA, sequence GC content (P = 0.311), double-stranded DNA damage rate (v. 0.309), and sequence clonality (P = 0.093). Small significant differences were found in single-strand DNA damage rate (δS; slightly lower for the BGISEQ-500, P = 0.011) and the background rate of difference from the reference genome (θ; slightly higher for BGISEQ-500, P = 0.012). This may result from the differences in amplification cycles used to polymerase chain reaction–amplify the libraries. A significant difference was also observed in the mitochondrial DNA percentages recovered (P = 0.018), although we believe this is likely a stochastic effect relating to the extremely low levels of mitochondria that were sequenced from 3 of the samples with overall very low levels of endogenous DNA. Although we acknowledge that our analyses were limited to animal material, our observations suggest that the BGISEQ-500 holds the potential to represent a valid and potentially valuable alternative platform for palaeogenomic data generation that is worthy of future exploration by those interested in the sequencing and analysis of degraded DNA.},
  number = {8},
  journaltitle = {GigaScience},
  shortjournal = {Gigascience},
  urldate = {2019-01-03},
  date = {2017-06-26},
  pages = {1-13},
  author = {Mak, Sarah Siu Tze and Gopalakrishnan, Shyam and Carøe, Christian and Geng, Chunyu and Liu, Shanlin and Sinding, Mikkel-Holger S and Kuderna, Lukas F K and Zhang, Wenwei and Fu, Shujin and Vieira, Filipe G and Germonpré, Mietje and Bocherens, Hervé and Fedorov, Sergey and Petersen, Bent and Sicheritz-Pontén, Thomas and Marques-Bonet, Tomas and Zhang, Guojie and Jiang, Hui and Gilbert, M Thomas P},
  file = {/Users/michelsen/Zotero/storage/X3XQ9ILX/Mak et al. - 2017 - Comparative performance of the BGISEQ-500 vs Illum.pdf},
  eprinttype = {pmid},
  eprint = {28854615},
  pmcid = {PMC5570000}
}

@article{yoonHiddenMarkovModels2009,
  title = {Hidden {{Markov Models}} and Their {{Applications}} in {{Biological Sequence Analysis}}},
  volume = {10},
  issn = {1389-2029},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2766791/},
  doi = {10.2174/138920209789177575},
  abstract = {Hidden Markov models (HMMs) have been extensively used in biological sequence analysis. In this paper, we give a tutorial review of HMMs and their applications in a variety of problems in molecular biology. We especially focus on three types of HMMs: the profile-HMMs, pair-HMMs, and context-sensitive HMMs. We show how these HMMs can be used to solve various sequence analysis problems, such as pairwise and multiple sequence alignments, gene annotation, classification, similarity search, and many others.},
  number = {6},
  journaltitle = {Current Genomics},
  shortjournal = {Curr Genomics},
  urldate = {2019-01-03},
  date = {2009-09},
  pages = {402-415},
  author = {Yoon, Byung-Jun},
  file = {/Users/michelsen/Zotero/storage/ZCIR5LBL/Yoon - 2009 - Hidden Markov Models and their Applications in Bio.pdf},
  eprinttype = {pmid},
  eprint = {20190955},
  pmcid = {PMC2766791}
}

@article{flagelUnreasonableEffectivenessConvolutional2018,
  langid = {english},
  title = {The {{Unreasonable Effectiveness}} of {{Convolutional Neural Networks}} in {{Population Genetic Inference}}},
  url = {http://biorxiv.org/lookup/doi/10.1101/336073},
  doi = {10.1101/336073},
  abstract = {Population-scale genomic datasets have given researchers incredible amounts of information from which to infer evolutionary histories. Concomitant with this flood of data, theoretical and methodological advances have sought to extract information from genomic sequences to infer demographic events such as population size changes and gene flow among closely related populations/species, construct recombination maps, and uncover loci underlying recent adaptation. To date most methods make use of only one or a few summaries of the input sequences and therefore ignore potentially useful information encoded in the data. The most sophisticated of these approaches involve likelihood calculations, which require theoretical advances for each new problem, and often focus on a single aspect of the data (e.g. only allele frequency information) in the interest of mathematical and computational tractability. Directly interrogating the entirety of the input sequence data in a likelihood-free manner would thus offer a fruitful alternative. Here we accomplish this by representing DNA sequence alignments as images and using a class of deep learning methods called convolutional neural networks (CNNs) to make population genetic inferences from these images. We apply CNNs to a number of evolutionary questions and find that they frequently match or exceed the accuracy of current methods. Importantly, we show that CNNs perform accurate evolutionary model selection and parameter estimation, even on problems that have not received detailed theoretical treatments. Thus, when applied to population genetic alignments, CNN are capable of outperforming expert-derived statistical methods, and offer a new path forward in cases where no likelihood approach exists.},
  journaltitle = {bioRxiv},
  urldate = {2019-01-03},
  date = {2018-11-27},
  author = {Flagel, Lex and Brandvain, Yaniv J and Schrider, Daniel R},
  file = {/Users/michelsen/Zotero/storage/74GZM445/Flagel et al. - 2018 - The Unreasonable Effectiveness of Convolutional Ne.pdf}
}

@article{schriderSupervisedMachineLearning2018,
  title = {Supervised {{Machine Learning}} for {{Population Genetics}}: {{A New Paradigm}}},
  volume = {34},
  issn = {0168-9525},
  url = {http://www.sciencedirect.com/science/article/pii/S0168952517302251},
  doi = {10.1016/j.tig.2017.12.005},
  shorttitle = {Supervised {{Machine Learning}} for {{Population Genetics}}},
  abstract = {As population genomic datasets grow in size, researchers are faced with the daunting task of making sense of a flood of information. To keep pace with this explosion of data, computational methodologies for population genetic inference are rapidly being developed to best utilize genomic sequence data. In this review we discuss a new paradigm that has emerged in computational population genomics: that of supervised machine learning (ML). We review the fundamentals of ML, discuss recent applications of supervised ML to population genetics that outperform competing methods, and describe promising future directions in this area. Ultimately, we argue that supervised ML is an important and underutilized tool that has considerable potential for the world of evolutionary genomics.},
  number = {4},
  journaltitle = {Trends in Genetics},
  shortjournal = {Trends in Genetics},
  urldate = {2019-01-03},
  date = {2018-04-01},
  pages = {301-312},
  author = {Schrider, Daniel R. and Kern, Andrew D.},
  file = {/Users/michelsen/Zotero/storage/Q6SCUPCV/Schrider and Kern - 2018 - Supervised Machine Learning for Population Genetic.pdf;/Users/michelsen/Zotero/storage/FYU4DRC2/S0168952517302251.html}
}

@article{sheehanDeepLearningPopulation2016,
  langid = {english},
  title = {Deep {{Learning}} for {{Population Genetic Inference}}},
  volume = {12},
  issn = {1553-7358},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004845},
  doi = {10.1371/journal.pcbi.1004845},
  abstract = {Given genomic variation data from multiple individuals, computing the likelihood of complex population genetic models is often infeasible. To circumvent this problem, we introduce a novel likelihood-free inference framework by applying deep learning, a powerful modern technique in machine learning. Deep learning makes use of multilayer neural networks to learn a feature-based function from the input (e.g., hundreds of correlated summary statistics of data) to the output (e.g., population genetic parameters of interest). We demonstrate that deep learning can be effectively employed for population genetic inference and learning informative features of data. As a concrete application, we focus on the challenging problem of jointly inferring natural selection and demography (in the form of a population size change history). Our method is able to separate the global nature of demography from the local nature of selection, without sequential steps for these two factors. Studying demography and selection jointly is motivated by Drosophila, where pervasive selection confounds demographic analysis. We apply our method to 197 African Drosophila melanogaster genomes from Zambia to infer both their overall demography, and regions of their genome under selection. We find many regions of the genome that have experienced hard sweeps, and fewer under selection on standing variation (soft sweep) or balancing selection. Interestingly, we find that soft sweeps and balancing selection occur more frequently closer to the centromere of each chromosome. In addition, our demographic inference suggests that previously estimated bottlenecks for African Drosophila melanogaster are too extreme.},
  number = {3},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2019-01-03},
  date = {2016-03-28},
  pages = {e1004845},
  keywords = {Drosophila melanogaster,Effective population size,Genomics statistics,Invertebrate genomics,Natural selection,Neural networks,Population genetics,Population size},
  author = {Sheehan, Sara and Song, Yun S.},
  file = {/Users/michelsen/Zotero/storage/PE2B8UNQ/Sheehan and Song - 2016 - Deep Learning for Population Genetic Inference.pdf;/Users/michelsen/Zotero/storage/Z8WHRA3J/article.html}
}

@thesis{al-nakeebMachineLearningTools2017,
  location = {{Department of Bio and Health Informatics}},
  title = {Machine {{Learning Tools}} for {{DNA Sequence Analysis}}},
  institution = {{Technical University of Copenhagen}},
  type = {PhD Thesis},
  date = {2017-12},
  author = {Al-Nakeeb, Kosai}
}

@article{nielsenTracingPeoplingWorld2017,
  langid = {english},
  title = {Tracing the Peopling of the World through Genomics},
  volume = {541},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/nature21347},
  doi = {10.1038/nature21347},
  abstract = {Advances in the sequencing and the analysis of the genomes of both modern and ancient peoples have facilitated a number of breakthroughs in our understanding of human evolutionary history. These include the discovery of interbreeding between anatomically modern humans and extinct hominins; the development of an increasingly detailed description of the complex dispersal of modern humans out of Africa and their population expansion worldwide; and the characterization of many of the genetic adaptions of humans to local environmental conditions. Our interpretation of the evolutionary history and adaptation of humans is being transformed by analyses of these new genomic data.},
  number = {7637},
  journaltitle = {Nature},
  urldate = {2019-01-03},
  date = {2017-01},
  pages = {302-310},
  author = {Nielsen, Rasmus and Akey, Joshua M. and Jakobsson, Mattias and Pritchard, Jonathan K. and Tishkoff, Sarah and Willerslev, Eske},
  file = {/Users/michelsen/Zotero/storage/MQ5Z6BM8/Nielsen et al. - 2017 - Tracing the peopling of the world through genomics.pdf;/Users/michelsen/Zotero/storage/K8I2HX53/nature21347.html}
}

@article{suDetectionIdentityDescent2012,
  title = {Detection of Identity by Descent Using Next-Generation Whole Genome Sequencing Data},
  volume = {13},
  issn = {1471-2105},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3403908/},
  doi = {10.1186/1471-2105-13-121},
  abstract = {Background
Identity by descent (IBD) has played a fundamental role in the discovery of genetic loci underlying human diseases. Both pedigree-based and population-based linkage analyses rely on estimating recent IBD, and evidence of ancient IBD can be used to detect population structure in genetic association studies. Various methods for detecting IBD, including those implemented in the soft- ware programs fastIBD and GERMLINE, have been developed in the past several years using population genotype data from microarray platforms. Now, next-generation DNA sequencing data is becoming increasingly available, enabling the comprehensive analysis of genomes, in- cluding identifying rare variants. These sequencing data may provide an opportunity to detect IBD with higher resolution than previously possible, potentially enabling the detection of disease causing loci that were previously undetectable with sparser genetic data.

Results
Here, we investigate how different levels of variant coverage in sequencing and microarray genotype data influences the resolution at which IBD can be detected. This includes microarray genotype data from the WTCCC study, denser genotype data from the HapMap Project, low coverage sequencing data from the 1000 Genomes Project, and deep coverage complete genome data from our own projects. With high power (78\%), we can detect segments of length 0.4 cM or larger using fastIBD and GERMLINE in sequencing data. This compares to similar power to detect segments of length 1.0 cM or higher with microarray genotype data. We find that GERMLINE has slightly higher power than fastIBD for detecting IBD segments using sequencing data, but also has a much higher false positive rate.

Conclusion
We further quantify the effect of variant density, conditional on genetic map length, on the power to resolve IBD segments. These investigations into IBD resolution may help guide the design of future next generation sequencing studies that utilize IBD, including family-based association studies, association studies in admixed populations, and homozygosity mapping studies.},
  journaltitle = {BMC Bioinformatics},
  shortjournal = {BMC Bioinformatics},
  urldate = {2019-01-03},
  date = {2012-06-06},
  pages = {121},
  author = {Su, Shu-Yi and Kasberger, Jay and Baranzini, Sergio and Byerley, William and Liao, Wilson and Oksenberg, Jorge and Sherr, Elliott and Jorgenson, Eric},
  file = {/Users/michelsen/Zotero/storage/HXAXM6CV/Su et al. - 2012 - Detection of identity by descent using next-genera.pdf},
  eprinttype = {pmid},
  eprint = {22672699},
  pmcid = {PMC3403908}
}

@article{scargleSTUDIESASTRONOMICALTIME2013,
  langid = {english},
  title = {{{STUDIES IN ASTRONOMICAL TIME SERIES ANALYSIS}}. {{VI}}. {{BAYESIAN BLOCK REPRESENTATIONS}}},
  volume = {764},
  issn = {0004-637X, 1538-4357},
  url = {http://stacks.iop.org/0004-637X/764/i=2/a=167?key=crossref.0539dc6f37f29e250567031865ebbe9a},
  doi = {10.1088/0004-637X/764/2/167},
  number = {2},
  journaltitle = {The Astrophysical Journal},
  urldate = {2019-01-03},
  date = {2013-02-04},
  pages = {167},
  author = {Scargle, Jeffrey D. and Norris, Jay P. and Jackson, Brad and Chiang, James},
  file = {/Users/michelsen/Zotero/storage/4A3PNMCM/Scargle et al. - 2013 - STUDIES IN ASTRONOMICAL TIME SERIES ANALYSIS. VI. .pdf}
}

@article{scargleStudiesAstronomicalTime2013,
  langid = {english},
  title = {Studies in {{Astronomical Time Series Analysis}}. {{VI}}. {{Bayesian Block Representations}}},
  volume = {764},
  issn = {0004-637X},
  url = {http://stacks.iop.org/0004-637X/764/i=2/a=167},
  doi = {10.1088/0004-637X/764/2/167},
  abstract = {This paper addresses the problem of detecting and characterizing local variability in time series and other forms of sequential data. The goal is to identify and characterize statistically significant variations, at the same time suppressing the inevitable corrupting observational errors. We present a simple nonparametric modeling technique and an algorithm implementing it—an improved and generalized version of Bayesian Blocks —that finds the optimal segmentation of the data in the observation interval. The structure of the algorithm allows it to be used in either a real-time trigger mode, or a retrospective mode. Maximum likelihood or marginal posterior functions to measure model fitness are presented for events, binned counts, and measurements at arbitrary times with known error distributions. Problems addressed include those connected with data gaps, variable exposure, extension to piecewise linear and piecewise exponential representations, multivariate time series data, analysis of variance, data on the circle, other data modes, and dispersed data. Simulations provide evidence that the detection efficiency for weak signals is close to a theoretical asymptotic limit derived by Arias-Castro et al. In the spirit of Reproducible Research all of the code and data necessary to reproduce all of the figures in this paper are included as supplementary material.},
  number = {2},
  journaltitle = {The Astrophysical Journal},
  shortjournal = {ApJ},
  urldate = {2019-01-03},
  date = {2013},
  pages = {167},
  keywords = {bayesian blocks,blocks},
  author = {Scargle, Jeffrey D. and Norris, Jay P. and Jackson, Brad and Chiang, James},
  file = {/Users/michelsen/Zotero/storage/4B2I2GI5/Scargle et al. - 2013 - Studies in Astronomical Time Series Analysis. VI. .pdf}
}

@article{liInferenceHumanPopulation2011,
  langid = {english},
  title = {Inference of Human Population History from Individual Whole-Genome Sequences},
  volume = {475},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/nature10231},
  doi = {10.1038/nature10231},
  abstract = {The history of human population size is important for understanding human evolution. Various studies1,2,3,4,5 have found evidence for a founder event (bottleneck) in East Asian and European populations, associated with the human dispersal out-of-Africa event around 60 thousand years (kyr) ago. However, these studies have had to assume simplified demographic models with few parameters, and they do not provide a precise date for the start and stop times of the bottleneck. Here, with fewer assumptions on population size changes, we present a more detailed history of human population sizes between approximately ten thousand and a million years ago, using the pairwise sequentially Markovian coalescent model applied to the complete diploid genome sequences of a Chinese male (YH)6, a Korean male (SJK)7, three European individuals (J. C. Venter8, NA12891 and NA12878 (ref. 9)) and two Yoruba males (NA18507 (ref. 10) and NA19239). We infer that European and Chinese populations had very similar population-size histories before 10–20 kyr ago. Both populations experienced a severe bottleneck 10–60 kyr ago, whereas African populations experienced a milder bottleneck from which they recovered earlier. All three populations have an elevated effective population size between 60 and 250 kyr ago, possibly due to population substructure11. We also infer that the differentiation of genetically modern humans may have started as early as 100–120 kyr ago12, but considerable genetic exchanges may still have occurred until 20–40 kyr ago.},
  number = {7357},
  journaltitle = {Nature},
  urldate = {2019-01-03},
  date = {2011-07},
  pages = {493-496},
  author = {Li, Heng and Durbin, Richard},
  file = {/Users/michelsen/Zotero/storage/F6P22Y3N/Li and Durbin - 2011 - Inference of human population history from individ.pdf;/Users/michelsen/Zotero/storage/LLCR4JBW/nature10231.html}
}

@online{veenNeuralNetworkZoo2016,
  langid = {american},
  title = {The {{Neural Network Zoo}}},
  url = {http://www.asimovinstitute.org/neural-network-zoo/},
  abstract = {With new neural network~architectures popping up every now and then, it’s hard to keep track of them all. Knowing all the abbreviations being thrown around (DCIGN, BiLSTM, DCGAN, anyone?) can be a bit overwhelming at first. So I decided to compose a cheat sheet containing~many of those~architectures. Most of these~are neural networks, some are completely …},
  journaltitle = {The Asimov Institute},
  urldate = {2019-01-03},
  date = {2016-09-14T10:31:14+00:00},
  author = {van Veen, Fjodor},
  file = {/Users/michelsen/Zotero/storage/7EBMLM6Y/neural-network-zoo.html}
}

@article{bakerClarificationUseCHIsquare1984,
  title = {Clarification of the Use of {{CHI}}-Square and Likelihood Functions in Fits to Histograms},
  volume = {221},
  issn = {0167-5087},
  url = {http://www.sciencedirect.com/science/article/pii/0167508784900164},
  doi = {10.1016/0167-5087(84)90016-4},
  abstract = {We consider the problem of fitting curves to histograms in which the data obey multinomial or Poisson statistics. Techniques commonly used by physicists are examined in light of standard results found in the statistics literature. We review the relationship between multinomial and Poisson distributions, and clarify a sufficient condition for equality of the area under the fitted curve and the number of events on the histogram. Following the statisticians, we use the likelihood ratio test to construct a general χ2 statistic, χλ2, which yields parameter and error estimates identical to those of the method of maximum likelihood. The χλ2 statist further useful for testing goodness-of-fit since the value of its minimum asymptotically obeys a classical chi-square distribution. One should be aware, however, of the potential for statistical bias, especially when the number of events is small.},
  number = {2},
  journaltitle = {Nuclear Instruments and Methods in Physics Research},
  shortjournal = {Nuclear Instruments and Methods in Physics Research},
  urldate = {2019-01-03},
  date = {1984-04-01},
  pages = {437-442},
  author = {Baker, Steve and Cousins, Robert D.},
  file = {/Users/michelsen/Zotero/storage/N3MVJ4K4/Baker and Cousins - 1984 - Clarification of the use of CHI-square and likelih.pdf;/Users/michelsen/Zotero/storage/V7252RPQ/0167508784900164.html}
}

@article{vanderplasFrequentismBayesianismPythondriven2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.5018},
  primaryClass = {astro-ph},
  title = {Frequentism and {{Bayesianism}}: {{A Python}}-Driven {{Primer}}},
  url = {http://arxiv.org/abs/1411.5018},
  shorttitle = {Frequentism and {{Bayesianism}}},
  abstract = {This paper presents a brief, semi-technical comparison of the essential features of the frequentist and Bayesian approaches to statistical inference, with several illustrative examples implemented in Python. The differences between frequentism and Bayesianism fundamentally stem from differing definitions of probability, a philosophical divide which leads to distinct approaches to the solution of statistical problems as well as contrasting ways of asking and answering questions about unknown parameters. After an example-driven discussion of these differences, we briefly compare several leading Python statistical packages which implement frequentist inference using classical methods and Bayesian inference using Markov Chain Monte Carlo.},
  urldate = {2019-01-03},
  date = {2014-11-18},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
  author = {VanderPlas, Jake},
  file = {/Users/michelsen/Zotero/storage/AGJKTGSF/VanderPlas - 2014 - Frequentism and Bayesianism A Python-driven Prime.pdf;/Users/michelsen/Zotero/storage/9RWVMQ6S/1411.html}
}

@article{salvatierProbabilisticProgrammingPython2016,
  langid = {english},
  title = {Probabilistic Programming in {{Python}} Using {{PyMC3}}},
  volume = {2},
  issn = {2376-5992},
  url = {https://peerj.com/articles/cs-55},
  doi = {10.7717/peerj-cs.55},
  abstract = {Probabilistic programming allows for automatic Bayesian inference on user-defined probabilistic models. Recent advances in Markov chain Monte Carlo (MCMC) sampling allow inference on increasingly complex models. This class of MCMC, known as Hamiltonian Monte Carlo, requires gradient information which is often not readily available. PyMC3 is a new open source probabilistic programming framework written in Python that uses Theano to compute gradients via automatic differentiation as well as compile probabilistic programs on-the-fly to C for increased speed. Contrary to other probabilistic programming languages, PyMC3 allows model specification directly in Python code. The lack of a domain specific language allows for great flexibility and direct interaction with the model. This paper is a tutorial-style introduction to this software package.},
  journaltitle = {PeerJ Computer Science},
  shortjournal = {PeerJ Comput. Sci.},
  urldate = {2019-01-03},
  date = {2016-04-06},
  pages = {e55},
  author = {Salvatier, John and Wiecki, Thomas V. and Fonnesbeck, Christopher},
  file = {/Users/michelsen/Zotero/storage/KYM98TKH/Salvatier et al. - 2016 - Probabilistic programming in Python using PyMC3.pdf;/Users/michelsen/Zotero/storage/88BQK4XS/cs-55.html}
}

@article{korneliussenANGSDAnalysisNext2014,
  title = {{{ANGSD}}: {{Analysis}} of {{Next Generation Sequencing Data}}},
  volume = {15},
  issn = {1471-2105},
  url = {https://doi.org/10.1186/s12859-014-0356-4},
  doi = {10.1186/s12859-014-0356-4},
  shorttitle = {{{ANGSD}}},
  abstract = {High-throughput DNA sequencing technologies are generating vast amounts of data. Fast, flexible and memory efficient implementations are needed in order to facilitate analyses of thousands of samples simultaneously.},
  number = {1},
  journaltitle = {BMC Bioinformatics},
  shortjournal = {BMC Bioinformatics},
  urldate = {2019-01-03},
  date = {2014-11-25},
  pages = {356},
  author = {Korneliussen, Thorfinn Sand and Albrechtsen, Anders and Nielsen, Rasmus},
  file = {/Users/michelsen/Zotero/storage/RVY8WNI4/Korneliussen et al. - 2014 - ANGSD Analysis of Next Generation Sequencing Data.pdf;/Users/michelsen/Zotero/storage/4QCYKUQB/s12859-014-0356-4.html}
}

@online{hviidWorkingPaperRegional2017,
  title = {Working {{Paper}}: {{A}} Regional Model of the {{Danish}} Housing Market},
  url = {http://www.nationalbanken.dk/en/publications/Pages/2017/11/Working-Paper-A-regional-model-of-the-Danish-housing-market.aspx},
  urldate = {2019-01-03},
  date = {2017-11-09},
  author = {Hviid, Juul, Simon},
  file = {/Users/michelsen/Zotero/storage/8IXI8BXR/WorkingPaper_nr121.pdf;/Users/michelsen/Zotero/storage/JYG65RDG/Working-Paper-A-regional-model-of-the-Danish-housing-market.html}
}

@article{slatkinStatisticalMethodsAnalyzing2016a,
  title = {Statistical Methods for Analyzing Ancient {{DNA}} from Hominins},
  volume = {41},
  issn = {0959-437X},
  url = {http://www.sciencedirect.com/science/article/pii/S0959437X16301083},
  doi = {10.1016/j.gde.2016.08.004},
  abstract = {In the past few years, the number of autosomal DNA sequences from human fossils has grown explosively and numerous partial or complete sequences are available from our closest relatives, Neanderthal and Denisovans. I review commonly used statistical methods applied to these sequences. These methods fall into three broad classes: methods for estimating levels of contamination, descriptive methods, and methods based on population genetic models. The latter two classes are largely methods developed for the analysis of present-day genomic data. When they are applied to ancient DNA (aDNA), they usually ignore the time dimension. A few methods, particularly those concerned with inferring something about selection or ancestor–descendant relationships, take explicit account of the ages of aDNA samples.},
  journaltitle = {Current Opinion in Genetics \& Development},
  shortjournal = {Current Opinion in Genetics \& Development},
  series = {Genetics of Human Origin},
  urldate = {2019-01-03},
  date = {2016-12-01},
  pages = {72-76},
  author = {Slatkin, Montgomery},
  file = {/Users/michelsen/Zotero/storage/PDHNVEMZ/Slatkin - 2016 - Statistical methods for analyzing ancient DNA from.pdf;/Users/michelsen/Zotero/storage/NYGSH49N/S0959437X16301083.html}
}

@article{spencerAuthenticityAncientDNAResults2004,
  title = {Authenticity of {{Ancient}}-{{DNA Results}}: {{A Statistical Approach}}},
  volume = {75},
  issn = {0002-9297},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1216058/},
  shorttitle = {Authenticity of {{Ancient}}-{{DNA Results}}},
  abstract = {Although there have been several papers recommending appropriate experimental designs for ancient-DNA studies, there have been few attempts at statistical analysis. We assume that we cannot decide whether a result is authentic simply by examining the sequence (e.g., when working with humans and domestic animals). We use a maximum-likelihood approach to estimate the probability that a positive result from a sample is (either partly or entirely) an amplification of DNA that was present in the sample before the experiment began. Our method is useful in two situations. First, we can decide in advance how many samples will be needed to achieve a given level of confidence. For example, to be almost certain (95\% confidence interval 0.96–1.00, maximum-likelihood estimate 1.00) that a positive result comes, at least in part, from DNA present before the experiment began, we need to analyze at least five samples and controls, even if all samples and no negative controls yield positive results. Second, we can decide how much confidence to place in results that have been obtained already, whether or not there are positive results from some controls. For example, the risk that at least one negative control yields a positive result increases with the size of the experiment, but the effects of occasional contamination are less severe in large experiments.},
  number = {2},
  journaltitle = {American Journal of Human Genetics},
  shortjournal = {Am J Hum Genet},
  urldate = {2019-01-03},
  date = {2004-08},
  pages = {240-250},
  author = {Spencer, Matthew and Howe, Christopher J.},
  file = {/Users/michelsen/Zotero/storage/QSS4IU3A/Spencer and Howe - 2004 - Authenticity of Ancient-DNA Results A Statistical.pdf},
  eprinttype = {pmid},
  eprint = {15199524},
  pmcid = {PMC1216058}
}

@thesis{nielsenTreeBoostingXGBoost2016a,
  langid = {english},
  location = {{Department of Mathematical Sciences}},
  title = {Tree {{Boosting With XGBoost}}},
  institution = {{Norwegian University of Science and Technology}},
  type = {Master Thesis},
  date = {2016-12},
  author = {Nielsen, Didrik},
  file = {/Users/michelsen/Zotero/storage/RQPZS3I9/Nielsen - Tree Boosting With XGBoost.pdf}
}

@article{precheltEarlyStoppingWhen,
  langid = {english},
  title = {Early {{Stopping}} | but When?},
  abstract = {Validation can be used to detect when over tting starts during supervised training of a neural network; training is then stopped before convergence to avoid the over tting   early stopping" . The exact criterion used for validation-based early stopping, however, is usually chosen in an ad-hoc fashion or training is stopped interactively. This trick describes how to select a stopping criterion in a systematic fashion; it is a trick for either speeding learning procedures or improving generalization, whichever is more important in the particular situation. An empirical investigation on multi-layer perceptrons shows that there exists a tradeo between training time and generalization: From the given mix of 1296 training runs using di erent 12 problems and 24 di erent network architectures I conclude slower stopping criteria allow for small improvements in generalization  here: about 4  on average , but cost much more training time  here: about factor 4 longer on average .},
  pages = {15},
  keywords = {early stopping},
  author = {Prechelt, Lutz},
  file = {/Users/michelsen/Zotero/storage/MNDYUW9T/Prechelt - Early Stopping  but when.pdf}
}

@article{varmaBiasErrorEstimation2006,
  title = {Bias in Error Estimation When Using Cross-Validation for Model Selection},
  volume = {7},
  issn = {1471-2105},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1397873/},
  doi = {10.1186/1471-2105-7-91},
  abstract = {Background
Cross-validation (CV) is an effective method for estimating the prediction error of a classifier. Some recent articles have proposed methods for optimizing classifiers by choosing classifier parameter values that minimize the CV error estimate. We have evaluated the validity of using the CV error estimate of the optimized classifier as an estimate of the true error expected on independent data.

Results
We used CV to optimize the classification parameters for two kinds of classifiers; Shrunken Centroids and Support Vector Machines (SVM). Random training datasets were created, with no difference in the distribution of the features between the two classes. Using these "null" datasets, we selected classifier parameter values that minimized the CV error estimate. 10-fold CV was used for Shrunken Centroids while Leave-One-Out-CV (LOOCV) was used for the SVM. Independent test data was created to estimate the true error. With "null" and "non null" (with differential expression between the classes) data, we also tested a nested CV procedure, where an inner CV loop is used to perform the tuning of the parameters while an outer CV is used to compute an estimate of the error., The CV error estimate for the classifier with the optimal parameters was found to be a substantially biased estimate of the true error that the classifier would incur on independent data. Even though there is no real difference between the two classes for the "null" datasets, the CV error estimate for the Shrunken Centroid with the optimal parameters was less than 30\% on 18.5\% of simulated training data-sets. For SVM with optimal parameters the estimated error rate was less than 30\% on 38\% of "null" data-sets. Performance of the optimized classifiers on the independent test set was no better than chance., The nested CV procedure reduces the bias considerably and gives an estimate of the error that is very close to that obtained on the independent testing set for both Shrunken Centroids and SVM classifiers for "null" and "non-null" data distributions.

Conclusion
We show that using CV to compute an error estimate for a classifier that has itself been tuned using CV gives a significantly biased estimate of the true error. Proper use of CV for estimating true error of a classifier developed using a well defined algorithm requires that all steps of the algorithm, including classifier parameter tuning, be repeated in each CV loop. A nested CV procedure provides an almost unbiased estimate of the true error.},
  journaltitle = {BMC Bioinformatics},
  shortjournal = {BMC Bioinformatics},
  urldate = {2019-01-03},
  date = {2006-02-23},
  pages = {91},
  author = {Varma, Sudhir and Simon, Richard},
  file = {/Users/michelsen/Zotero/storage/UPSI3VDS/Varma and Simon - 2006 - Bias in error estimation when using cross-validati.pdf},
  eprinttype = {pmid},
  eprint = {16504092},
  pmcid = {PMC1397873}
}

@online{cochraneTimeSeriesNested2018,
  title = {Time {{Series Nested Cross}}-{{Validation}}},
  url = {https://towardsdatascience.com/time-series-nested-cross-validation-76adba623eb9},
  abstract = {This blog post discusses the pitfalls of using traditional cross-validation with time series data. Specifically, we address 1) splitting a…},
  journaltitle = {Towards Data Science},
  urldate = {2019-01-03},
  date = {2018-05-19T02:14:00.938Z},
  author = {Cochrane, Courtney},
  file = {/Users/michelsen/Zotero/storage/RQJVKDDC/time-series-nested-cross-validation-76adba623eb9.html}
}

@article{tashmanOutofsampleTestsForecasting2000,
  langid = {english},
  title = {Out-of-Sample Tests of Forecasting Accuracy: An Analysis and Review},
  volume = {16},
  issn = {01692070},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0169207000000650},
  doi = {10.1016/S0169-2070(00)00065-0},
  shorttitle = {Out-of-Sample Tests of Forecasting Accuracy},
  abstract = {In evaluations of forecasting accuracy, including forecasting competitions, researchers have paid attention to the selection of time series and to the appropriateness of forecast-error measures. However, they have not formally analyzed choices in the implementation of out-of-sample tests, making it difficult to replicate and compare forecasting accuracy studies. In this paper, I (1) explain the structure of out-of-sample tests, (2) provide guidelines for implementing these tests, and (3) evaluate the adequacy of out-of-sample tests in forecasting software. The issues examined include series-splitting rules, fixed versus rolling origins, updating versus recalibration of model coefficients, fixed versus rolling windows, single versus multiple test periods, diversification through multiple time series, and design characteristics of forecasting competitions. For individual time series, the efficiency and reliability of out-of-sample tests can be improved by employing rolling-origin evaluations, recalibrating coefficients, and using multiple test periods. The results of forecasting competitions would be more generalizable if based upon precisely described groups of time series, in which the series are homogeneous within group and heterogeneous between groups. Few forecasting software programs adequately implement out-of-sample evaluations, especially general statistical packages and spreadsheet add-ins. © 2000 International Institute of Forecasters. Published by Elsevier Science B.V. All rights reserved.},
  number = {4},
  journaltitle = {International Journal of Forecasting},
  urldate = {2019-01-03},
  date = {2000-10},
  pages = {437-450},
  author = {Tashman, Leonard J.},
  file = {/Users/michelsen/Zotero/storage/Q9R89YLB/Tashman - 2000 - Out-of-sample tests of forecasting accuracy an an.pdf}
}

@article{tashmanOutofsampleTestsForecasting2000a,
  title = {Out-of-Sample Tests of Forecasting Accuracy: An Analysis and Review},
  volume = {16},
  issn = {0169-2070},
  url = {http://www.sciencedirect.com/science/article/pii/S0169207000000650},
  doi = {10.1016/S0169-2070(00)00065-0},
  shorttitle = {Out-of-Sample Tests of Forecasting Accuracy},
  abstract = {In evaluations of forecasting accuracy, including forecasting competitions, researchers have paid attention to the selection of time series and to the appropriateness of forecast-error measures. However, they have not formally analyzed choices in the implementation of out-of-sample tests, making it difficult to replicate and compare forecasting accuracy studies. In this paper, I (1) explain the structure of out-of-sample tests, (2) provide guidelines for implementing these tests, and (3) evaluate the adequacy of out-of-sample tests in forecasting software. The issues examined include series-splitting rules, fixed versus rolling origins, updating versus recalibration of model coefficients, fixed versus rolling windows, single versus multiple test periods, diversification through multiple time series, and design characteristics of forecasting competitions. For individual time series, the efficiency and reliability of out-of-sample tests can be improved by employing rolling-origin evaluations, recalibrating coefficients, and using multiple test periods. The results of forecasting competitions would be more generalizable if based upon precisely described groups of time series, in which the series are homogeneous within group and heterogeneous between groups. Few forecasting software programs adequately implement out-of-sample evaluations, especially general statistical packages and spreadsheet add-ins.},
  number = {4},
  journaltitle = {International Journal of Forecasting},
  shortjournal = {International Journal of Forecasting},
  series = {The {{M3}}- {{Competition}}},
  urldate = {2019-01-03},
  date = {2000-10-01},
  pages = {437-450},
  keywords = {Fit period,Fixed origin,Forecasting competitions,Out-of-sample,Recalibration,Rolling origin,Rolling window,Sliding simulation,Test period,Updating,cross validation,rolling-origin,time,time series},
  author = {Tashman, Leonard J.},
  file = {/Users/michelsen/Zotero/storage/2QLX4AT7/Tashman - 2000 - Out-of-sample tests of forecasting accuracy an an.pdf;/Users/michelsen/Zotero/storage/SQ7LXNWW/S0169207000000650.html}
}

@online{loweStratifiedValidationSplits,
  langid = {english},
  title = {Stratified {{Validation Splits}} for {{Regression Problems}}},
  url = {http://scottclowe.com/2016-03-19-stratified-regression-partitions//},
  urldate = {2019-01-03},
  keywords = {stratify,validation},
  author = {Lowe, Scott C.},
  file = {/Users/michelsen/Zotero/storage/V5Q5EXSE/2016-03-19-stratified-regression-partitions.html}
}

@article{smithSuperConvergenceVeryFast2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1708.07120},
  primaryClass = {cs, stat},
  title = {Super-{{Convergence}}: {{Very Fast Training}} of {{Neural Networks Using Large Learning Rates}}},
  url = {http://arxiv.org/abs/1708.07120},
  shorttitle = {Super-{{Convergence}}},
  abstract = {In this paper, we describe a phenomenon, which we named "super-convergence", where neural networks can be trained an order of magnitude faster than with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with one learning rate cycle and a large maximum learning rate. A primary insight that allows super-convergence training is that large learning rates regularize the training, hence requiring a reduction of all other forms of regularization in order to preserve an optimal regularization balance. We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate. Experiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet datasets, and resnet, wide-resnet, densenet, and inception architectures. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. The architectures and code to replicate the figures in this paper are available at github.com/lnsmith54/super-convergence. See http://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of super-convergence to win the DAWNBench challenge (see https://dawn.cs.stanford.edu/benchmark/).},
  urldate = {2019-01-03},
  date = {2017-08-23},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  author = {Smith, Leslie N. and Topin, Nicholay},
  file = {/Users/michelsen/Zotero/storage/YD2IFDRW/Smith and Topin - 2017 - Super-Convergence Very Fast Training of Neural Ne.pdf;/Users/michelsen/Zotero/storage/ANNSW88J/1708.html}
}

@unpublished{hintonNeuralNetworksMachine2012,
  venue = {{Coursera}},
  title = {Neural Networks for Machine Learning},
  url = {www.class-central.com/mooc/398/coursera-neural-networks-for-machine-learning},
  date = {2012},
  author = {Hinton, Geoffrey}
}

@article{morelandDivergingColorMaps,
  langid = {english},
  title = {Diverging {{Color Maps}} for {{Scientiﬁc Visualization}} ({{Expanded}})},
  abstract = {One of the most fundamental features of scientific visualization is the process of mapping scalar values to colors. This process allows us to view scalar fields by coloring surfaces and volumes. Unfortunately, the majority of scientific visualization tools still use a color map that is famous for its ineffectiveness: the rainbow color map. This color map, which na¨ıvely sweeps through the most saturated colors, is well known for its ability to obscure data, introduce artifacts, and confuse users. Although many alternate color maps have been proposed, none have achieved widespread adoption by the visualization community for scientific visualization. This paper explores the use of diverging color maps (sometimes also called ratio, bipolar, or double-ended color maps) for use in scientific visualization, provides a diverging color map that generally performs well in scientific visualization applications, and presents an algorithm that allows users to easily generate their own customized color maps.},
  pages = {20},
  author = {Moreland, Kenneth},
  file = {/Users/michelsen/Zotero/storage/SD7I32AG/Moreland - Diverging Color Maps for Scientiﬁc Visualization (.pdf}
}

@inproceedings{morelandDivergingColorMaps2009,
  langid = {english},
  title = {Diverging {{Color Maps}} for {{Scientific Visualization}}},
  isbn = {978-3-642-10520-3},
  abstract = {One of the most fundamental features of scientific visualization is the process of mapping scalar values to colors. This process allows us to view scalar fields by coloring surfaces and volumes. Unfortunately, the majority of scientific visualization tools still use a color map that is famous for its ineffectiveness: the rainbow color map. This color map, which naïvely sweeps through the most saturated colors, is well known for its ability to obscure data, introduce artifacts, and confuse users. Although many alternate color maps have been proposed, none have achieved widespread adoption by the visualization community for scientific visualization. This paper explores the use of diverging color maps (sometimes also called ratio, bipolar, or double-ended color maps) for use in scientific visualization, provides a diverging color map that generally performs well in scientific visualization applications, and presents an algorithm that allows users to easily generate their own customized color maps.},
  booktitle = {Advances in {{Visual Computing}}},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer Berlin Heidelberg}},
  date = {2009},
  pages = {92-103},
  keywords = {CIELAB Color Space,Color Space,High Frequency Data,Saturated Color,Scientific Visualization},
  author = {Moreland, Kenneth},
  editor = {Bebis, George and Boyle, Richard and Parvin, Bahram and Koracin, Darko and Kuno, Yoshinori and Wang, Junxian and Pajarola, Renato and Lindstrom, Peter and Hinkenjann, André and Encarnação, Miguel L. and Silva, Cláudio T. and Coming, Daniel},
  file = {/Users/michelsen/Zotero/storage/YTIGKUV3/Moreland - 2009 - Diverging Color Maps for Scientific Visualization.pdf}
}

@article{fosterFeaturizingTextConverting2013,
  langid = {english},
  title = {Featurizing {{Text}}: {{Converting Text}} into {{Predictors}} for {{Regression Analysis}}},
  abstract = {Modern data streams routinely combine text with the familiar numerical data used in regression analysis. For example, listings for real estate that show the price of a property typically include a verbal description. Some descriptions include numerical data, such as the number of rooms or the size of the home. Many others, however, only verbally describe the property, often using an idiosyncratic vernacular. For modeling such data, we describe several methods that that convert such text into numerical features suitable for regression analysis. The proposed featurizing techniques create regressors directly from text, requiring minimal user input. The techniques range naive to subtle. One can simply use raw counts of words, obtain principal components from these counts, or build regressors from counts of adjacent words. Our example that models real estate prices illustrates the surprising success of these methods. To partially explain this success, we offer a motivating probabilistic model. Because the derived regressors are difficult to interpret, we further show how the presence of partial quantitative features extracted from text can elucidate the structure of a model.},
  date = {2013},
  pages = {37},
  author = {Foster, Dean P},
  file = {/Users/michelsen/Zotero/storage/FMH26GD6/Foster - 2013 - Featurizing Text Converting Text into Predictors .pdf}
}

@online{HowPowerfulAre2016,
  langid = {english},
  title = {How Powerful Are {{Graph Convolutions}}? (Review of {{Kipf}} \& {{Welling}}, 2016)},
  url = {https://www.inference.vc/how-powerful-are-graph-convolutions-review-of-kipf-welling-2016-2/},
  shorttitle = {How Powerful Are {{Graph Convolutions}}?},
  abstract = {This post is about a paper that has just come out recently on practical generalizations of convolutional layers to graphs: Thomas N. Kipf and Max Welling (2016) Semi-Supervised Classification with Graph Convolutional Networks Along the way I found this earlier, related paper: Defferrard, Bresson and Vandergheynst (NIPS 2016) Convolutional Neural},
  journaltitle = {inFERENCe},
  urldate = {2019-01-03},
  date = {2016-09-13T13:46:10.000Z},
  keywords = {convolutions,graph},
  file = {/Users/michelsen/Zotero/storage/WCF6NA9X/how-powerful-are-graph-convolutions-review-of-kipf-welling-2016-2.html}
}

@book{rasmussenGaussianProcessesMachine2006,
  langid = {english},
  location = {{Cambridge, Mass}},
  title = {Gaussian Processes for Machine Learning},
  isbn = {978-0-262-18253-9},
  pagetotal = {248},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{MIT Press}},
  date = {2006},
  keywords = {Data processing,Gaussian processes,Machine learning,Mathematical models,gaussian processes},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  file = {/Users/michelsen/Zotero/storage/WARG567C/Rasmussen and Williams - 2006 - Gaussian processes for machine learning.pdf},
  note = {OCLC: ocm61285753}
}

@article{brochuTutorialBayesianOptimization2010,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1012.2599},
  primaryClass = {cs},
  title = {A {{Tutorial}} on {{Bayesian Optimization}} of {{Expensive Cost Functions}}, with {{Application}} to {{Active User Modeling}} and {{Hierarchical Reinforcement Learning}}},
  url = {http://arxiv.org/abs/1012.2599},
  abstract = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
  urldate = {2019-01-03},
  date = {2010-12-12},
  keywords = {Computer Science - Machine Learning,G.1.6,G.3,I.2.6,bayesian optimization},
  author = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
  options = {useprefix=true},
  file = {/Users/michelsen/Zotero/storage/DA5KMXPM/Brochu et al. - 2010 - A Tutorial on Bayesian Optimization of Expensive C.pdf;/Users/michelsen/Zotero/storage/YWMGHYQJ/1012.html}
}

@article{snoekPracticalBayesianOptimization2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1206.2944},
  primaryClass = {cs, stat},
  title = {Practical {{Bayesian Optimization}} of {{Machine Learning Algorithms}}},
  url = {http://arxiv.org/abs/1206.2944},
  abstract = {Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a "black art" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
  urldate = {2019-01-03},
  date = {2012-06-13},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,bayesian optimization},
  author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
  file = {/Users/michelsen/Zotero/storage/5A9WKANX/Snoek et al. - 2012 - Practical Bayesian Optimization of Machine Learnin.pdf;/Users/michelsen/Zotero/storage/YJ9R56PD/1206.html}
}

@online{HyperparameterTuningCloud,
  title = {Hyperparameter Tuning in {{Cloud Machine Learning Engine}} Using {{Bayesian Optimization}}},
  url = {https://cloud.google.com/blog/products/gcp/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization/},
  abstract = {Cloud Machine Learning Engine~is a managed service that enables you to easily build machine learning models that work on any type of data, of any size. And},
  journaltitle = {Google Cloud Blog},
  urldate = {2019-01-03},
  keywords = {bayesian optimization},
  file = {/Users/michelsen/Zotero/storage/XFDGIKC7/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization.html}
}

@thesis{monstedUsingBigData2015,
  location = {{Copenhagen, Denmark}},
  title = {Using {{Big Data}} to Predict Huma Behaviour},
  institution = {{University of Copenhagen}},
  type = {Master Thesis},
  date = {2015},
  author = {Mønsted, Bjarke}
}

@online{CondaMythsMisconceptions,
  title = {Conda: {{Myths}} and {{Misconceptions}} | {{Pythonic Perambulations}}},
  url = {https://jakevdp.github.io/blog/2016/08/25/conda-myths-and-misconceptions/},
  urldate = {2019-01-03},
  keywords = {anaconda,conda},
  file = {/Users/michelsen/Zotero/storage/Y7YHLZVT/conda-myths-and-misconceptions.html}
}

@article{saitoPrecisionRecallPlotMore2015,
  langid = {english},
  title = {The {{Precision}}-{{Recall Plot Is More Informative}} than the {{ROC Plot When Evaluating Binary Classifiers}} on {{Imbalanced Datasets}}},
  volume = {10},
  issn = {1932-6203},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432},
  doi = {10.1371/journal.pone.0118432},
  abstract = {Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics (ROC) plots. Alternative measures such as positive predictive value (PPV) and the associated Precision/Recall (PRC) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While ROC plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether ROC plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. PRC plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use ROC plots on imbalanced datasets.},
  number = {3},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2019-01-03},
  date = {2015-03-04},
  pages = {e0118432},
  keywords = {Bioinformatics,Caenorhabditis elegans,Exponential functions,Genome analysis,Genome-wide association studies,Interpolation,MicroRNAs,Support vector machines,imbalanced,precision,recall,roc},
  author = {Saito, Takaya and Rehmsmeier, Marc},
  file = {/Users/michelsen/Zotero/storage/JMRJBFHA/Saito and Rehmsmeier - 2015 - The Precision-Recall Plot Is More Informative than.pdf;/Users/michelsen/Zotero/storage/XIWHHWX6/article.html}
}

@article{swamidassCROCStrongerROC2010,
  title = {A {{CROC}} Stronger than {{ROC}}: Measuring, Visualizing and Optimizing Early Retrieval},
  volume = {26},
  issn = {1367-4803},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2865862/},
  doi = {10.1093/bioinformatics/btq140},
  shorttitle = {A {{CROC}} Stronger than {{ROC}}},
  abstract = {Motivation: The performance of classifiers is often assessed using Receiver Operating Characteristic ROC [or (AC) accumulation curve or enrichment curve] curves and the corresponding areas under the curves (AUCs). However, in many fundamental problems ranging from information retrieval to drug discovery, only the very top of the ranked list of predictions is of any interest and ROCs and AUCs are not very useful. New metrics, visualizations and optimization tools are needed to address this ‘early retrieval’ problem., Results: To address the early retrieval problem, we develop the general concentrated ROC (CROC) framework. In this framework, any relevant portion of the ROC (or AC) curve is magnified smoothly by an appropriate continuous transformation of the coordinates with a corresponding magnification factor. Appropriate families of magnification functions confined to the unit square are derived and their properties are analyzed together with the resulting CROC curves. The area under the CROC curve (AUC[CROC]) can be used to assess early retrieval. The general framework is demonstrated on a drug discovery problem and used to discriminate more accurately the early retrieval performance of five different predictors. From this framework, we propose a novel metric and visualization—the CROC(exp), an exponential transform of the ROC curve—as an alternative to other methods. The CROC(exp) provides a principled, flexible and effective way for measuring and visualizing early retrieval performance with excellent statistical power. Corresponding methods for optimizing early retrieval are also described in the ., Availability: Datasets are publicly available. Python code and command-line utilities implementing CROC curves and metrics are available at http://pypi.python.org/pypi/CROC/, Contact: pfbaldi@ics.uci.edu},
  number = {10},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  urldate = {2019-01-03},
  date = {2010-05-15},
  pages = {1348-1356},
  keywords = {roc,croc},
  author = {Swamidass, S. Joshua and Azencott, Chloé-Agathe and Daily, Kenny and Baldi, Pierre},
  file = {/Users/michelsen/Zotero/storage/E3WAZLFQ/Swamidass et al. - 2010 - A CROC stronger than ROC measuring, visualizing a.pdf},
  eprinttype = {pmid},
  eprint = {20378557},
  pmcid = {PMC2865862}
}

@article{schubertImprovingAncientDNA2012,
  title = {Improving Ancient {{DNA}} Read Mapping against Modern Reference Genomes},
  volume = {13},
  issn = {1471-2164},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3468387/},
  doi = {10.1186/1471-2164-13-178},
  abstract = {Background
Next-Generation Sequencing has revolutionized our approach to ancient DNA (aDNA) research, by providing complete genomic sequences of ancient individuals and extinct species. However, the recovery of genetic material from long-dead organisms is still complicated by a number of issues, including post-mortem DNA damage and high levels of environmental contamination. Together with error profiles specific to the type of sequencing platforms used, these specificities could limit our ability to map sequencing reads against modern reference genomes and therefore limit our ability to identify endogenous ancient reads, reducing the efficiency of shotgun sequencing aDNA.

Results
In this study, we compare different computational methods for improving the accuracy and sensitivity of aDNA sequence identification, based on shotgun sequencing reads recovered from Pleistocene horse extracts using Illumina GAIIx and Helicos Heliscope platforms. We show that the performance of the Burrows Wheeler Aligner (BWA), that has been developed for mapping of undamaged sequencing reads using platforms with low rates of indel-types of sequencing errors, can be employed at acceptable run-times by modifying default parameters in a platform-specific manner. We also examine if trimming likely damaged positions at read ends can increase the recovery of genuine aDNA fragments and if accurate identification of human contamination can be achieved using a strategy previously suggested based on best hit filtering. We show that combining our different mapping and filtering approaches can increase the number of high-quality endogenous hits recovered by up to 33\%.

Conclusions
We have shown that Illumina and Helicos sequences recovered from aDNA extracts could not be aligned to modern reference genomes with the same efficiency unless mapping parameters are optimized for the specific types of errors generated by these platforms and by post-mortem DNA damage. Our findings have important implications for future aDNA research, as we define mapping guidelines that improve our ability to identify genuine aDNA sequences, which in turn could improve the genotyping accuracy of ancient specimens. Our framework provides a significant improvement to the standard procedures used for characterizing ancient genomes, which is challenged by contamination and often low amounts of DNA material.},
  journaltitle = {BMC Genomics},
  shortjournal = {BMC Genomics},
  urldate = {2019-01-03},
  date = {2012-05-10},
  pages = {178},
  author = {Schubert, Mikkel and Ginolhac, Aurelien and Lindgreen, Stinus and Thompson, John F and AL-Rasheid, Khaled AS and Willerslev, Eske and Krogh, Anders and Orlando, Ludovic},
  file = {/Users/michelsen/Zotero/storage/PFBHIWHF/Schubert et al. - 2012 - Improving ancient DNA read mapping against modern .pdf},
  eprinttype = {pmid},
  eprint = {22574660},
  pmcid = {PMC3468387}
}

@article{azurMultipleImputationChained2011,
  title = {Multiple {{Imputation}} by {{Chained Equations}}: {{What}} Is It and How Does It Work?},
  volume = {20},
  issn = {1049-8931},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/},
  doi = {10.1002/mpr.329},
  shorttitle = {Multiple {{Imputation}} by {{Chained Equations}}},
  abstract = {Multivariate imputation by chained equations (MICE) has emerged as a principled method of dealing with missing data. Despite properties that make MICE particularly useful for large imputation procedures and advances in software development that now make it accessible to many researchers, many psychiatric researchers have not been trained in these methods and few practical resources exist to guide researchers in the implementation of this technique. This paper provides an introduction to the MICE method with a focus on practical aspects and challenges in using this method. A brief review of software programs available to implement MICE and then analyze multiply imputed data is also provided.},
  number = {1},
  journaltitle = {International journal of methods in psychiatric research},
  shortjournal = {Int J Methods Psychiatr Res},
  urldate = {2019-01-03},
  date = {2011-03-01},
  pages = {40-49},
  keywords = {imputation,impute,mice},
  author = {Azur, Melissa J. and Stuart, Elizabeth A. and Frangakis, Constantine and Leaf, Philip J.},
  file = {/Users/michelsen/Zotero/storage/F3IQX4U2/Azur et al. - 2011 - Multiple Imputation by Chained Equations What is .pdf},
  eprinttype = {pmid},
  eprint = {21499542},
  pmcid = {PMC3074241}
}

@online{doshiDeepLearningBest2018,
  title = {Deep {{Learning Best Practices}} (1) — {{Weight Initialization}}},
  url = {https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94},
  abstract = {Basics, weight initialization pitfalls \& best practices},
  journaltitle = {Medium},
  urldate = {2019-01-03},
  date = {2018-03-26T16:02:43.107Z},
  keywords = {clipping,leaky,relu,weights},
  author = {Doshi, Neerja},
  file = {/Users/michelsen/Zotero/storage/968QH42U/deep-learning-best-practices-1-weight-initialization-14e5c0295b94.html}
}

@article{ruderOverviewGradientDescent2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.04747},
  primaryClass = {cs},
  title = {An Overview of Gradient Descent Optimization Algorithms},
  url = {http://arxiv.org/abs/1609.04747},
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  urldate = {2019-01-03},
  date = {2016-09-15},
  keywords = {Computer Science - Machine Learning,adam; momentum;},
  author = {Ruder, Sebastian},
  file = {/Users/michelsen/Zotero/storage/4J5EQSFU/Ruder - 2016 - An overview of gradient descent optimization algor.pdf;/Users/michelsen/Zotero/storage/XEEJWEM6/1609.html}
}

@online{koehrsenVisualizingDataPairs2018,
  title = {Visualizing {{Data}} with {{Pairs Plots}} in {{Python}}},
  url = {https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166},
  abstract = {How to quickly create a powerful exploratory data analysis visualization},
  journaltitle = {Towards Data Science},
  urldate = {2019-01-03},
  date = {2018-04-06T19:11:35.487Z},
  keywords = {pair plot,pairgrid,pairplot,seaborn},
  author = {Koehrsen, Will},
  file = {/Users/michelsen/Zotero/storage/T43T27KW/visualizing-data-with-pair-plots-in-python-f228cf529166.html}
}

@online{lauraeXgboostHiGamma2016,
  title = {Xgboost: “{{Hi I}}’m {{Gamma}}. {{What}} Can {{I}} Do for You?” — And the Tuning of Regularization},
  url = {https://medium.com/data-design/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6},
  shorttitle = {Xgboost},
  abstract = {Laurae: This post is about tuning the regularization in the tree-based xgboost (Maximum Depth, Minimum Child Weight, Gamma). It also…},
  journaltitle = {Medium},
  urldate = {2019-01-03},
  date = {2016-08-22T18:25:28.550Z},
  keywords = {xgb,xgboost,gamma},
  author = {Laurae},
  file = {/Users/michelsen/Zotero/storage/FDX8QPYT/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6.html}
}

@online{SurvivalGuidePhD,
  title = {A {{Survival Guide}} to a {{PhD}}},
  url = {https://karpathy.github.io/2016/09/07/phd/},
  urldate = {2019-01-03},
  keywords = {guide,Ph.D.,phd},
  file = {/Users/michelsen/Zotero/storage/RKZF8FK3/phd.html}
}

@article{alonHowChooseGood2009,
  title = {How {{To Choose}} a {{Good Scientific Problem}}},
  volume = {35},
  issn = {1097-2765},
  url = {http://www.sciencedirect.com/science/article/pii/S1097276509006418},
  doi = {10.1016/j.molcel.2009.09.013},
  abstract = {Choosing good problems is essential for being a good scientist. But what is a good problem, and how do you choose one? The subject is not usually discussed explicitly within our profession. Scientists are expected to be smart enough to figure it out on their own and through the observation of their teachers. This lack of explicit discussion leaves a vacuum that can lead to approaches such as choosing problems that can give results that merit publication in valued journals, resulting in a job and tenure.},
  number = {6},
  journaltitle = {Molecular Cell},
  shortjournal = {Molecular Cell},
  urldate = {2019-01-03},
  date = {2009-09-24},
  pages = {726-728},
  keywords = {guide,phd},
  author = {Alon, Uri},
  file = {/Users/michelsen/Zotero/storage/HZG4DP8X/Alon - 2009 - How To Choose a Good Scientific Problem.pdf;/Users/michelsen/Zotero/storage/HNXVY9V4/S1097276509006418.html}
}

@article{mcdonnellPaperWritingGone2017,
  langid = {english},
  title = {Paper Writing Gone {{Hollywood}}},
  volume = {355},
  issn = {0036-8075, 1095-9203},
  url = {http://science.sciencemag.org/content/355/6320/102},
  doi = {10.1126/science.355.6320.102},
  abstract = {“So you want to be a writer?” one of my professors asked me when he learned I was interested in a career as an academic scientist—a pointed warning that a life of science is also a life of writing. But even knowing this in advance, I found that writing was a challenge as I made my way down the tenure track. I had trouble finding stories in my data sets. Even when I had a good tale, I struggled to tell it. I tried starting with the opening sentences and hoping I'd make it to the paper's end. But more often than not, I wrote my way down many blind alleys. My permanently unfinished papers outnumbered my published ones. Worst of all, I was not helping my Ph.D. students and postdocs learn proper writing craft.

![Figure][1]{$<$}/img{$>$}

ILLUSTRATION: ROBERT NEUBECKER

{$>$} “I … ask the Ph.D. student or postdoc to play the role of would-be movie director.” 

My big break came shortly after getting tenure. In a passing conversation, a senior colleague mentioned that his process for writing research papers centered on structure. Rather than focus on words and sentences, the part of writing that so bogged me down, he highlighted the importance of outlining the overall story to be told. I had thought that the standard paper structure—introduction, methods, results, discussion, conclusions—was enough to keep me on track. But my colleague helped me realize that, even with those sections, there is still enough freedom to get stuck in writing cul-de-sacs.

I now see each of the standard paper sections as its own Russian nesting doll. Writing papers is easiest when you spend considerable thought and time stacking all these pieces first. I call it the top-down writing approach.

Each of my group's papers now starts with a storyboard session at a whiteboard. I pretend to be a big-time Hollywood producer and ask the Ph.D. student or postdoc to play the role of would-be movie director pitching a new movie to me. Their pitch must answer three questions: What is the status quo? What is wrong with the status quo? How does this new paper go beyond the status quo?

This approach helps frame the story and place key figures and technical findings in context. Balancing each of the status quo elements is a great way to set up the introduction—often the toughest section for early-career scientists to write—and to lead the reader to the research questions or hypotheses. Say too little about what we already know and one risks losing a large audience who may be unfamiliar with the topic. Too little about what's wrong with the current state of knowledge and the reader may wonder why we need yet another paper on that topic. Too little about how the work goes beyond what others have done and the novelty is unclear. The result is a roadmap of the novel elements in the work, which brings the discussion—the other tough section for the writing newcomer—into final focus.

Once the pitch makes sense, we go back and forth stacking the Russian dolls on the whiteboard until the outline subheadings become paragraph topics, with every paragraph explicitly represented in the outline. Honing this outline prior to any writing allows us to determine whether the research story resonates from start to finish. We might spend days or weeks on the outline to get it right, but it's time well spent. The slavish adherence to nested headings shows at a glance whether the paper makes a clear and worthy contribution; whether the title, objectives, and results are properly aligned; what figures are truly essential to the storyline; and whether the message hums. Writing then becomes a much easier process of filling in the blanks. The paper is effectively finished before the sentence writing starts.

I haven't mastered the writing game, and I am still constantly learning. But the top-down approach has been a game changer in my group. Now, when a new grad student indicates an interest in an academic career, I ask, “So you want to be a Hollywood producer?”

 [1]: pending:yes},
  number = {6320},
  journaltitle = {Science},
  urldate = {2019-01-03},
  date = {2017-01-06},
  pages = {102-102},
  keywords = {guide},
  author = {McDonnell, Jeffrey J.},
  file = {/Users/michelsen/Zotero/storage/VPUR4XJ6/McDonnell - 2017 - Paper writing gone Hollywood.pdf;/Users/michelsen/Zotero/storage/IRB2R7YF/102.html},
  eprinttype = {pmid},
  eprint = {28059772}
}

@article{alonHowGiveGood2009,
  langid = {english},
  title = {How {{To Give}} a {{Good Talk}}},
  volume = {36},
  issn = {1097-2765},
  url = {https://www.cell.com/molecular-cell/abstract/S1097-2765(09)00742-4},
  doi = {10.1016/j.molcel.2009.10.007},
  number = {2},
  journaltitle = {Molecular Cell},
  shortjournal = {Molecular Cell},
  urldate = {2019-01-03},
  date = {2009-10-23},
  pages = {165-167},
  keywords = {guide},
  author = {Alon, Uri},
  file = {/Users/michelsen/Zotero/storage/R4I94QMW/Alon - 2009 - How To Give a Good Talk.pdf;/Users/michelsen/Zotero/storage/IWV6KBZ4/S1097-2765(09)00742-4.html},
  eprinttype = {pmid},
  eprint = {19854123}
}

@article{vanSuperLearner2007,
  title = {Super {{Learner}}},
  volume = {6},
  issn = {1544-6115},
  url = {https://www.degruyter.com/view/j/sagmb.2007.6.issue-1/sagmb.2007.6.1.1309/sagmb.2007.6.1.1309.xml},
  doi = {10.2202/1544-6115.1309},
  abstract = {When trying to learn a model for the prediction of an outcome given a set of covariates, a statistician has many estimation procedures in their toolbox. A few examples of these candidate learners are: least squares, least angle regression, random forests, and spline regression. Previous articles (van der Laan and Dudoit (2003); van der Laan et al. (2006); Sinisi et al. (2007)) theoretically validated the use of cross validation to select an optimal learner among many candidate learners. Motivated by this use of cross validation, we propose a new prediction method for creating a weighted combination of many candidate learners to build the super learner. This article proposes a fast algorithm for constructing a super learner in prediction which uses V-fold cross-validation to select weights to combine an initial set of candidate learners. In addition, this paper contains a practical demonstration of the adaptivity of this so called super learner to various true data generating distributions. This approach for construction of a super learner generalizes to any parameter which can be defined as a minimizer of a loss function.},
  number = {1},
  journaltitle = {Statistical Applications in Genetics and Molecular Biology},
  urldate = {2019-01-03},
  date = {2007},
  keywords = {machine learning,ensemble,super learner,superlearner,cross-validation,loss-based estimation,prediction},
  author = {{van}, der Laan Mark J. and Polley, Eric C and Hubbard, Alan E.},
  file = {/Users/michelsen/Zotero/storage/C9AFSBMW/van et al. - 2007 - Super Learner.pdf}
}

@article{polleySuperLearnerPrediction,
  langid = {english},
  title = {Super {{Learner In Prediction}}},
  abstract = {Super learning is a general loss based learning method that has been proposed and analyzed theoretically in van der Laan et al. (2007). In this article we consider super learning for prediction. The super learner is a prediction method designed to find the optimal combination of a collection of prediction algorithms. The super learner algorithm finds the combination of algorithms minimizing the cross-validated risk. The super learner framework is built on the theory of crossvalidation and allows for a general class of prediction algorithms to be considered for the ensemble. Due to the previously established oracle results for the crossvalidation selector, the super learner has been proven to represent an asymptotically optimal system for learning. In this article we demonstrate the practical implementation and finite sample performance of super learning in prediction.},
  pages = {21},
  author = {Polley, Eric C},
  file = {/Users/michelsen/Zotero/storage/AGTMEN7Z/Polley - Super Learner In Prediction.pdf}
}

@article{kovesiGoodColourMaps2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.03700},
  primaryClass = {cs},
  title = {Good {{Colour Maps}}: {{How}} to {{Design Them}}},
  url = {http://arxiv.org/abs/1509.03700},
  shorttitle = {Good {{Colour Maps}}},
  abstract = {Many colour maps provided by vendors have highly uneven perceptual contrast over their range. It is not uncommon for colour maps to have perceptual flat spots that can hide a feature as large as one tenth of the total data range. Colour maps may also have perceptual discontinuities that induce the appearance of false features. Previous work in the design of perceptually uniform colour maps has mostly failed to recognise that CIELAB space is only designed to be perceptually uniform at very low spatial frequencies. The most important factor in designing a colour map is to ensure that the magnitude of the incremental change in perceptual lightness of the colours is uniform. The specific requirements for linear, diverging, rainbow and cyclic colour maps are developed in detail. To support this work two test images for evaluating colour maps are presented. The use of colour maps in combination with relief shading is considered and the conditions under which colour can enhance or disrupt relief shading are identified. Finally, a set of new basis colours for the construction of ternary images are presented. Unlike the RGB primaries these basis colours produce images whereby the salience of structures are consistent irrespective of the assignment of basis colours to data channels.},
  urldate = {2019-01-03},
  date = {2015-09-11},
  keywords = {Computer Science - Graphics,I.3.3,color,color map},
  author = {Kovesi, Peter},
  file = {/Users/michelsen/Zotero/storage/PP7NGNNK/Kovesi - 2015 - Good Colour Maps How to Design Them.pdf;/Users/michelsen/Zotero/storage/TD2UTS53/1509.html}
}

@article{scarselliGraphNeuralNetwork2009,
  title = {The Graph Neural Network Model},
  abstract = {The graph neural network model Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
  journaltitle = {IEEE Transactions on Neural Networks},
  date = {2009},
  keywords = {graph},
  author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  file = {/Users/michelsen/Zotero/storage/2NLTGKPG/Scarselli et al. - 2009 - The graph neural network model.pdf;/Users/michelsen/Zotero/storage/BL87KMTD/summary.html}
}

@online{SamplersSamplersEverywhere,
  title = {Samplers, Samplers, Everywhere... | {{Samplers Demo}}},
  url = {https://mattpitkin.github.io/samplers-demo/pages/samplers-samplers-everywhere/},
  urldate = {2019-01-03},
  keywords = {monte carlo,emcee,markov chain,mcmc,nested sampling,pymc3},
  file = {/Users/michelsen/Zotero/storage/N67ZABUU/samplers-samplers-everywhere.html}
}

@article{kingmaAdamMethodStochastic2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6980},
  primaryClass = {cs},
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  url = {http://arxiv.org/abs/1412.6980},
  shorttitle = {Adam},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  urldate = {2019-01-03},
  date = {2014-12-22},
  keywords = {Computer Science - Machine Learning,adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  file = {/Users/michelsen/Zotero/storage/RIXCNZKX/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf;/Users/michelsen/Zotero/storage/CGBQSHQN/1412.html}
}

@incollection{awadSupportVectorRegression2015,
  langid = {english},
  location = {{Berkeley, CA}},
  title = {Support {{Vector Regression}}},
  isbn = {978-1-4302-5990-9},
  url = {https://doi.org/10.1007/978-1-4302-5990-9_4},
  doi = {10.1007/978-1-4302-5990-9_4},
  abstract = {Rooted in statistical learning or Vapnik-Chervonenkis (VC) theory, support vector machines (SVMs) are well positioned to generalize on yet-to-be-seen data. The SVM concepts presented in Chapter 3 can be generalized to become applicable to regression problems. As in classification, support vector regression (SVR) is characterized by the use of kernels, sparse solution, and VC control of the margin and the number of support vectors. Although less popular than SVM, SVR has been proven to be an effective tool in real-value function estimation. As a supervised-learning approach, SVR trains using a symmetrical loss function, which equally penalizes high and low misestimates. Using Vapnik’s -insensitive approach, a flexible tube of minimal radius is formed symmetrically around the estimated function, such that the absolute values of errors less than a certain threshold are ignored both above and below the estimate. In this manner, points outside the tube are penalized, but those within the tube, either above or below the function, receive no penalty. One of the main advantages of SVR is that its computational complexity does not depend on the dimensionality of the input space. Additionally, it has excellent generalization capability, with high prediction accuracy.},
  booktitle = {Efficient {{Learning Machines}}: {{Theories}}, {{Concepts}}, and {{Applications}} for {{Engineers}} and {{System Designers}}},
  publisher = {{Apress}},
  urldate = {2019-01-03},
  date = {2015},
  pages = {67-80},
  keywords = {Asymmetrical Loss Function,Loss Function,Short Term Load Forecast,Support Vector,Support Vector Regression},
  author = {Awad, Mariette and Khanna, Rahul},
  editor = {Awad, Mariette and Khanna, Rahul},
  file = {/Users/michelsen/Zotero/storage/LMIPVGWR/Awad and Khanna - 2015 - Support Vector Regression.pdf}
}

@article{zouRegularizationVariableSelection2005,
  langid = {english},
  title = {Regularization and Variable Selection via the Elastic Net},
  volume = {67},
  issn = {1369-7412, 1467-9868},
  url = {http://doi.wiley.com/10.1111/j.1467-9868.2005.00503.x},
  doi = {10.1111/j.1467-9868.2005.00503.x},
  abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in (out) the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p n case. An efficient algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like the LARS algorithm does for the lasso.},
  number = {2},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  urldate = {2019-01-03},
  date = {2005-04},
  pages = {301-320},
  keywords = {elastic net,elnet},
  author = {Zou, Hui and Hastie, Trevor},
  file = {/Users/michelsen/Zotero/storage/T4JGPIA7/Zou and Hastie - 2005 - Regularization and variable selection via the elas.pdf}
}

@online{WhyEveryStatistician,
  title = {Why Every Statistician Should Know about Cross-Validation | {{Rob J Hyndman}}},
  url = {https://robjhyndman.com/hyndsight/crossvalidation/},
  urldate = {2019-01-03},
  keywords = {cross validation,cross-validation},
  file = {/Users/michelsen/Zotero/storage/7CT6HN75/crossvalidation.html}
}

@online{bUsingLatestAdvancements2019,
  title = {Using the Latest Advancements in Deep Learning to Predict Stock Price Movements},
  url = {https://towardsdatascience.com/aifortrading-2edd6fac689d},
  abstract = {Link to the complete notebook: https://github.com/borisbanushev/stockpredictionai},
  journaltitle = {Towards Data Science},
  urldate = {2019-01-23},
  date = {2019-01-10T02:44:15.318Z},
  keywords = {time series,stock price,timeseries},
  author = {B, Boris},
  file = {/Users/michelsen/Zotero/storage/9I9PCB8F/aifortrading-2edd6fac689d.html}
}

@article{reyzinUnprovabilityComesMachine2019,
  langid = {english},
  title = {Unprovability Comes to Machine Learning},
  volume = {565},
  url = {http://www.nature.com/articles/d41586-019-00012-4},
  doi = {10.1038/d41586-019-00012-4},
  abstract = {Evidence of a limitation in certain machine-learning algorithms.},
  number = {7738},
  journaltitle = {Nature},
  urldate = {2019-01-23},
  date = {2019-01},
  pages = {166},
  keywords = {machine learning,ml,impossible,unsolveable},
  author = {Reyzin, Lev},
  file = {/Users/michelsen/Zotero/storage/NLD7WRLH/d41586-019-00012-4.pdf;/Users/michelsen/Zotero/storage/C8A4VVU9/d41586-019-00012-4.html}
}

@article{kimConvolutionalNeuralNetworks2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1408.5882},
  primaryClass = {cs},
  title = {Convolutional {{Neural Networks}} for {{Sentence Classification}}},
  url = {http://arxiv.org/abs/1408.5882},
  abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
  urldate = {2019-01-23},
  date = {2014-08-25},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,NLP,CNN,text classification},
  author = {Kim, Yoon},
  file = {/Users/michelsen/Zotero/storage/UW38FZE7/Kim - 2014 - Convolutional Neural Networks for Sentence Classif.pdf;/Users/michelsen/Zotero/storage/LVGQG83C/1408.html}
}

@inproceedings{yangHierarchicalAttentionNetworks2016,
  location = {{San Diego, California}},
  title = {Hierarchical {{Attention Networks}} for {{Document Classification}}},
  url = {http://www.aclweb.org/anthology/N16-1174},
  booktitle = {Proceedings of the 2016 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  publisher = {{Association for Computational Linguistics}},
  urldate = {2019-01-23},
  date = {2016-06},
  pages = {1480--1489},
  keywords = {nlp,text classification,attention},
  author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
  file = {/Users/michelsen/Zotero/storage/K8JUBN7T/Yang et al. - 2016 - Hierarchical Attention Networks for Document Class.pdf}
}

@article{linWhyDoesDeep2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1608.08225},
  title = {Why Does Deep and Cheap Learning Work so Well?},
  volume = {168},
  issn = {0022-4715, 1572-9613},
  url = {http://arxiv.org/abs/1608.08225},
  doi = {10.1007/s10955-017-1836-5},
  abstract = {We show how the success of deep learning could depend not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can frequently be approximated through "cheap learning" with exponentially fewer parameters than generic ones. We explore how properties frequently encountered in physics such as symmetry, locality, compositionality, and polynomial log-probability translate into exceptionally simple neural networks. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine-learning, a deep neural network can be more efficient than a shallow one. We formalize these claims using information theory and discuss the relation to the renormalization group. We prove various "no-flattening theorems" showing when efficient linear deep networks cannot be accurately approximated by shallow ones without efficiency loss, for example, we show that \$n\$ variables cannot be multiplied using fewer than 2\^n neurons in a single hidden layer.},
  number = {6},
  journaltitle = {Journal of Statistical Physics},
  urldate = {2019-01-24},
  date = {2017-09},
  pages = {1223-1247},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,deep,entropy,hamiltonian,physics ml,physics-ml},
  author = {Lin, Henry W. and Tegmark, Max and Rolnick, David},
  file = {/Users/michelsen/Zotero/storage/KKRS6LZD/Lin et al. - 2017 - Why does deep and cheap learning work so well.pdf;/Users/michelsen/Zotero/storage/99TAINI9/1608.html}
}

@online{bos50TimesFaster2018,
  title = {50 Times Faster Data Loading for {{Pandas}}: No Problem},
  url = {https://blog.esciencecenter.nl/irregular-data-in-pandas-using-c-88ce311cb9ef},
  shorttitle = {50 Times Faster Data Loading for {{Pandas}}},
  abstract = {Loading irregular data into Pandas using C++},
  journaltitle = {Netherlands eScience Center},
  urldate = {2019-01-24},
  date = {2018-09-03T08:17:56.238Z},
  keywords = {c++,cpp,pandas},
  author = {Bos, Patrick},
  file = {/Users/michelsen/Zotero/storage/TK8IWV4N/irregular-data-in-pandas-using-c-88ce311cb9ef.html}
}

@article{hyndmanANOTHERLOOKFORECASTACCURACY,
  langid = {english},
  title = {{{ANOTHER LOOK AT FORECAST}}-{{ACCURACY METRICS FOR INTERMITTENT DEMAND}}},
  pages = {4},
  keywords = {forecast,time series,timeseries,metric},
  author = {Hyndman, Rob J},
  file = {/Users/michelsen/Zotero/storage/FDN4T6T5/Hyndman - ANOTHER LOOK AT FORECAST-ACCURACY METRICS FOR INTE.pdf}
}

@article{jonssonMapDamage2FastApproximate2013,
  title = {{{mapDamage2}}.0: Fast Approximate {{Bayesian}} Estimates of Ancient {{DNA}} Damage Parameters},
  volume = {29},
  issn = {1367-4803},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3694634/},
  doi = {10.1093/bioinformatics/btt193},
  shorttitle = {{{mapDamage2}}.0},
  abstract = {Motivation: Ancient DNA (aDNA) molecules in fossilized bones and teeth, coprolites, sediments, mummified specimens and museum collections represent fantastic sources of information for evolutionary biologists, revealing the agents of past epidemics and the dynamics of past populations. However, the analysis of aDNA generally faces two major issues. Firstly, sequences consist of a mixture of endogenous and various exogenous backgrounds, mostly microbial. Secondly, high nucleotide misincorporation rates can be observed as a result of severe post-mortem DNA damage. Such misincorporation patterns are instrumental to authenticate ancient sequences versus modern contaminants. We recently developed the user-friendly mapDamage package that identifies such patterns from next-generation sequencing (NGS) sequence datasets. The absence of formal statistical modeling of the DNA damage process, however, precluded rigorous quantitative comparisons across samples., Results: Here, we describe mapDamage 2.0 that extends the original features of mapDamage by incorporating a statistical model of DNA damage. Assuming that damage events depend only on sequencing position and post-mortem deamination, our Bayesian statistical framework provides estimates of four key features of aDNA molecules: the average length of overhangs (λ), nick frequency (ν) and cytosine deamination rates in both double-stranded regions () and overhangs (). Our model enables rescaling base quality scores according to their probability of being damaged. mapDamage 2.0 handles NGS datasets with ease and is compatible with a wide range of DNA library protocols., Availability: mapDamage 2.0 is available at ginolhac.github.io/mapDamage/ as a Python package and documentation is maintained at the Centre for GeoGenetics Web site (geogenetics.ku.dk/publications/mapdamage2.0/)., Contact:
jonsson.hakon@gmail.com, Supplementary information:
Supplementary data are available at Bioinformatics online.},
  number = {13},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  urldate = {2019-01-24},
  date = {2013-07-01},
  pages = {1682-1684},
  keywords = {adna,ancient,dna,mapdamage},
  author = {Jónsson, Hákon and Ginolhac, Aurélien and Schubert, Mikkel and Johnson, Philip L. F. and Orlando, Ludovic},
  file = {/Users/michelsen/Zotero/storage/SHHXN72V/Jónsson et al. - 2013 - mapDamage2.0 fast approximate Bayesian estimates .pdf},
  eprinttype = {pmid},
  eprint = {23613487},
  pmcid = {PMC3694634}
}

@online{koehrsenInteractiveControlsJupyter2019,
  title = {Interactive {{Controls}} for {{Jupyter Notebooks}}},
  url = {https://towardsdatascience.com/interactive-controls-for-jupyter-notebooks-f5c94829aee6},
  abstract = {How to use IPywidgets to enhance your data exploration and analysis},
  journaltitle = {Towards Data Science},
  urldate = {2019-02-11},
  date = {2019-01-27T22:23:17.482Z},
  keywords = {jupyter,notebook,interactive,widget,widgets},
  author = {Koehrsen, Will},
  file = {/Users/michelsen/Zotero/storage/ZNYIEJPI/interactive-controls-for-jupyter-notebooks-f5c94829aee6.html}
}

@online{koehrsenNextLevelData2019,
  title = {The {{Next Level}} of {{Data Visualization}} in {{Python}}},
  url = {https://towardsdatascience.com/the-next-level-of-data-visualization-in-python-dd6e99039d5e},
  abstract = {How to make great-looking, fully-interactive plots with a single line of Python},
  journaltitle = {Towards Data Science},
  urldate = {2019-02-11},
  date = {2019-01-09T04:08:44.483Z},
  keywords = {interactive,iplot,matplotlib,plot,plotly},
  author = {Koehrsen, Will},
  file = {/Users/michelsen/Zotero/storage/ZEMYNK23/the-next-level-of-data-visualization-in-python-dd6e99039d5e.html}
}

@online{qiaoLargeScaleVisualizations2018,
  title = {Large {{Scale Visualizations}} and {{Mapping}} with {{Datashader}}},
  url = {https://towardsdatascience.com/large-scale-visualizations-and-mapping-with-datashader-d465f5c47fb5},
  abstract = {A bird’s eye view of businesses in San Francisco.},
  journaltitle = {Towards Data Science},
  urldate = {2019-02-11},
  date = {2018-12-25T14:26:02.654Z},
  keywords = {plot,datashader},
  author = {Qiao, Finn},
  file = {/Users/michelsen/Zotero/storage/GBBIGQXM/large-scale-visualizations-and-mapping-with-datashader-d465f5c47fb5.html}
}

@article{pollackBayesianBlocksHigh2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1708.00810},
  primaryClass = {hep-ex, physics:physics},
  title = {Bayesian {{Blocks}} in {{High Energy Physics}}: {{Better Binning}} Made Easy!},
  url = {http://arxiv.org/abs/1708.00810},
  shorttitle = {Bayesian {{Blocks}} in {{High Energy Physics}}},
  abstract = {The Bayesian Block algorithm, originally developed for applications in astronomy, can be used to improve the binning of histograms in high energy physics. The visual improvement can be dramatic, as shown here with two simple examples. More importantly, this algorithm and the histogram is produces is a non-parametric density estimate, providing a description of background distributions that does not suffer from the arbitrariness of ad hoc analytical functions. The statistical power of an hypothesis test based on Bayesian Blocks is nearly as good as that obtained by fitting analytical functions. Two examples are provided: a narrow peak on a smoothly-falling background, and an excess in the tail of a background that falls rapidly over several orders of magnitude. These examples show the usefulness of the binning provided by the Bayesian Blocks algorithm both for presentation of data and when searching for new physics.},
  urldate = {2019-02-13},
  date = {2017-08-02},
  keywords = {High Energy Physics - Experiment,Physics - Data Analysis; Statistics and Probability,bayesian blocks,blocks,binning,histogram},
  author = {Pollack, Brian and Bhattacharya, Saptaparna and Schmitt, Michael},
  file = {/Users/michelsen/Zotero/storage/EQ57P27P/Pollack et al. - 2017 - Bayesian Blocks in High Energy Physics Better Bin.pdf;/Users/michelsen/Zotero/storage/6YJVUQRE/1708.html}
}

@article{scargleBayesianBlocksDivide2000,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {physics/0009033},
  title = {Bayesian {{Blocks}}: {{Divide}} and {{Conquer}}, {{MCMC}}, and {{Cell Coalescence Approaches}}},
  url = {http://arxiv.org/abs/physics/0009033},
  shorttitle = {Bayesian {{Blocks}}},
  abstract = {Identification of local structure in intensive data -- such as time series, images, and higher dimensional processes -- is an important problem in astronomy. Since the data are typically generated by an inhomogeneous Poisson process, an appropriate model is one that partitions the data space into cells, each of which is described by a homogeneous (constant event rate) Poisson process. It is key that the sizes and locations of the cells are determined by the data, and are not predefined or even constrained to be evenly spaced. For one-dimensional time series, the method amounts to Bayesian changepoint detection. Three approaches to solving the multiple changepoint problem are sketched, based on: (1) divide and conquer with single changepoints, (2) maximum posterior for the number of changepoints, and (3) cell coalescence. The last method starts from the Voronoi tessellation of the data, and thus should easily generalize to spaces of higher dimension.},
  urldate = {2019-02-13},
  date = {2000-09-09},
  keywords = {Physics - Data Analysis; Statistics and Probability,Physics - Computational Physics,bayesian blocks,binning,histogram},
  author = {Scargle, Jeffrey D.},
  file = {/Users/michelsen/Zotero/storage/9JTEGVY5/Scargle - 2000 - Bayesian Blocks Divide and Conquer, MCMC, and Cel.pdf;/Users/michelsen/Zotero/storage/C3BC3X5S/0009033.html}
}

@article{scargleBayesianBlocksTwo2002,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {math/0111128},
  title = {Bayesian {{Blocks}} in {{Two}} or {{More Dimensions}}: {{Image Segmentation}} and {{Cluster Analysis}}},
  volume = {617},
  issn = {0094243X},
  url = {http://arxiv.org/abs/math/0111128},
  doi = {10.1063/1.1477046},
  shorttitle = {Bayesian {{Blocks}} in {{Two}} or {{More Dimensions}}},
  abstract = {This paper describes an extension, to higher dimensions, of the Bayesian Blocks algorithm for estimating signals in noisy time series data (Scargle 1998, 2000). The mathematical problem is to find the partition of the data space with the maximum posterior probability for a model consisting of a homogeneous Poisson process for each partition element. For model M\_\{n\}, attributing the data within region n of the data space to a Poisson process with a fixed event rate lambda\_\{n\}, the global posterior is: P(M\_\{n\}) = Phi(N,V) = Gamma(N+1)Gamma(V-N+1) / Gamma(V+2) = N!(V-N)!/(V+1)! . Note that lambda\_\{n\} does not appear, since it has been marginalized, using a flat, improper prior. Other priors yield similar formulas. This expression is valid for a data space of any dimension. It depends on only N, the number of data points within the region, and V, the volume of the region. No information about the actual locations of the points enters this expression. Suppose two such regions, described by N\_\{1\},V\_\{1\} and N\_\{2\},V\_\{2\}, are candidates for being merged into one. From the above equation, construct a Bayes merge factor, giving the ratio of posteriors for the two regions merged and not merged, respectively: P(Merge) = Phi(N\_\{1\}+N\_\{2\},V\_\{1\}+V\_\{2\}) / Phi(N\_\{1\},V\_\{1\}) Phi(N\_\{2\},V\_\{2\}) . Then collect data points into blocks with a greedy cell coalescence algorithm.},
  journaltitle = {AIP Conference Proceedings},
  urldate = {2019-02-13},
  date = {2002},
  pages = {163-173},
  keywords = {bayesian blocks,binning,histogram,Mathematics - Numerical Analysis},
  author = {Scargle, Jeffrey D.},
  file = {/Users/michelsen/Zotero/storage/3S4U7A3L/Scargle - 2002 - Bayesian Blocks in Two or More Dimensions Image S.pdf;/Users/michelsen/Zotero/storage/NWST82BQ/0111128.html}
}

@online{leszczynskiSeveralTipsBuilding2018,
  title = {Several Tips on Building {{Machine Learning}} Pipelines},
  url = {https://medium.com/asap-report/ml-pipelines-python-lifehacks-7f00356e0977},
  abstract = {Based on processing EEG signals in python for seizure prediction.},
  journaltitle = {Medium},
  urldate = {2019-02-13},
  date = {2018-12-03T22:37:38.249Z},
  keywords = {log,logging,parse,parsing},
  author = {Leszczyński, Krzysztof},
  file = {/Users/michelsen/Zotero/storage/62XNIHFR/ml-pipelines-python-lifehacks-7f00356e0977.html}
}

@online{ThoseDeceivingError2014,
  langid = {english},
  title = {Those {{Deceiving Error Bars}} | {{Science}} 2.0},
  url = {https://www.science20.com/quantum_diaries_survivor/those_deceiving_error_bars-85735},
  abstract = {Have you ever looked at a histogram with the data displayed as counts per bin in the form of points with error bars, and wondered whether those fluctuations and departures from the underlying hypothesized model (usually overimposed as a continuous line or histogram) were really significant or worth ignoring ?},
  urldate = {2019-02-13},
  date = {2014-08-27T00:00:00Z},
  keywords = {uncertainty,error bars,poisson},
  file = {/Users/michelsen/Zotero/storage/4UIRLMEY/those_deceiving_error_bars-85735.html}
}

@online{NextLevelData,
  title = {The {{Next Level}} of {{Data Visualization}} in {{Python}} – {{Towards Data Science}}},
  url = {https://towardsdatascience.com/the-next-level-of-data-visualization-in-python-dd6e99039d5e},
  urldate = {2019-02-13}
}

@article{varmaBiasErrorEstimation2006a,
  title = {Bias in Error Estimation When Using Cross-Validation for Model Selection},
  volume = {7},
  issn = {1471-2105},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1397873/},
  doi = {10.1186/1471-2105-7-91},
  abstract = {Background
Cross-validation (CV) is an effective method for estimating the prediction error of a classifier. Some recent articles have proposed methods for optimizing classifiers by choosing classifier parameter values that minimize the CV error estimate. We have evaluated the validity of using the CV error estimate of the optimized classifier as an estimate of the true error expected on independent data.

Results
We used CV to optimize the classification parameters for two kinds of classifiers; Shrunken Centroids and Support Vector Machines (SVM). Random training datasets were created, with no difference in the distribution of the features between the two classes. Using these "null" datasets, we selected classifier parameter values that minimized the CV error estimate. 10-fold CV was used for Shrunken Centroids while Leave-One-Out-CV (LOOCV) was used for the SVM. Independent test data was created to estimate the true error. With "null" and "non null" (with differential expression between the classes) data, we also tested a nested CV procedure, where an inner CV loop is used to perform the tuning of the parameters while an outer CV is used to compute an estimate of the error., The CV error estimate for the classifier with the optimal parameters was found to be a substantially biased estimate of the true error that the classifier would incur on independent data. Even though there is no real difference between the two classes for the "null" datasets, the CV error estimate for the Shrunken Centroid with the optimal parameters was less than 30\% on 18.5\% of simulated training data-sets. For SVM with optimal parameters the estimated error rate was less than 30\% on 38\% of "null" data-sets. Performance of the optimized classifiers on the independent test set was no better than chance., The nested CV procedure reduces the bias considerably and gives an estimate of the error that is very close to that obtained on the independent testing set for both Shrunken Centroids and SVM classifiers for "null" and "non-null" data distributions.

Conclusion
We show that using CV to compute an error estimate for a classifier that has itself been tuned using CV gives a significantly biased estimate of the true error. Proper use of CV for estimating true error of a classifier developed using a well defined algorithm requires that all steps of the algorithm, including classifier parameter tuning, be repeated in each CV loop. A nested CV procedure provides an almost unbiased estimate of the true error.},
  journaltitle = {BMC Bioinformatics},
  shortjournal = {BMC Bioinformatics},
  urldate = {2019-02-13},
  date = {2006-02-23},
  pages = {91},
  keywords = {cross validation,bias,CV,k-fold,nested},
  author = {Varma, Sudhir and Simon, Richard},
  file = {/Users/michelsen/Zotero/storage/SMCVYBTQ/Varma and Simon - 2006 - Bias in error estimation when using cross-validati.pdf},
  eprinttype = {pmid},
  eprint = {16504092},
  pmcid = {PMC1397873}
}

@article{kingmaAdamMethodStochastic2014a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6980},
  primaryClass = {cs},
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  url = {http://arxiv.org/abs/1412.6980},
  shorttitle = {Adam},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  urldate = {2019-02-13},
  date = {2014-12-22},
  keywords = {Computer Science - Machine Learning,optimization,adam,OP,stochastic},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  file = {/Users/michelsen/Zotero/storage/N6QZ67UM/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf;/Users/michelsen/Zotero/storage/YTSSN4HH/1412.html}
}

@article{bakerClarificationUseCHIsquare1984a,
  title = {Clarification of the Use of {{CHI}}-Square and Likelihood Functions in Fits to Histograms},
  volume = {221},
  issn = {0167-5087},
  url = {http://www.sciencedirect.com/science/article/pii/0167508784900164},
  doi = {10.1016/0167-5087(84)90016-4},
  abstract = {We consider the problem of fitting curves to histograms in which the data obey multinomial or Poisson statistics. Techniques commonly used by physicists are examined in light of standard results found in the statistics literature. We review the relationship between multinomial and Poisson distributions, and clarify a sufficient condition for equality of the area under the fitted curve and the number of events on the histogram. Following the statisticians, we use the likelihood ratio test to construct a general χ2 statistic, χλ2, which yields parameter and error estimates identical to those of the method of maximum likelihood. The χλ2 statist further useful for testing goodness-of-fit since the value of its minimum asymptotically obeys a classical chi-square distribution. One should be aware, however, of the potential for statistical bias, especially when the number of events is small.},
  number = {2},
  journaltitle = {Nuclear Instruments and Methods in Physics Research},
  shortjournal = {Nuclear Instruments and Methods in Physics Research},
  urldate = {2019-02-13},
  date = {1984-04-01},
  pages = {437-442},
  keywords = {binning,histogram,OP,chi,chi2,fitting,likelihod},
  author = {Baker, Steve and Cousins, Robert D.},
  file = {/Users/michelsen/Zotero/storage/NAR4YQ6I/Baker and Cousins - 1984 - Clarification of the use of CHI-square and likelih.pdf;/Users/michelsen/Zotero/storage/UEHELQ7B/0167508784900164.html}
}

@online{laboratoriesModulesPackagesHow,
  title = {Modules and Packages: How to Create a {{Python}} Project},
  url = {http://www.internalpointers.com/post/modules-and-packages-create-python-project},
  shorttitle = {Modules and Packages},
  abstract = {A quick and dirty tutorial on how to get things done.},
  journaltitle = {Internal Pointers},
  urldate = {2019-02-13},
  keywords = {python,module,package},
  author = {Laboratories, Monocasual},
  file = {/Users/michelsen/Zotero/storage/AFJRL9QZ/modules-and-packages-create-python-project.html}
}

@online{defilippiCookiecutterDataScience2018,
  title = {Cookiecutter {{Data Science}} — {{Organize}} Your {{Projects}} — {{Atom}} and {{Jupyter}}},
  url = {https://medium.com/@rrfd/cookiecutter-data-science-organize-your-projects-atom-and-jupyter-2be7862f487e},
  abstract = {One thing prevalent in most data science departments is messy notebooks and messy code. There are examples of beautiful notebooks out…},
  journaltitle = {Medium},
  urldate = {2019-02-13},
  date = {2018-09-23T02:40:12.270Z},
  keywords = {cookiecutter,organize,project},
  author = {DeFilippi, Robert R. F.},
  file = {/Users/michelsen/Zotero/storage/66YHA825/cookiecutter-data-science-organize-your-projects-atom-and-jupyter-2be7862f487e.html}
}

@online{HowWriteGit,
  title = {How to {{Write}} a {{Git Commit Message}}},
  url = {https://chris.beams.io/posts/git-commit/#imperative},
  urldate = {2019-02-13},
  keywords = {git,guide,commit},
  file = {/Users/michelsen/Zotero/storage/6GTPL9Y3/git-commit.html}
}

@article{scargleSTUDIESASTRONOMICALTIME2013a,
  langid = {english},
  title = {{{STUDIES IN ASTRONOMICAL TIME SERIES ANALYSIS}}. {{VI}}. {{BAYESIAN BLOCK REPRESENTATIONS}}},
  volume = {764},
  issn = {0004-637X, 1538-4357},
  url = {http://stacks.iop.org/0004-637X/764/i=2/a=167?key=crossref.0539dc6f37f29e250567031865ebbe9a},
  doi = {10.1088/0004-637X/764/2/167},
  number = {2},
  journaltitle = {The Astrophysical Journal},
  urldate = {2019-02-13},
  date = {2013-02-04},
  pages = {167},
  keywords = {bayesian blocks,binning,histogram},
  author = {Scargle, Jeffrey D. and Norris, Jay P. and Jackson, Brad and Chiang, James},
  file = {/Users/michelsen/Zotero/storage/WWYBDAHU/Scargle et al. - 2013 - STUDIES IN ASTRONOMICAL TIME SERIES ANALYSIS. VI. .pdf}
}

@article{icecubecollaborationIceCubeNeutrinoObservatory2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.01201},
  primaryClass = {astro-ph},
  title = {The {{IceCube Neutrino Observatory}} - {{Contributions}} to {{ICRC}} 2017 {{Part V}}: {{Solar}} Flares, {{Supernovae}}, {{Event}} Reconstruction, {{Education}} \& {{Outreach}}},
  url = {http://arxiv.org/abs/1710.01201},
  shorttitle = {The {{IceCube Neutrino Observatory}} - {{Contributions}} to {{ICRC}} 2017 {{Part V}}},
  abstract = {Papers on solar flares, supernovae, event reconstruction and education \& outreach, submitted to the 35th International Cosmic Ray Conference (ICRC 2017, Busan, South Korea) by the IceCube Collaboration},
  urldate = {2019-02-13},
  date = {2017-10-03},
  keywords = {bayesian blocks,binning,histogram,Astrophysics - High Energy Astrophysical Phenomena},
  author = {IceCube Collaboration and Aartsen, M. G. and Ackermann, M. and Adams, J. and Aguilar, J. A. and Ahlers, M. and Ahrens, M. and Samarai, I. Al and Altmann, D. and Andeen, K. and Anderson, T. and Ansseau, I. and Anton, G. and Argüelles, C. and Auffenberg, J. and Axani, S. and Bagherpour, H. and Bai, X. and Barron, J. P. and Barwick, S. W. and Baum, V. and Bay, R. and Beatty, J. J. and Tjus, J. Becker and Becker, K.-H. and BenZvi, S. and Berley, D. and Bernardini, E. and Besson, D. Z. and Binder, G. and Bindig, D. and Blaufuss, E. and Blot, S. and Bohm, C. and Börner, M. and Bos, F. and Bose, D. and Böser, S. and Botner, O. and Bourbeau, E. and Bourbeau, J. and Bradascio, F. and Braun, J. and Brayeur, L. and Brenzke, M. and Bretz, H.-P. and Bron, S. and Brostean-Kaiser, J. and Burgman, A. and Carver, T. and Casey, J. and Casier, M. and Cheung, E. and Chirkin, D. and Christov, A. and Clark, K. and Classen, L. and Collin, G. H. and Conrad, J. M. and Cowen, D. F. and Cross, R. and Day, M. and de André, J. P. A. M. and De Clercq, C. and DeLaunay, J. J. and Dembinski, H. and De Ridder, S. and Desiati, P. and de Vries, K. D. and de Wasseige, G. and de With, M. and DeYoung, T. and Díaz-Vélez, J. C. and di Lorenzo, V. and Dujmovic, H. and Dumm, J. P. and Dunkman, M. and Dvorak, E. and Eberhardt, B. and Ehrhardt, T. and Eichmann, B. and Eller, P. and Evenson, P. A. and Fahey, S. and Fazely, A. R. and Felde, J. and Filimonov, K. and Finley, C. and Flis, S. and Franckowiak, A. and Friedman, E. and Gaisser, T. K. and Gallagher, J. and Gerhardt, L. and Ghorbani, K. and Giang, W. and Glauch, T. and Glüsenkamp, T. and Goldschmidt, A. and Gonzalez, J. G. and Grant, D. and Griffith, Z. and Haack, C. and Hallgren, A. and Halzen, F. and Hanson, K. and Hebecker, D. and Heereman, D. and Helbing, K. and Hellauer, R. and Hickford, S. and Hignight, J. and Hill, G. C. and Hoffman, K. D. and Hoffmann, R. and Hokanson-Fasig, B. and Hoshina, K. and Huang, F. and Huber, M. and Hultqvist, K. and Hünnefeld, M. and In, S. and Ishihara, A. and Jacobi, E. and Japaridze, G. S. and Jeong, M. and Jero, K. and Jones, B. J. P. and Kalaczynski, P. and Kang, W. and Kappes, A. and Karg, T. and Karle, A. and Katz, U. and Kauer, M. and Keivani, A. and Kelley, J. L. and Kheirandish, A. and Kim, J. and Kim, M. and Kintscher, T. and Kiryluk, J. and Kittler, T. and Klein, S. R. and Koirala, R. and Kolanoski, H. and Köpke, L. and Kopper, C. and Kopper, S. and Koschinsky, J. P. and Koskinen, D. J. and Kowalski, M. and Krings, K. and Kroll, M. and Krückl, G. and Kunnen, J. and Kunwar, S. and Kurahashi, N. and Kuwabara, T. and Kyriacou, A. and Labare, M. and Lanfranchi, J. L. and Larson, M. J. and Lauber, F. and Lesiak-Bzdak, M. and Leuermann, M. and Liu, Q. R. and Lu, L. and Lünemann, J. and Luszczak, W. and Madsen, J. and Maggi, G. and Mahn, K. B. M. and Mancina, S. and Maruyama, R. and Mase, K. and Maunu, R. and McNally, F. and Meagher, K. and Medici, M. and Meier, M. and Menne, T. and Merino, G. and Meures, T. and Miarecki, S. and Micallef, J. and Momenté, G. and Montaruli, T. and Moore, R. W. and Moulai, M. and Nahnhauer, R. and Nakarmi, P. and Naumann, U. and Neer, G. and Niederhausen, H. and Nowicki, S. C. and Nygren, D. R. and Pollmann, A. Obertacke and Olivas, A. and O'Murchadha, A. and Palczewski, T. and Pandya, H. and Pankova, D. V. and Peiffer, P. and Pepper, J. A. and de los Heros, C. Pérez and Pieloth, D. and Pinat, E. and Plum, M. and Price, P. B. and Przybylski, G. T. and Raab, C. and Rädel, L. and Rameez, M. and Rawlins, K. and Rea, I. C. and Reimann, R. and Relethford, B. and Relich, M. and Resconi, E. and Rhode, W. and Richman, M. and Robertson, S. and Rongen, M. and Rott, C. and Ruhe, T. and Ryckbosch, D. and Rysewyk, D. and Sälzer, T. and Herrera, S. E. Sanchez and Sandrock, A. and Sandroos, J. and Santander, M. and Sarkar, S. and Sarkar, S. and Satalecka, K. and Schlunder, P. and Schmidt, T. and Schneider, A. and Schoenen, S. and Schöneberg, S. and Schumacher, L. and Seckel, D. and Seunarine, S. and Soedingrekso, J. and Soldin, D. and Song, M. and Spiczak, G. M. and Spiering, C. and Stachurska, J. and Stamatikos, M. and Stanev, T. and Stasik, A. and Stettner, J. and Steuer, A. and Stezelberger, T. and Stokstad, R. G. and Stößl, A. and Strotjohann, N. L. and Stuttard, T. and Sullivan, G. W. and Sutherland, M. and Taboada, I. and Tatar, J. and Tenholt, F. and Ter-Antonyan, S. and Terliuk, A. and Tilav, S. and Toale, P. A. and Tobin, M. N. and Toscano, S. and Tosi, D. and Tselengidou, M. and Tung, C. F. and Turcati, A. and Turley, C. F. and Ty, B. and Unger, E. and Usner, M. and Vandenbroucke, J. and Van Driessche, W. and van Eijndhoven, N. and Vanheule, S. and van Santen, J. and Vogel, E. and Vraeghe, M. and Walck, C. and Wallace, A. and Wallraff, M. and Wandler, F. D. and Wandkowsky, N. and Waza, A. and Weaver, C. and Weiss, M. J. and Wendt, C. and Werthebach, J. and Westerhoff, S. and Whelan, B. J. and Wiebe, K. and Wiebusch, C. H. and Wille, L. and Williams, D. R. and Wills, L. and Wolf, M. and Wood, J. and Wood, T. R. and Woolsey, E. and Woschnagg, K. and Xu, D. L. and Xu, X. W. and Xu, Y. and Yanez, J. P. and Yodh, G. and Yoshida, S. and Yuan, T. and Zoll, M.},
  options = {useprefix=true},
  file = {/Users/michelsen/Zotero/storage/K8MMVVAN/IceCube Collaboration et al. - 2017 - The IceCube Neutrino Observatory - Contributions t.pdf;/Users/michelsen/Zotero/storage/7ZPD6MEH/1710.html}
}

@online{petrouMinimallySufficientPandas2019,
  title = {Minimally {{Sufficient Pandas}}},
  url = {https://medium.com/dunder-data/minimally-sufficient-pandas-a8e67f2a2428},
  abstract = {In this article, I will offer an opinionated perspective on how to best use the Pandas library for data analysis. My objective is to argue…},
  journaltitle = {Dunder Data},
  urldate = {2019-02-15},
  date = {2019-01-30T20:57:42.030Z},
  keywords = {guide,pandas},
  author = {Petrou, Ted},
  file = {/Users/michelsen/Zotero/storage/E6ZJFAX7/minimally-sufficient-pandas-a8e67f2a2428.html}
}

@article{swamidassCROCStrongerROC2010a,
  title = {A {{CROC}} Stronger than {{ROC}}: Measuring, Visualizing and Optimizing Early Retrieval},
  volume = {26},
  issn = {1367-4803},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2865862/},
  doi = {10.1093/bioinformatics/btq140},
  shorttitle = {A {{CROC}} Stronger than {{ROC}}},
  abstract = {Motivation: The performance of classifiers is often assessed using Receiver Operating Characteristic ROC [or (AC) accumulation curve or enrichment curve] curves and the corresponding areas under the curves (AUCs). However, in many fundamental problems ranging from information retrieval to drug discovery, only the very top of the ranked list of predictions is of any interest and ROCs and AUCs are not very useful. New metrics, visualizations and optimization tools are needed to address this ‘early retrieval’ problem., Results: To address the early retrieval problem, we develop the general concentrated ROC (CROC) framework. In this framework, any relevant portion of the ROC (or AC) curve is magnified smoothly by an appropriate continuous transformation of the coordinates with a corresponding magnification factor. Appropriate families of magnification functions confined to the unit square are derived and their properties are analyzed together with the resulting CROC curves. The area under the CROC curve (AUC[CROC]) can be used to assess early retrieval. The general framework is demonstrated on a drug discovery problem and used to discriminate more accurately the early retrieval performance of five different predictors. From this framework, we propose a novel metric and visualization—the CROC(exp), an exponential transform of the ROC curve—as an alternative to other methods. The CROC(exp) provides a principled, flexible and effective way for measuring and visualizing early retrieval performance with excellent statistical power. Corresponding methods for optimizing early retrieval are also described in the ., Availability: Datasets are publicly available. Python code and command-line utilities implementing CROC curves and metrics are available at http://pypi.python.org/pypi/CROC/, Contact: pfbaldi@ics.uci.edu},
  number = {10},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  urldate = {2019-02-15},
  date = {2010-05-15},
  pages = {1348-1356},
  keywords = {roc,croc,imbalance},
  author = {Swamidass, S. Joshua and Azencott, Chloé-Agathe and Daily, Kenny and Baldi, Pierre},
  file = {/Users/michelsen/Zotero/storage/IF9A2ET8/Swamidass et al. - 2010 - A CROC stronger than ROC measuring, visualizing a.pdf},
  eprinttype = {pmid},
  eprint = {20378557},
  pmcid = {PMC2865862}
}

@article{saitoPrecisionRecallPlotMore2015a,
  langid = {english},
  title = {The {{Precision}}-{{Recall Plot Is More Informative}} than the {{ROC Plot When Evaluating Binary Classifiers}} on {{Imbalanced Datasets}}},
  volume = {10},
  issn = {1932-6203},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432},
  doi = {10.1371/journal.pone.0118432},
  abstract = {Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics (ROC) plots. Alternative measures such as positive predictive value (PPV) and the associated Precision/Recall (PRC) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While ROC plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether ROC plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. PRC plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use ROC plots on imbalanced datasets.},
  number = {3},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2019-02-15},
  date = {2015-03-04},
  pages = {e0118432},
  keywords = {Bioinformatics,Caenorhabditis elegans,Exponential functions,Genome analysis,Genome-wide association studies,Interpolation,MicroRNAs,Support vector machines,precision,recall,roc,imbalance,PRC},
  author = {Saito, Takaya and Rehmsmeier, Marc},
  file = {/Users/michelsen/Zotero/storage/8YQN7JSI/Saito and Rehmsmeier - 2015 - The Precision-Recall Plot Is More Informative than.pdf;/Users/michelsen/Zotero/storage/BK9X4R7W/article.html}
}

@article{halmosHOWWRITEMATHEMATICS,
  langid = {english},
  title = {{{HOW TO WRITE MATHEMATICS}}},
  pages = {22},
  keywords = {how to write mathematics},
  author = {Halmos, Paul R},
  file = {/Users/michelsen/Zotero/storage/USZZQ765/Halmos - HOW TO WRITE MATHEMATICS.pdf}
}

@article{al-asadiInferenceVisualizationDNA,
  langid = {english},
  title = {Inference and Visualization of {{DNA}} Damage Patterns Using a Grade of Membership Model},
  abstract = {Motivation: Quality control plays a major role in the analysis of ancient DNA (aDNA). One key step in this quality control is assessment of DNA damage: aDNA contains unique signatures of DNA damage that distinguish it from modern DNA, and so analyses of damage patterns can help confirm that DNA sequences obtained are from endogenous aDNA rather than from modern contamination. Predominant signatures of DNA damage include a high frequency of cytosine to thymine substitutions (C-to-T) at the ends of fragments, and elevated rates of purines (A \& G) before the 50 strand-breaks. Existing QC procedures help assess damage by simply plotting for each sample, the C-to-T mismatch rate along the read and the composition of bases before the 50 strand-breaks. Here we present a more flexible and comprehensive model-based approach to infer and visualize damage patterns in aDNA, implemented in an R package aRchaic. This approach is based on a ‘grade of membership’ model (also known as ‘admixture’ or ‘topic’ model) in which each sample has an estimated grade of membership in each of K damage profiles that are estimated from the data.},
  pages = {7},
  keywords = {adna,ancient dna,damage,damage pattern,mismatch},
  author = {Al-Asadi, Hussein and Dey, Kushal K and Novembre, John and Stephens, Matthew},
  file = {/Users/michelsen/Zotero/storage/UXRKCH8R/Al-Asadi et al. - Inference and visualization of DNA damage patterns.pdf}
}

@article{briggsPatternsDamageGenomic2007,
  title = {Patterns of Damage in Genomic {{DNA}} Sequences from a {{Neandertal}}},
  volume = {104},
  issn = {0027-8424},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1976210/},
  doi = {10.1073/pnas.0704665104},
  abstract = {High-throughput direct sequencing techniques have recently opened the possibility to sequence genomes from Pleistocene organisms. Here we analyze DNA sequences determined from a Neandertal, a mammoth, and a cave bear. We show that purines are overrepresented at positions adjacent to the breaks in the ancient DNA, suggesting that depurination has contributed to its degradation. We furthermore show that substitutions resulting from miscoding cytosine residues are vastly overrepresented in the DNA sequences and drastically clustered in the ends of the molecules, whereas other substitutions are rare. We present a model where the observed substitution patterns are used to estimate the rate of deamination of cytosine residues in single- and double-stranded portions of the DNA, the length of single-stranded ends, and the frequency of nicks. The results suggest that reliable genome sequences can be obtained from Pleistocene organisms.},
  number = {37},
  journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
  shortjournal = {Proc Natl Acad Sci U S A},
  urldate = {2019-03-04},
  date = {2007-09-11},
  pages = {14616-14621},
  keywords = {adna,ancient dna,damage,damage pattern,mismatch},
  author = {Briggs, Adrian W. and Stenzel, Udo and Johnson, Philip L. F. and Green, Richard E. and Kelso, Janet and Prüfer, Kay and Meyer, Matthias and Krause, Johannes and Ronan, Michael T. and Lachmann, Michael and Pääbo, Svante},
  file = {/Users/michelsen/Zotero/storage/8RS8X78U/Briggs et al. - 2007 - Patterns of damage in genomic DNA sequences from a.pdf},
  eprinttype = {pmid},
  eprint = {17715061},
  pmcid = {PMC1976210}
}

@article{huangMaximumLikelihoodEstimation,
  langid = {english},
  title = {Maximum {{Likelihood Estimation}} of {{Dirichlet Distribution Parameters}}},
  abstract = {Dirichlet distributions are commonly used as priors over proportional data. In this paper, I will introduce this distribution, discuss why it is useful, and compare implementations of 4 different methods for estimating its parameters from observed data.},
  pages = {9},
  keywords = {dirichlet,lda,mle},
  author = {Huang, Jonathan},
  file = {/Users/michelsen/Zotero/storage/NNFM2LI2/Huang - Maximum Likelihood Estimation of Dirichlet Distrib.pdf}
}

@book{molnarShapleyValuesInterpretable,
  title = {5.8 {{Shapley Values}} | {{Interpretable Machine Learning}}},
  url = {https://christophm.github.io/interpretable-ml-book/shapley.html},
  abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners on how to make machine learning decisions more interpretable.},
  urldate = {2019-04-15},
  keywords = {shap,shapley},
  author = {Molnar, Christoph},
  file = {/Users/michelsen/Zotero/storage/JM7KTFW2/shapley.html}
}

@online{tomarTopicModelingUsing2018,
  title = {Topic Modeling Using {{LDA}} and {{Gibbs Sampling}} Explained!!},
  url = {https://medium.com/@tomar.ankur287/topic-modeling-using-lda-and-gibbs-sampling-explained-49d49b3d1045},
  abstract = {Hi everyone. It’s been a long time since I wrote my last blog on the different type of recommender systems. This time, I will be writing…},
  journaltitle = {Medium},
  urldate = {2019-04-15},
  date = {2018-11-25T01:34:20.867Z},
  keywords = {lda,gibbs,gibbs sampling,topic modelling},
  author = {Tomar, Ankur},
  file = {/Users/michelsen/Zotero/storage/42GBL246/topic-modeling-using-lda-and-gibbs-sampling-explained-49d49b3d1045.html}
}

@article{udellGeneralizedLowRank2016,
  langid = {english},
  title = {Generalized {{Low Rank Models}}},
  volume = {9},
  issn = {1935-8237, 1935-8245},
  url = {http://www.nowpublishers.com/article/Details/MAL-055},
  doi = {10.1561/2200000055},
  number = {1},
  journaltitle = {Foundations and Trends® in Machine Learning},
  urldate = {2019-04-15},
  date = {2016},
  pages = {1-118},
  keywords = {OP,generalized low rank model,GLRM,pca},
  author = {Udell, Madeleine and Horn, Corinne and Zadeh, Reza and Boyd, Stephen},
  file = {/Users/michelsen/Zotero/storage/IBZXY5IN/Udell et al. - 2016 - Generalized Low Rank Models.pdf}
}

@article{belkinReconcilingModernMachine2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.11118},
  primaryClass = {cs, stat},
  title = {Reconciling Modern Machine Learning and the Bias-Variance Trade-Off},
  url = {http://arxiv.org/abs/1812.11118},
  abstract = {The question of generalization in machine learning---how algorithms are able to learn predictors from a training sample to make accurate predictions out-of-sample---is revisited in light of the recent breakthroughs in modern machine learning technology. The classical approach to understanding generalization is based on bias-variance trade-offs, where model complexity is carefully calibrated so that the fit on the training sample reflects performance out-of-sample. However, it is now common practice to fit highly complex models like deep neural networks to data with (nearly) zero training error, and yet these interpolating predictors are observed to have good out-of-sample accuracy even for noisy data. How can the classical understanding of generalization be reconciled with these observations from modern machine learning practice? In this paper, we bridge the two regimes by exhibiting a new "double descent" risk curve that extends the traditional U-shaped bias-variance curve beyond the point of interpolation. Specifically, the curve shows that as soon as the model complexity is high enough to achieve interpolation on the training sample---a point that we call the "interpolation threshold"---the risk of suitably chosen interpolating predictors from these models can, in fact, be decreasing as the model complexity increases, often below the risk achieved using non-interpolating models. The double descent risk curve is demonstrated for a broad range of models, including neural networks and random forests, and a mechanism for producing this behavior is posited.},
  urldate = {2019-04-15},
  date = {2018-12-28},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,bias,bias-variance,complexity,double descent,interpolating,overfitting,trade-off,variance},
  author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  file = {/Users/michelsen/Zotero/storage/VSSVAWBY/Belkin et al. - 2018 - Reconciling modern machine learning and the bias-v.pdf;/Users/michelsen/Zotero/storage/XZBX6S9D/1812.html}
}

@online{mcclureStepbyStepGuideCreating2019,
  title = {Step-by-{{Step Guide}} to {{Creating R}} and {{Python Libraries}} (in {{JupyterLab}})},
  url = {https://towardsdatascience.com/step-by-step-guide-to-creating-r-and-python-libraries-e81bbea87911},
  abstract = {R and Python are the bread and butter of today’s machine learning languages. R provides powerful statistics and quick visualizations…},
  journaltitle = {Towards Data Science},
  urldate = {2019-04-15},
  date = {2019-03-30T22:07:52.188Z},
  keywords = {module,package,github},
  author = {McClure, Sean}
}

@online{csefalvayJITFastSupercharge2019,
  title = {{{JIT}} Fast! {{Supercharge}} Tensor Processing in {{Python}} with {{JIT}} Compilation},
  url = {https://medium.com/starschema-blog/jit-fast-supercharge-tensor-processing-in-python-with-jit-compilation-47598de6ee96},
  abstract = {Numba can speed up tensor processing work in Python by an order of magnitude. Read on for one of the easiest ways to speed up your code!},
  journaltitle = {Medium},
  urldate = {2019-04-15},
  date = {2019-03-23T18:17:46.168Z},
  keywords = {jit,numba},
  author = {von Csefalvay, Chris},
  file = {/Users/michelsen/Zotero/storage/4SB3L8EU/jit-fast-supercharge-tensor-processing-in-python-with-jit-compilation-47598de6ee96.html}
}

@article{barronGeneralAdaptiveRobust2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.03077},
  primaryClass = {cs, stat},
  title = {A {{General}} and {{Adaptive Robust Loss Function}}},
  url = {http://arxiv.org/abs/1701.03077},
  abstract = {We present a generalization of the Cauchy/Lorentzian, Geman-McClure, Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2 loss functions. By introducing robustness as a continuous parameter, our loss function allows algorithms built around robust loss minimization to be generalized, which improves performance on basic vision tasks such as registration and clustering. Interpreting our loss as the negative log of a univariate density yields a general probability distribution that includes normal and Cauchy distributions as special cases. This probabilistic interpretation enables the training of neural networks in which the robustness of the loss automatically adapts itself during training, which improves performance on learning-based tasks such as generative image synthesis and unsupervised monocular depth estimation, without requiring any manual parameter tuning.},
  urldate = {2019-06-24},
  date = {2017-01-11},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Computer Vision and Pattern Recognition,loss,robust},
  author = {Barron, Jonathan T.},
  file = {/Users/michelsen/Zotero/storage/4GP7EKNA/Barron - 2017 - A General and Adaptive Robust Loss Function.pdf;/Users/michelsen/Zotero/storage/L5QDUA37/1701.html}
}

@inproceedings{shivaswamyPermutationInvariantSVMs2006,
  langid = {english},
  location = {{Pittsburgh, Pennsylvania}},
  title = {Permutation Invariant {{SVMs}}},
  isbn = {978-1-59593-383-6},
  url = {http://portal.acm.org/citation.cfm?doid=1143844.1143947},
  doi = {10.1145/1143844.1143947},
  abstract = {We extend Support Vector Machines to input spaces that are sets by ensuring that the classifier is invariant to permutations of subelements within each input. Such permutations include reordering of scalars in an input vector, re-orderings of tuples in an input matrix or re-orderings of general objects (in Hilbert spaces) within a set as well. This approach induces permutational invariance in the classifier which can then be directly applied to unusual set-based representations of data. The permutation invariant Support Vector Machine alternates the Hungarian method for maximum weight matching within the maximum margin learning procedure. We effectively estimate and apply permutations to the input data points to maximize classification margin while minimizing data radius. This procedure has a strong theoretical justification via well established error probability bounds. Experiments are shown on character recognition, 3D object recognition and various UCI datasets.},
  eventtitle = {The 23rd International Conference},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning  - {{ICML}} '06},
  publisher = {{ACM Press}},
  urldate = {2019-09-04},
  date = {2006},
  pages = {817-824},
  author = {Shivaswamy, Pannagadatta K. and Jebara, Tony},
  file = {/Users/michelsen/Zotero/storage/ZVXGHVM6/Shivaswamy and Jebara - 2006 - Permutation invariant SVMs.pdf}
}

@article{zaheerDeepSets2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.06114},
  primaryClass = {cs, stat},
  title = {Deep {{Sets}}},
  url = {http://arxiv.org/abs/1703.06114},
  abstract = {We study the problem of designing models for machine learning tasks defined on \textbackslash{}emph\{sets\}. In contrast to traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets that are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics \textbackslash{}cite\{poczos13aistats\}, to anomaly detection in piezometer data of embankment dams \textbackslash{}cite\{Jung15Exploration\}, to cosmology \textbackslash{}cite\{Ntampaka16Dynamical,Ravanbakhsh16ICML1\}. Our main theorem characterizes the permutation invariant functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We also derive the necessary and sufficient conditions for permutation equivariance in deep models. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.},
  urldate = {2019-09-04},
  date = {2017-03-10},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,invariant,permutation,deep sets,equivariant,permutation invariant},
  author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Ruslan and Smola, Alexander},
  file = {/Users/michelsen/Zotero/storage/97DAN9SX/Zaheer et al. - 2017 - Deep Sets.pdf;/Users/michelsen/Zotero/storage/NDYYAT86/1703.html}
}

@article{pollackBayesianBlockHistogramming2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1708.00810},
  primaryClass = {hep-ex, physics:physics},
  title = {Bayesian {{Block Histogramming}} for {{High Energy Physics}}},
  url = {http://arxiv.org/abs/1708.00810},
  abstract = {The Bayesian Block algorithm, originally developed for applications in astronomy, can be used to improve the binning of histograms in high energy physics. The visual improvement can be dramatic, as shown here with two simple examples. More importantly, this algorithm and the histogram is produces is a non-parametric density estimate, providing a description of background distributions that does not suffer from the arbitrariness of ad hoc analytical functions. The statistical power of an hypothesis test based on Bayesian Blocks is nearly as good as that obtained by fitting analytical functions. Two examples are provided: a narrow peak on a smoothly-falling background, and an excess in the tail of a background that falls rapidly over several orders of magnitude. These examples show the usefulness of the binning provided by the Bayesian Blocks algorithm both for presentation of data and when searching for new physics.},
  urldate = {2019-10-01},
  date = {2017-08-02},
  keywords = {High Energy Physics - Experiment,Physics - Data Analysis; Statistics and Probability,bayesian blocks,blocks},
  author = {Pollack, Brian and Bhattacharya, Saptaparna and Schmitt, Michael},
  file = {/Users/michelsen/Zotero/storage/CWXH5YH5/Pollack et al. - 2017 - Bayesian Block Histogramming for High Energy Physi.pdf;/Users/michelsen/Zotero/storage/GXNCDZQS/1708.html}
}

@online{bishopMixedDensityNetworks1994,
  title = {{{MixedDensityNetworks}}},
  url = {https://eprints.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf},
  shorttitle = {Mixed {{Density Networks}}},
  urldate = {2019-10-01},
  date = {1994-02},
  author = {Bishop, Christopher},
  file = {/Users/michelsen/Zotero/storage/KVH45RTT/NCRG_94_004.pdf}
}

@article{baileyNotNormalUncertainties2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.00778},
  title = {Not {{Normal}}: The Uncertainties of Scientific Measurements},
  volume = {4},
  issn = {2054-5703, 2054-5703},
  url = {http://arxiv.org/abs/1612.00778},
  doi = {10.1098/rsos.160600},
  shorttitle = {Not {{Normal}}},
  abstract = {Judging the significance and reproducibility of quantitative research requires a good understanding of relevant uncertainties, but it is often unclear how well these have been evaluated and what they imply. Reported scientific uncertainties were studied by analysing 41000 measurements of 3200 quantities from medicine, nuclear and particle physics, and interlaboratory comparisons ranging from chemistry to toxicology. Outliers are common, with 5\{\textbackslash{}sigma\} disagreements up to five orders of magnitude more frequent than naively expected. Uncertainty-normalized differences between multiple measurements of the same quantity are consistent with heavy-tailed Student-t distributions that are often almost Cauchy, far from a Gaussian Normal bell curve. Medical research uncertainties are generally as well evaluated as those in physics, but physics uncertainty improves more rapidly, making feasible simple significance criteria such as the 5\{\textbackslash{}sigma\} discovery convention in particle physics. Contributions to measurement uncertainty from mistakes and unknown problems are not completely unpredictable. Such errors appear to have power-law distributions consistent with how designed complex systems fail, and how unknown systematic errors are constrained by researchers. This better understanding may help improve analysis and meta-analysis of data, and help scientists and the public have more realistic expectations of what scientific results imply.},
  number = {1},
  journaltitle = {Royal Society Open Science},
  shortjournal = {R. Soc. open sci.},
  urldate = {2019-10-01},
  date = {2017-01},
  pages = {160600},
  keywords = {Physics - Data Analysis; Statistics and Probability,uncertainties,Statistics - Applications,normal,Not Normal},
  author = {Bailey, David C.},
  file = {/Users/michelsen/Zotero/storage/I8J3SI8I/Bailey - 2017 - Not Normal the uncertainties of scientific measur.pdf;/Users/michelsen/Zotero/storage/ME2JZXKM/1612.html}
}

@article{killoranGeneratingDesigningDNA2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.06148},
  primaryClass = {cs, q-bio, stat},
  title = {Generating and Designing {{DNA}} with Deep Generative Models},
  url = {http://arxiv.org/abs/1712.06148},
  abstract = {We propose generative neural network methods to generate DNA sequences and tune them to have desired properties. We present three approaches: creating synthetic DNA sequences using a generative adversarial network; a DNA-based variant of the activation maximization ("deep dream") design method; and a joint procedure which combines these two approaches together. We show that these tools capture important structures of the data and, when applied to designing probes for protein binding microarrays, allow us to generate new sequences whose properties are estimated to be superior to those found in the training data. We believe that these results open the door for applying deep generative models to advance genomics research.},
  urldate = {2019-10-01},
  date = {2017-12-17},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,dna,Quantitative Biology - Genomics,GAN},
  author = {Killoran, Nathan and Lee, Leo J. and Delong, Andrew and Duvenaud, David and Frey, Brendan J.},
  file = {/Users/michelsen/Zotero/storage/H96ZJIXN/Killoran et al. - 2017 - Generating and designing DNA with deep generative .pdf;/Users/michelsen/Zotero/storage/4ICGP6CQ/1712.html}
}

@article{ngDna2vecConsistentVector2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.06279},
  primaryClass = {cs, q-bio, stat},
  title = {Dna2vec: {{Consistent}} Vector Representations of Variable-Length k-Mers},
  url = {http://arxiv.org/abs/1701.06279},
  shorttitle = {Dna2vec},
  abstract = {One of the ubiquitous representation of long DNA sequence is dividing it into shorter k-mer components. Unfortunately, the straightforward vector encoding of k-mer as a one-hot vector is vulnerable to the curse of dimensionality. Worse yet, the distance between any pair of one-hot vectors is equidistant. This is particularly problematic when applying the latest machine learning algorithms to solve problems in biological sequence analysis. In this paper, we propose a novel method to train distributed representations of variable-length k-mers. Our method is based on the popular word embedding model word2vec, which is trained on a shallow two-layer neural network. Our experiments provide evidence that the summing of dna2vec vectors is akin to nucleotides concatenation. We also demonstrate that there is correlation between Needleman-Wunsch similarity score and cosine similarity of dna2vec vectors.},
  urldate = {2019-10-01},
  date = {2017-01-23},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Computation and Language,dna,Quantitative Biology - Quantitative Methods,dna2vec,embedding},
  author = {Ng, Patrick},
  file = {/Users/michelsen/Zotero/storage/YZQUSFFJ/Ng - 2017 - dna2vec Consistent vector representations of vari.pdf;/Users/michelsen/Zotero/storage/CHA7Y2FC/1701.html}
}

@article{zouPrimerDeepLearning2019,
  langid = {english},
  title = {A Primer on Deep Learning in Genomics},
  volume = {51},
  issn = {1546-1718},
  url = {https://www.nature.com/articles/s41588-018-0295-5},
  doi = {10.1038/s41588-018-0295-5},
  abstract = {This perspective presents a primer on deep learning applications for the genomics field. It includes a general guide for how to use deep learning and describes the current tools and resources that are available to the community.},
  number = {1},
  journaltitle = {Nature Genetics},
  shortjournal = {Nat Genet},
  urldate = {2019-10-01},
  date = {2019-01},
  pages = {12-18},
  author = {Zou, James and Huss, Mikael and Abid, Abubakar and Mohammadi, Pejman and Torkamani, Ali and Telenti, Amalio},
  file = {/Users/michelsen/Zotero/storage/CZI2WPXV/Zou et al. - 2019 - A primer on deep learning in genomics.pdf;/Users/michelsen/Zotero/storage/2MM3PEKQ/s41588-018-0295-5.html}
}

@article{zouPrimerDeepLearning2019a,
  langid = {english},
  title = {A Primer on Deep Learning in Genomics},
  volume = {51},
  issn = {1061-4036, 1546-1718},
  url = {http://www.nature.com/articles/s41588-018-0295-5},
  doi = {10.1038/s41588-018-0295-5},
  number = {1},
  journaltitle = {Nature Genetics},
  shortjournal = {Nat Genet},
  urldate = {2019-10-01},
  date = {2019-01},
  pages = {12-18},
  author = {Zou, James and Huss, Mikael and Abid, Abubakar and Mohammadi, Pejman and Torkamani, Ali and Telenti, Amalio},
  file = {/Users/michelsen/Zotero/storage/QSRIYGHK/Zou et al. - 2019 - A primer on deep learning in genomics.pdf}
}

@article{daleyModelingGenomeCoverage2014,
  title = {Modeling Genome Coverage in Single-Cell Sequencing},
  volume = {30},
  issn = {1367-4803},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4221128/},
  doi = {10.1093/bioinformatics/btu540},
  abstract = {Motivation: Single-cell DNA sequencing is necessary for examining genetic variation at the cellular level, which remains hidden in bulk sequencing experiments. But because they begin with such small amounts of starting material, the amount of information that is obtained from single-cell sequencing experiment is highly sensitive to the choice of protocol employed and variability in library preparation. In particular, the fraction of the genome represented in single-cell sequencing libraries exhibits extreme variability due to quantitative biases in amplification and loss of genetic material., Results: We propose a method to predict the genome coverage of a deep sequencing experiment using information from an initial shallow sequencing experiment mapped to a reference genome. The observed coverage statistics are used in a non-parametric empirical Bayes Poisson model to estimate the gain in coverage from deeper sequencing. This approach allows researchers to know statistical features of deep sequencing experiments without actually sequencing deeply, providing a basis for optimizing and comparing single-cell sequencing protocols or screening libraries., Availability and implementation: The method is available as part of the preseq software package. Source code is available at http://smithlabresearch.org/preseq., Contact:
andrewds@usc.edu, Supplementary information:
Supplementary material is available at Bioinformatics online.},
  number = {22},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  urldate = {2019-10-01},
  date = {2014-11-15},
  pages = {3159-3165},
  author = {Daley, Timothy and Smith, Andrew D.},
  file = {/Users/michelsen/Zotero/storage/6ZK3XXI9/Daley and Smith - 2014 - Modeling genome coverage in single-cell sequencing.pdf},
  eprinttype = {pmid},
  eprint = {25107873},
  pmcid = {PMC4221128}
}

@article{fanSelectiveOverviewDeep2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.05526},
  primaryClass = {cs, math, stat},
  title = {A {{Selective Overview}} of {{Deep Learning}}},
  url = {http://arxiv.org/abs/1904.05526},
  abstract = {Deep learning has arguably achieved tremendous success in recent years. In simple words, deep learning uses the composition of many nonlinear functions to model the complex dependency between input features and labels. While neural networks have a long history, recent advances have greatly improved their performance in computer vision, natural language processing, etc. From the statistical and scientific perspective, it is natural to ask: What is deep learning? What are the new characteristics of deep learning, compared with classical methods? What are the theoretical foundations of deep learning? To answer these questions, we introduce common neural network models (e.g., convolutional neural nets, recurrent neural nets, generative adversarial nets) and training techniques (e.g., stochastic gradient descent, dropout, batch normalization) from a statistical point of view. Along the way, we highlight new characteristics of deep learning (including depth and over-parametrization) and explain their practical and theoretical benefits. We also sample recent results on theories of deep learning, many of which are only suggestive. While a complete understanding of deep learning remains elusive, we hope that our perspectives and discussions serve as a stimulus for new statistical research.},
  urldate = {2019-10-01},
  date = {2019-04-10},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Mathematics - Statistics Theory,Statistics - Methodology},
  author = {Fan, Jianqing and Ma, Cong and Zhong, Yiqiao},
  file = {/Users/michelsen/Zotero/storage/YBUHFDLU/Fan et al. - 2019 - A Selective Overview of Deep Learning.pdf;/Users/michelsen/Zotero/storage/3FTFH83L/1904.html}
}

@article{guttenbergPermutationequivariantNeuralNetworks2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.04530},
  primaryClass = {cs, stat},
  title = {Permutation-Equivariant Neural Networks Applied to Dynamics Prediction},
  url = {http://arxiv.org/abs/1612.04530},
  abstract = {The introduction of convolutional layers greatly advanced the performance of neural networks on image tasks due to innately capturing a way of encoding and learning translation-invariant operations, matching one of the underlying symmetries of the image domain. In comparison, there are a number of problems in which there are a number of different inputs which are all 'of the same type' --- multiple particles, multiple agents, multiple stock prices, etc. The corresponding symmetry to this is permutation symmetry, in that the algorithm should not depend on the specific ordering of the input data. We discuss a permutation-invariant neural network layer in analogy to convolutional layers, and show the ability of this architecture to learn to predict the motion of a variable number of interacting hard discs in 2D. In the same way that convolutional layers can generalize to different image sizes, the permutation layer we describe generalizes to different numbers of objects.},
  urldate = {2019-10-01},
  date = {2016-12-14},
  keywords = {Statistics - Machine Learning,Computer Science - Computer Vision and Pattern Recognition,permutation},
  author = {Guttenberg, Nicholas and Virgo, Nathaniel and Witkowski, Olaf and Aoki, Hidetoshi and Kanai, Ryota},
  file = {/Users/michelsen/Zotero/storage/BXW3NK3P/Guttenberg et al. - 2016 - Permutation-equivariant neural networks applied to.pdf;/Users/michelsen/Zotero/storage/YYKBE66Z/1612.html}
}

@article{meyerNuclearDNASequences2016,
  langid = {english},
  title = {Nuclear {{DNA}} Sequences from the {{Middle Pleistocene Sima}} de Los {{Huesos}} Hominins},
  volume = {531},
  issn = {0028-0836, 1476-4687},
  url = {http://www.nature.com/articles/nature17405},
  doi = {10.1038/nature17405},
  number = {7595},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2019-10-01},
  date = {2016-03},
  pages = {504-507},
  author = {Meyer, Matthias and Arsuaga, Juan-Luis and de Filippo, Cesare and Nagel, Sarah and Aximu-Petri, Ayinuer and Nickel, Birgit and Martínez, Ignacio and Gracia, Ana and de Castro, José María Bermúdez and Carbonell, Eudald and Viola, Bence and Kelso, Janet and Prüfer, Kay and Pääbo, Svante},
  options = {useprefix=true},
  file = {/Users/michelsen/Zotero/storage/UUNXMIBU/Meyer et al. - 2016 - Nuclear DNA sequences from the Middle Pleistocene .pdf}
}

@article{meyerNuclearDNASequences2016a,
  langid = {english},
  title = {Nuclear {{DNA}} Sequences from the {{Middle Pleistocene Sima}} de Los {{Huesos}} Hominins},
  volume = {531},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/nature17405},
  doi = {10.1038/nature17405},
  abstract = {A unique assemblage of 28 hominin individuals, found in Sima de los Huesos in the Sierra de Atapuerca in Spain, has recently been dated to approximately 430,000 years ago1. An interesting question is how these Middle Pleistocene hominins were related to those who lived in the Late Pleistocene epoch, in particular to Neanderthals in western Eurasia and to Denisovans, a sister group of Neanderthals so far known only from southern Siberia. While the Sima de los Huesos hominins share some derived morphological features with Neanderthals, the mitochondrial genome retrieved from one individual from Sima de los Huesos is more closely related to the mitochondrial DNA of Denisovans than to that of Neanderthals2. However, since the mitochondrial DNA does not reveal the full picture of relationships among populations, we have investigated DNA preservation in several individuals found at Sima de los Huesos. Here we recover nuclear DNA sequences from two specimens, which show that the Sima de los Huesos hominins were related to Neanderthals rather than to Denisovans, indicating that the population divergence between Neanderthals and Denisovans predates 430,000 years ago. A mitochondrial DNA recovered from one of the specimens shares the previously described relationship to Denisovan mitochondrial DNAs, suggesting, among other possibilities, that the mitochondrial DNA gene pool of Neanderthals turned over later in their history.},
  number = {7595},
  journaltitle = {Nature},
  urldate = {2019-10-01},
  date = {2016-03},
  pages = {504-507},
  author = {Meyer, Matthias and Arsuaga, Juan-Luis and de Filippo, Cesare and Nagel, Sarah and Aximu-Petri, Ayinuer and Nickel, Birgit and Martínez, Ignacio and Gracia, Ana and de Castro, José María Bermúdez and Carbonell, Eudald and Viola, Bence and Kelso, Janet and Prüfer, Kay and Pääbo, Svante},
  options = {useprefix=true},
  file = {/Users/michelsen/Zotero/storage/4J452WUH/Meyer et al. - 2016 - Nuclear DNA sequences from the Middle Pleistocene .pdf;/Users/michelsen/Zotero/storage/8STA35B9/nature17405.html}
}

@online{bourbeauPyUnfoldPythonPackage2018,
  langid = {english},
  title = {{{PyUnfold}}: {{A Python}} Package for Iterative Unfolding},
  url = {https://joss.theoj.org},
  doi = {10.21105/joss.00741},
  shorttitle = {{{PyUnfold}}},
  abstract = {Bourbeau et al., (2018). PyUnfold: A Python package for iterative unfolding. Journal of Open Source Software, 3(26), 741, https://doi.org/10.21105/joss.00741},
  journaltitle = {Journal of Open Source Software},
  urldate = {2019-10-01},
  date = {2018-06-04},
  keywords = {PyUnfold},
  author = {Bourbeau, James and Hampel-Arias, Zigfried},
  file = {/Users/michelsen/Zotero/storage/DXFX6XIT/Bourbeau and Hampel-Arias - 2018 - PyUnfold A Python package for iterative unfolding.pdf;/Users/michelsen/Zotero/storage/Y7JNBDEC/joss.html}
}

@online{Theory,
  title = {Some {{Theory}}},
  url = {https://jlmelville.github.io/smallvis/theory.html#umap},
  urldate = {2019-10-01},
  file = {/Users/michelsen/Zotero/storage/9LU4NVBZ/theory.html}
}

@article{buskulicPerformanceALEPHDetector,
  langid = {english},
  title = {Performance of the {{ALEPH}} Detector at {{LEP}}},
  abstract = {The performance of the ALEPH detector at the LEP e+e‘ collider is reviewed. The accuracy of the tracking detectors to measure the impact parameter and momentum of charged tracks is specified. Calorimeters are used to measure photons and neutral hadrons, and the accuracy obtained in energy and angle is given. An essential property of the detector is its ability to identify particles; the performance in identification of electrons, muons, neutrinos (from missing energy), charged hadrons, ·rr° ’s and V° ’s is described.},
  pages = {45},
  author = {Buskulic, Damir},
  file = {/Users/michelsen/Zotero/storage/82MTWWWL/Buskulic - Performance of the ALEPH detector at LEP.pdf}
}

@article{buskulicPreciseMeasurementGZ1993,
  title = {A Precise Measurement of {{ΓZ}}→bb/{{ΓZ}}→hadrons},
  volume = {313},
  issn = {0370-2693},
  url = {http://www.sciencedirect.com/science/article/pii/037026939390028G},
  doi = {10.1016/0370-2693(93)90028-G},
  abstract = {A measurement of the partial width ratio ΓbbΓhad using a method which tags the Z → bb decays through the lif etime of the produced heavy hadrons is presented. This method relies on the tracking precision afforded by a double-sided silicon vertex detector. The tag algorithm makes a probabilistic interpretation of three-dimensional track impact parameters, using the data to measure the resolution. By tagging the two b hadrons separately, both ΓbbΓhad and the tag efficiency can be determined from the data. For a 26\% efficiency of tagging a single b hadron within the vertex detector solid angle coverage, a purity of 96\% is achieved. A value of ΓbbΓhad = 0.2192±0.0026(stat.)±0.0016(Γcc/ Γhad) is found. Combining this result with other recent ALEPH ΓbbΓhad measurements gives a 95\% confidence upper limit on the Standard Model top mass of Mt {$<$} 228 GeV.},
  number = {3},
  journaltitle = {Physics Letters B},
  shortjournal = {Physics Letters B},
  urldate = {2019-10-15},
  date = {1993-09-02},
  pages = {535-548},
  keywords = {b-tag},
  author = {Buskulic, D. and De Bonis, I. and Decamp, D. and Ghez, P. and Goy, C. and Lees, J. -P. and Minard, M. -N. and Pietrzyk, B. and Ariztizabal, F. and Comas, P. and Crespo, J. M. and Delfino, M. and Efthymiopoulos, I. and Fernandez, E. and Fernandez-Bosman, M. and Gaitan, V. and Garrido, Ll. and Mattison, T. and Pacheco, A. and Padilla, C. and Pascual, A. and Creanza, D. and de Palma, M. and Farilla, A. and Iaselli, G. and Maggi, G. and Natali, S. and Nuzzo, S. and Quattromini, M. and Ranieri, A. and Raso, G. and Romano, F. and Ruggieri, F. and Selvaggi, G. and Silvestris, L. and Tempesta, P. and Zito, G. and Chai, Y. and Hu, H. and Huang, D. and Huang, X. and Lin, J. and Wang, T. and Xie, Y. and Xu, D. and Xu, R. and Zhang, J. and Zhang, L. and Zhao, W. and Blucher, E. and Bonvicini, G. and Boudreau, J. and Casper, D. and Drevermann, H. and Forty, R. W. and Ganis, G. and Gay, C. and Hagelberg, R. and Harvey, J. and Hilgart, J. and Jacobsen, R. and Jost, B. and Knobloch, J. and Lehraus, I. and Lohse, T. and Maggi, M. and Markou, C. and Martinez, M. and Mato, P. and Meinhard, H. and Minten, A. and Miotto, A. and Miquel, R. and Moser, H. -G. and Palazzi, P. and Pater, J. R. and Perlas, J. A. and Pusztaszeri, J. -F. and Ranjard, F. and Redlinger, G. and Rolandi, L. and Rothberg, J. and Ruan, T. and Saich, M. and Schlatter, D. and Schmelling, M. and Sefkow, F. and Tejessy, W. and Tomalin, I. R. and Veenhof, R. and Wachsmuth, H. and Wasserbaech, S. and Wiedenmann, W. and Wildish, T. and Witzeling, W. and Wotschack, J. and Ajaltouni, Z. and Badaud, F. and Bardadin-Otwinowska, M. and El Fellous, R. and Falvard, A. and Gay, P. and Guicheney, C. and Henrard, P. and Jousset, J. and Michel, B. and Montret, J. -C. and Pallin, D. and Perret, P. and Podlyski, F. and Proriol, J. and Prulhière, F. and Saadi, F. and Fearnley, T. and Hansen, J. B. and Hansen, J. D. and Hansen, J. R. and Hansen, P. H. and M∅llerud, R. and Nilsson, B. S. and Kyriakis, A. and Simopoulou, E. and Siotis, I. and Vayaki, A. and Zachariadou, K. and Badier, J. and Blondel, A. and Bonneaud, G. and Brient, J. C. and Fouque, G. and Orteu, S. and Rougé, A. and Rumpf, M. and Tanaka, R. and Verderi, M. and Videau, H. and Candlin, D. J. and Parsons, M. I. and Veitch, E. and Focardi, E. and Moneta, L. and Parrini, G. and Corden, M. and Georgiopoulos, C. and Ikeda, M. and Levinthal, D. and Antonelli, A. and Baldini, R. and Bencivenni, G. and Bologna, G. and Bossi, F. and Campana, P. and Capon, G. and Cerutti, F. and Chiarella, V. and D'Ettorre-Piazzoli, B. and Felici, G. and Laurelli, P. and Mannocchi, G. and Murtas, F. and Murtas, G. P. and Passalacqua, L. and Pepe-Altarelli, M. and Picchi, P. and Colrain, P. and ten Have, I. and Lynch, J. G. and Maitland, W. and Morton, W. T. and Raine, C. and Reeves, P. and Scarr, J. M. and Smith, K. and Smith, M. G. and Thompson, A. S. and Turnbull, R. M. and Brandl, B. and Braun, O. and Geweniger, C. and Hanke, P. and Hepp, V. and Kluge, E. E. and Maumary, Y. and Putzer, A. and Rensch, B. and Stahl, A. and Tittel, K. and Wunsch, M. and Beuselinck, R. and Binnie, D. M. and Cameron, W. and Cattaneo, M. and Colling, D. J. and Dornan, P. J. and Greene, A. M. and Hassard, J. F. and Lieske, N. M. and Moutoussi, A. and Nash, J. and Patton, S. and Payne, D. G. and Phillips, M. J. and San Martin, G. and Sedgbeer, J. K. and Wright, A. G. and Girtler, P. and Kuhn, D. and Rudolph, G. and Vogl, R. and Bowdery, C. K. and Brodbeck, T. J. and Finch, A. J. and Foster, F. and Hughes, G. and Jackson, D. and Keemer, N. R. and Nuttall, M. and Patel, A. and Sloan, T. and Snow, S. W. and Whelan, E. P. and Kleinknecht, K. and Raab, J. and Renk, B. and Sander, H. -G. and Schmidt, H. and Steeg, F. and Walther, S. M. and Wanke, R. and Wolf, B. and Bencheikh, A. M. and Benchouk, C. and Bonissent, A. and Carr, J. and Coyle, P. and Drinkard, J. and Etienne, F. and Nicod, D. and Papalexiou, S. and Payre, P. and Roos, L. and Rousseau, D. and Schwemling, P. and Talby, M. and Adlung, S. and Assmann, R. and Bauer, C. and Blum, W. and Brown, D. and Cattaneo, P. and Dehning, B. and Dietl, H. and Dydak, F. and Frank, M. and Halley, A. W. and Jakobs, K. and Lauber, J. and Lütjens, G. and Lutz, G. and Männer, W. and Richter, R. and Schröder, J. and Schwarz, A. S. and Settles, R. and Seywerd, H. and Stierlin, U. and Stiegler, U. and St. Denis, R. and Wolf, G. and Alemany, R. and Boucrot, J. and Callot, O. and Cordier, A. and Davier, M. and Duflot, L. and Grivaz, J. -F. and Heusse, Ph. and Jaffe, D. E. and Janot, P. and Kim, D. W. and Le Diberder, F. and Lefrançois, J. and Lutz, A. -M. and Schune, M. -H. and Veillet, J. -J. and Videau, I. and Zhang, Z. and Abbaneo, D. and Bagliesi, G. and Batignani, G. and Bottigli, U. and Bozzi, C. and Calderini, G. and Carpinelli, M. and Ciocci, M. A. and Dell'Orso, R. and Ferrante, I. and Fidecaro, F. and Foà, L. and Forti, F. and Giassi, A. and Giorgi, M. A. and Gregorio, A. and Ligabue, F. and Lusiani, A. and Mannelli, E. B. and Marrocchesi, P. S. and Messineo, A. and Palla, F. and Rizzo, G. and Sanguinetti, G. and Spagnolo, P. and Steinberger, J. and Tenchini, R. and Tonelli, G. and Triggiani, G. and Vannini, C. and Venturi, A. and Verdini, P. G. and Walsh, J. and Betteridge, A. P. and Gao, Y. and Green, M. G. and March, P. V. and Mir, Ll. M. and Medcalf, T. and Quazi, I. S. and Strong, J. A. and West, L. R. and Botterill, D. R. and Clifft, R. W. and Edgecock, T. R. and Haywood, S. and Norton, P. R. and Thompson, J. C. and Bloch-Devaux, B. and Colas, P. and Duarte, H. and Emery, S. and Kozanecki, W. and Lançon, E. and Lemaire, M. C. and Locci, E. and Marx, B. and Perez, P. and Rander, J. and Renardy, J. -F. and Rosowsky, A. and Roussarie, A. and Schuller, J. -P. and Schwindling, J. and Si Mohand, D. and Vallage, B. and Johnson, R. P. and Litke, A. M. and Taylor, G. and Wear, J. and Ashman, J. G. and Babbage, W. and Booth, C. N. and Buttar, C. and Cartwright, S. and Combley, F. and Dawson, I. and Thompson, L. F. and Barberio, E. and Böhrer, A. and Brandt, S. and Cowan, G. and Grupen, C. and Lutters, G. and Rivera, F. and Schäfer, U. and Smolik, L. and Bosisio, L. and Marina, R. Della and Giannini, G. and Gobbo, B. and Ragusa, F. and Bellantoni, L. and Chen, W. and Conway, J. S. and Feng, Z. and Ferguson, D. P. S. and Gao, Y. S. and Grahl, J. and Harton, J. L. and Hayes, O. J. and Nachtman, J. M. and Pan, Y. B. and Saadi, Y. and Schmitt, M. and Scott, I. and Sharma, V. and Shi, Z. H. and Turk, J. D. and Walsh, A. M. and Weber, F. V. and {Sau Lan Wu} and Wu, X. and Zheng, M. and Zobernig, G.},
  options = {useprefix=true},
  file = {/Users/michelsen/Zotero/storage/3PCNDHYN/Buskulic et al. - 1993 - A precise measurement of ΓZ→bbΓZ→hadrons.pdf;/Users/michelsen/Zotero/storage/9DYC8PK3/037026939390028G.html}
}

@article{buskulicMeasurementRatioGbbGhad1993,
  title = {Measurement of the Ratio {{ΓbbΓhad}} Using Event Shape Variables},
  volume = {313},
  issn = {0370-2693},
  url = {http://www.sciencedirect.com/science/article/pii/037026939390029H},
  doi = {10.1016/0370-2693(93)90029-H},
  abstract = {The branching fraction of Z → bb relative to all hadronic decays of the Z has been measured using event shape variables to preferentially select Z → bb events. The method chosen applies a combination of shape discriminators and the selection of high transverse momentum leptons to event hemispheres. From a sample of 440 000 hadronic Z decays collected with the ALEPH detector at LEP, the ration ΓbbΓhad = 0.228±0.005(stat.)±0.005(syst.) is measured.},
  number = {3},
  journaltitle = {Physics Letters B},
  shortjournal = {Physics Letters B},
  urldate = {2019-10-15},
  date = {1993-09-02},
  pages = {549-563},
  author = {Buskulic, D. and De Bonis, I. and Decamp, D. and Ghez, P. and Goy, C. and Lees, J. -P. and Minard, M. -N. and Pietrzyk, B. and Ariztizabal, F. and Comas, P. and Crespo, J. M. and Delfino, M. and Efthymiopoulos, I. and Fernandez, E. and Fernandez-Bosman, M. and Gaitan, V. and Garrido, Ll. and Mattison, T. and Pacheco, A. and Padilla, C. and Pascual, A. and Creanza, D. and de Palma, M. and Farilla, A. and Iaselli, G. and Maggi, G. and Natali, S. and Nuzzo, S. and Quattromini, M. and Ranieri, A. and Raso, G. and Romano, F. and Ruggieri, F. and Selvaggi, G. and Silvestris, L. and Tempesta, P. and Zito, G. and Chai, Y. and Hu, H. and Huang, D. and Huang, X. and Lin, J. and Wang, T. and Xie, Y. and Xie, D. and Xu, D. and Xu, R. and Zhang, J. and Zhang, L. and Zhao, W. and Blucher, E. and Bonvicini, G. and Boudreau, J. and Casper, D. and Drevermann, H. and Forty, R. W. and Ganis, G. and Gay, C. and Hagelberg, R. and Harvey, J. and Hilgart, J. and Jacobsen, R. and Jost, B. and Knobloch, J. and Lehraus, I. and Lohse, T. and Maggi, M. and Markou, C. and Martinez, M. and Mato, P. and Meinhard, H. and Minten, A. and Miotto, A. and Miquel, R. and Moser, H. -G. and Palazzi, P. and Pater, J. R. and Perlas, J. A. and Pusztaszeri, J. -F. and Ranjard, F. and Redlinger, G. and Rolandi, L. and Rothberg, J. and Ruan, T. and Saich, M. and Schlatter, D. and Schmelling, M. and Sefkow, F. and Tejessy, W. and Tomalin, I. R. and Veenhof, R. and Wachsmuth, H. and Wasserbaech, S. and Wiedenmann, W. and Wildish, T. and Witzeling, W. and Wotschack, J. and Ajaltouni, Z. and Badaud, F. and Bardadin-Otwinowska, M. and El Fellous, R. and Falvard, A. and Gay, P. and Guicheney, C. and Henrard, P. and Jousset, J. and Michel, B. and Montret, J-C. and Pallin, D. and Perret, P. and Podlyski, F. and Proriol, J. and Prulhière, F. and Saadi, F. and Fearnley, T. and Hansen, J. B. and Hansen, J. D. and Hansen, J. R. and Hansen, P. H. and Møllerud, R. and Nilsson, B. S. and Kyriakis, A. and Simopoulou, E. and Siotis, I. and Vayaki, A. and Zachariadou, K. and Badier, J. and Blondel, A. and Bonneaud, G. and Brient, J. C. and Fouque, G. and Orteu, S. and Rougé, A. and Rumpf, M. and Tanaka, R. and Verderi, M. and Videau, H. and Candlin, D. J. and Parsons, M. I. and Veitch, E. and Focardi, E. and Moneta, L. and Parrini, G. and Corden, M. and Georgiopoulos, C. and Ikeda, M. and Levinthal, D. and Antonelli, A. and Baldini, R. and Bencivenni, G. and Bologna, G. and Bossi, F. and Campana, P. and Capon, G. and Cerutti, F. and Chiarella, V. and D'Ettorre-Piazzoli, B. and Felici, G. and Laurelli, P. and Mannocchi, G. and Murtas, F. and Murtas, G. P. and Passalacqua, L. and Pepe-Altarelli, M. and Picchi, P. and Colrain, P. and ten Have, I. and Lynch, J. G. and Maitland, W. and Morton, W. T. and Raine, C. and Reeves, P. and Scarr, J. M. and Smith, K. and Smith, M. G. and Thompson, A. S. and Turnbull, R. M. and Brandl, B. and Braun, O. and Geweniger, C. and Hanke, P. and Hepp, V. and Kluge, E. E. and Maumary, Y. and Putzer, A. and Rensch, B. and Stahl, A. and Tittel, K. and Wunsch, M. and Beuselinck, R. and Binnie, D. M. and Cameron, W. and Cattaneo, M. and Colling, D. J. and Dornan, P. J. and Greene, A. M. and Hassard, J. F. and Lieske, N. M. and Moutoussi, A. and Nash, J. and Patton, S. and Payne, D. G. and Phillips, M. J. and San Martin, G. and Sedgbeer, J. K. and Wright, A. G. and Girtler, P. and Kuhn, D. and Rudolph, G. and Vogl, R. and Bowdery, C. K. and Brodbeck, T. J. and Finch, A. J. and Foster, F. and Hughes, G. and Jackson, D. and Keemer, N. R. and Nuttall, M. and Patel, A. and Sloan, T. and Snow, S. W. and Whelan, E. P. and Kleinknecht, K. and Raab, J. and Renk, B. and Sander, H. -G. and Schmidt, H. and Steeg, F. and Walther, S. M. and Wanke, R. and Wolf, B. and Bencheikh, A. M. and Benchouk, C. and Bonissent, A. and Carr, J. and Coyle, P. and Drinkard, J. and Etienne, F. and Nicod, D. and Papalexiou, S. and Payre, P. and Roos, L. and Rousseau, D. and Schwemling, P. and Talby, M. and Adlung, S. and Assmann, R. and Bauer, C. and Blum, W. and Brown, D. and Cattaneo, P. and Dehning, B. and Dietl, H. and Dydak, F. and Frank, M. and Halley, A. W. and Jakobs, K. and Lauber, J. and Lütjens, G. and Lutz, G. and Männer, W. and Richter, R. and Schröder, J. and Schwarz, A. S. and Settles, R. and Seywerd, H. and Stierlin, U. and Stiegler, U. and St. Denis, R. and Wolf, G. and Alemany, R. and Boucrot, J. and Callot, O. and Cordier, A. and Davier, M. and Duflot, L. and Grivaz, J. -F. and Heusse, Ph. and Jaffe, D. E. and Janot, P. and Kim, D. W. and Le Diberder, F. and Lefrançois, J. and Lutz, A. -M. and Schune, M. -H. and Veillet, J. -J. and Videau, I. and Zhang, Z. and Abbaneo, D. and Bagliesi, G. and Batignani, G. and Bottigli, U. and Bozzi, C. and Calderini, G. and Carpinelli, M. and Ciocci, M. A. and Dell'Orso, R. and Ferrante, I. and Fidecaro, F. and Foà, L. and Forti, F. and Giassi, A. and Giorgi, M. A. and Gregorio, A. and Ligabue, F. and Lusiani, A. and Mannelli, E. B. and Marrocchesi, P. S. and Messineo, A. and Palla, F. and Rizzo, G. and Sanguinetti, G. and Spagnolo, P. and Steinberger, J. and Tenchini, R. and Tonelli, G. and Triggiani, G. and Vannini, C. and Venturi, A. and Verdini, P. G. and Walsh, J. and Betteridge, A. P. and Gao, Y. and Green, M. G. and March, P. V. and Mir, Ll. M. and Medcalf, T. and Quazi, I. S. and Strong, J. A. and West, L. R. and Botterill, D. R. and Clifft, R. W. and Edgecock, T. R. and Haywood, S. and Norton, P. R. and Thompson, J. C. and Bloch-Devaux, B. and Colas, P. and Duarte, H. and Emery, S. and Kozanecki, W. and Lançon, E. and Lemaire, M. C. and Locci, E. and Marx, B. and Perez, P. and Rander, J. and Renardy, J. -F. and Rosowsky, A. and Roussarie, A. and Schuller, J. -P. and Schwindling, J. and Si Mohand, D. and Vallage, B. and Johnson, R. P. and Litke, A. M. and Taylor, G. and Wear, J. and Ashman, J. G. and Babbage, W. and Booth, C. N. and Buttar, C. and Cartwright, S. and Combley, F. and Dawson, I. and Thompson, L. F. and Barberio, E. and Böhrer, A. and Brandt, S. and Cowan, G. and Grupen, C. and Lutters, G. and Rivera, F. and Schäfer, U. and Smolik, L. and Bosisio, L. and Della Marina, R. and Giannini, G. and Gobbo, B. and Ragusa, F. and Bellantoni, L. and Chen, W. and Conway, J. S. and Feng, Z. and Ferguson, D. P. S. and Gao, Y. S. and Grahl, J. and Harton, J. L. and Hayes, O. J. and Nachtman, J. M. and Pan, Y. B. and Saadi, Y. and Schmitt, M. and Scott, I. and Sharma, V. and Shi, Z. H. and Turk, J. D. and Walsh, A. M. and Weber, F. V. and Lan Wu, Sau and Wu, X. and Zheng, M. and Zobernig, G.},
  options = {useprefix=true},
  file = {/Users/michelsen/Zotero/storage/JWTE5BKL/Buskulic et al. - 1993 - Measurement of the ratio ΓbbΓhad using event shape.pdf;/Users/michelsen/Zotero/storage/BLCJY5BQ/037026939390029H.html}
}

@online{TaggingQuarksEvents,
  title = {Tagging {{B}} and {{C}} Quarks in Events in E+e- Collisoons with Neural Networks},
  url = {https://cds.cern.ch/record/245758/files/p658.pdf},
  urldate = {2019-10-15},
  file = {/Users/michelsen/Zotero/storage/URGP5NEY/p658.pdf}
}

@article{proriolTAGGINGQUARKEVENTS,
  langid = {english},
  title = {{{TAGGING B QUARK EVENTS IN ALEPH WITH NEURAL NETWORKS}} (Comparison of Different Methods : {{Neural Networks}} and {{Discriminant Analysis}})},
  abstract = {In this work we present the comparison of different methods to tag b quark events: multilayered perceptron, LVQ, discriminant analysis, combination of two methods. The sample events come from the ALEPH Monte Carlo and data.},
  pages = {27},
  keywords = {b-tag},
  author = {Proriol, J and Jousset, J and Guicheney, C and Falvard, A and Henrard, P and Pallin, D and Perret, P and Brandl, B},
  file = {/Users/michelsen/Zotero/storage/GMY4YMFV/Proriol et al. - TAGGING B QUARK EVENTS IN ALEPH WITH NEURAL NETWOR.pdf}
}

@article{lundbergConsistentIndividualizedFeature2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.03888},
  primaryClass = {cs, stat},
  title = {Consistent {{Individualized Feature Attribution}} for {{Tree Ensembles}}},
  url = {http://arxiv.org/abs/1802.03888},
  abstract = {Interpreting predictions from tree ensemble methods such as gradient boosting machines and random forests is important, yet feature attribution for trees is often heuristic and not individualized for each prediction. Here we show that popular feature attribution methods are inconsistent, meaning they can lower a feature's assigned importance when the true impact of that feature actually increases. This is a fundamental problem that casts doubt on any comparison between features. To address it we turn to recent applications of game theory and develop fast exact tree solutions for SHAP (SHapley Additive exPlanation) values, which are the unique consistent and locally accurate attribution values. We then extend SHAP values to interaction effects and define SHAP interaction values. We propose a rich visualization of individualized feature attributions that improves over classic attribution summaries and partial dependence plots, and a unique "supervised" clustering (clustering based on feature attributions). We demonstrate better agreement with human intuition through a user study, exponential improvements in run time, improved clustering performance, and better identification of influential features. An implementation of our algorithm has also been merged into XGBoost and LightGBM, see http://github.com/slundberg/shap for details.},
  urldate = {2019-10-24},
  date = {2019-03-06},
  keywords = {feature importance,shap,Computer Science - Machine Learning,Statistics - Machine Learning,shapley},
  author = {Lundberg, Scott M. and Erion, Gabriel G. and Lee, Su-In},
  file = {/Users/michelsen/Zotero/storage/UBSQV4K3/Lundberg et al. - 2019 - Consistent Individualized Feature Attribution for .pdf;/Users/michelsen/Zotero/storage/BRJLL9QJ/1802.html}
}

@article{lundbergExplainableMachinelearningPredictions2018,
  langid = {english},
  title = {Explainable Machine-Learning Predictions for the Prevention of Hypoxaemia during Surgery},
  volume = {2},
  issn = {2157-846X},
  url = {http://www.nature.com/articles/s41551-018-0304-0},
  doi = {10.1038/s41551-018-0304-0},
  number = {10},
  journaltitle = {Nature Biomedical Engineering},
  shortjournal = {Nat Biomed Eng},
  urldate = {2019-10-24},
  date = {2018-10},
  pages = {749-760},
  keywords = {feature importance,shap,shapley},
  author = {Lundberg, Scott M. and Nair, Bala and Vavilala, Monica S. and Horibe, Mayumi and Eisses, Michael J. and Adams, Trevor and Liston, David E. and Low, Daniel King-Wai and Newman, Shu-Fang and Kim, Jerry and Lee, Su-In},
  file = {/Users/michelsen/Zotero/storage/49URFQWN/Lundberg et al. - 2018 - Explainable machine-learning predictions for the p.pdf}
}

@article{lundbergExplainableAITrees2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.04610},
  primaryClass = {cs, stat},
  title = {Explainable {{AI}} for {{Trees}}: {{From Local Explanations}} to {{Global Understanding}}},
  url = {http://arxiv.org/abs/1905.04610},
  shorttitle = {Explainable {{AI}} for {{Trees}}},
  abstract = {Tree-based machine learning models such as random forests, decision trees, and gradient boosted trees are the most popular non-linear predictive models used in practice today, yet comparatively little attention has been paid to explaining their predictions. Here we significantly improve the interpretability of tree-based models through three main contributions: 1) The first polynomial time algorithm to compute optimal explanations based on game theory. 2) A new type of explanation that directly measures local feature interaction effects. 3) A new set of tools for understanding global model structure based on combining many local explanations of each prediction. We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model. These tools enable us to i) identify high magnitude but low frequency non-linear mortality risk factors in the general US population, ii) highlight distinct population sub-groups with shared risk characteristics, iii) identify non-linear interaction effects among risk factors for chronic kidney disease, and iv) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model's performance over time. Given the popularity of tree-based machine learning models, these improvements to their interpretability have implications across a broad set of domains.},
  urldate = {2019-10-24},
  date = {2019-05-11},
  keywords = {feature importance,shap,Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence,shapley},
  author = {Lundberg, Scott M. and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M. and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In},
  file = {/Users/michelsen/Zotero/storage/VX5QUSCN/Lundberg et al. - 2019 - Explainable AI for Trees From Local Explanations .pdf;/Users/michelsen/Zotero/storage/97J59AWH/1905.html}
}

@article{anderssonPartonFragmentationString1983,
  langid = {english},
  title = {Parton Fragmentation and String Dynamics},
  volume = {97},
  issn = {0370-1573},
  url = {http://www.sciencedirect.com/science/article/pii/0370157383900807},
  doi = {10.1016/0370-1573(83)90080-7},
  number = {2},
  journaltitle = {Physics Reports},
  shortjournal = {Physics Reports},
  urldate = {2019-11-11},
  date = {1983-07-01},
  pages = {31-145},
  keywords = {hadron,lund,model,string},
  author = {Andersson, B. and Gustafson, G. and Ingelman, G. and Sjöstrand, T.},
  file = {/Users/michelsen/Zotero/storage/YGXKB4YF/Andersson et al. - 1983 - Parton fragmentation and string dynamics.pdf;/Users/michelsen/Zotero/storage/W28ILZKI/0370157383900807.html}
}

@article{bartelExperimentalStudyJets1981,
  langid = {english},
  title = {Experimental Study of Jets in Electron-Positron Annihilation},
  volume = {101},
  issn = {0370-2693},
  url = {http://www.sciencedirect.com/science/article/pii/0370269381905050},
  doi = {10.1016/0370-2693(81)90505-0},
  abstract = {Data on hadron production by e+e−-annihilation at c.m. energies between 30 GeV and 36 GeV are presented and compared with two models both based on first-order QCD but using different schemes for the fragmentation of quarks and gluons into hadrons. In one model the fragmentation proceeds along the parton momenta, in the other along the colour-anticolour axes. The data are reproduced better by fragmentation along the colour axes.},
  number = {1},
  journaltitle = {Physics Letters B},
  shortjournal = {Physics Letters B},
  urldate = {2019-11-11},
  date = {1981-04-30},
  pages = {129-134},
  keywords = {JADE},
  author = {Bartel, W. and Cords, D. and Dittmann, P. and Eichler, R. and Felst, R. and Haidt, D. and Krehbiel, H. and Naroska, B. and O'Neill, L. H. and Steffen, P. and Wenninger, H. and Zhang, Y. and Elsen, E. and Helm, M. and Petersen, A. and Warming, P. and Weber, G. and Bethke, S. and Drumm, H. and Heintze, J. and Heinzelmann, G. and Hellenbrand, K. H. and Heuer, R. D. and Von Krogh, J. and Lennert, P. and Kawabata, S. and Matsumura, H. and Nozaki, T. and Olsson, J. and Rieseberg, H. and Wagner, A. and Bell, A. and Foster, F. and Hughes, G. and Wriedt, H. and Allison, J. and Ball, A. H. and Bamford, G. and Barlow, R. and Bowdery, C. and Duerdoth, I. P. and Hassard, J. F. and King, B. T. and Loebinger, F. K. and Macbeth, A. A. and McCann, H. and Mills, H. E. and Murphy, P. G. and Prosper, H. B. and Stephens, K. and Clarke, D. and Goddard, M. C. and Marshall, R. and Pearce, G. F. and Imori, M. and Kobayashi, T. and Komamiya, S. and Koshiba, M. and Minowa, M. and Nozaki, M. and Orito, S. and Sato, A. and Suda, T. and Takeda, H. and Totsuka, Y. and Watanabe, Y. and Yamada, S. and Yanagisawa, C.},
  file = {/Users/michelsen/Zotero/storage/V2C6JR5W/Bartel et al. - 1981 - Experimental study of jets in electron-positron an.pdf;/Users/michelsen/Zotero/storage/HYBVBK4I/0370269381905050.html}
}

@article{buskulicPreciseMeasurementGZ1993a,
  langid = {english},
  title = {A Precise Measurement of {{ΓZ}}→bb/{{ΓZ}}→hadrons},
  volume = {313},
  issn = {0370-2693},
  url = {http://www.sciencedirect.com/science/article/pii/037026939390028G},
  doi = {10.1016/0370-2693(93)90028-G},
  abstract = {A measurement of the partial width ratio ΓbbΓhad using a method which tags the Z → bb decays through the lif etime of the produced heavy hadrons is presented. This method relies on the tracking precision afforded by a double-sided silicon vertex detector. The tag algorithm makes a probabilistic interpretation of three-dimensional track impact parameters, using the data to measure the resolution. By tagging the two b hadrons separately, both ΓbbΓhad and the tag efficiency can be determined from the data. For a 26\% efficiency of tagging a single b hadron within the vertex detector solid angle coverage, a purity of 96\% is achieved. A value of ΓbbΓhad = 0.2192±0.0026(stat.)±0.0016(Γcc/ Γhad) is found. Combining this result with other recent ALEPH ΓbbΓhad measurements gives a 95\% confidence upper limit on the Standard Model top mass of Mt {$<$} 228 GeV.},
  number = {3},
  journaltitle = {Physics Letters B},
  shortjournal = {Physics Letters B},
  urldate = {2019-11-11},
  date = {1993-09-02},
  pages = {535-548},
  keywords = {aleph,b-tag,nnbjet},
  author = {Buskulic, D. and De Bonis, I. and Decamp, D. and Ghez, P. and Goy, C. and Lees, J. -P. and Minard, M. -N. and Pietrzyk, B. and Ariztizabal, F. and Comas, P. and Crespo, J. M. and Delfino, M. and Efthymiopoulos, I. and Fernandez, E. and Fernandez-Bosman, M. and Gaitan, V. and Garrido, Ll. and Mattison, T. and Pacheco, A. and Padilla, C. and Pascual, A. and Creanza, D. and de Palma, M. and Farilla, A. and Iaselli, G. and Maggi, G. and Natali, S. and Nuzzo, S. and Quattromini, M. and Ranieri, A. and Raso, G. and Romano, F. and Ruggieri, F. and Selvaggi, G. and Silvestris, L. and Tempesta, P. and Zito, G. and Chai, Y. and Hu, H. and Huang, D. and Huang, X. and Lin, J. and Wang, T. and Xie, Y. and Xu, D. and Xu, R. and Zhang, J. and Zhang, L. and Zhao, W. and Blucher, E. and Bonvicini, G. and Boudreau, J. and Casper, D. and Drevermann, H. and Forty, R. W. and Ganis, G. and Gay, C. and Hagelberg, R. and Harvey, J. and Hilgart, J. and Jacobsen, R. and Jost, B. and Knobloch, J. and Lehraus, I. and Lohse, T. and Maggi, M. and Markou, C. and Martinez, M. and Mato, P. and Meinhard, H. and Minten, A. and Miotto, A. and Miquel, R. and Moser, H. -G. and Palazzi, P. and Pater, J. R. and Perlas, J. A. and Pusztaszeri, J. -F. and Ranjard, F. and Redlinger, G. and Rolandi, L. and Rothberg, J. and Ruan, T. and Saich, M. and Schlatter, D. and Schmelling, M. and Sefkow, F. and Tejessy, W. and Tomalin, I. R. and Veenhof, R. and Wachsmuth, H. and Wasserbaech, S. and Wiedenmann, W. and Wildish, T. and Witzeling, W. and Wotschack, J. and Ajaltouni, Z. and Badaud, F. and Bardadin-Otwinowska, M. and El Fellous, R. and Falvard, A. and Gay, P. and Guicheney, C. and Henrard, P. and Jousset, J. and Michel, B. and Montret, J. -C. and Pallin, D. and Perret, P. and Podlyski, F. and Proriol, J. and Prulhière, F. and Saadi, F. and Fearnley, T. and Hansen, J. B. and Hansen, J. D. and Hansen, J. R. and Hansen, P. H. and M∅llerud, R. and Nilsson, B. S. and Kyriakis, A. and Simopoulou, E. and Siotis, I. and Vayaki, A. and Zachariadou, K. and Badier, J. and Blondel, A. and Bonneaud, G. and Brient, J. C. and Fouque, G. and Orteu, S. and Rougé, A. and Rumpf, M. and Tanaka, R. and Verderi, M. and Videau, H. and Candlin, D. J. and Parsons, M. I. and Veitch, E. and Focardi, E. and Moneta, L. and Parrini, G. and Corden, M. and Georgiopoulos, C. and Ikeda, M. and Levinthal, D. and Antonelli, A. and Baldini, R. and Bencivenni, G. and Bologna, G. and Bossi, F. and Campana, P. and Capon, G. and Cerutti, F. and Chiarella, V. and D'Ettorre-Piazzoli, B. and Felici, G. and Laurelli, P. and Mannocchi, G. and Murtas, F. and Murtas, G. P. and Passalacqua, L. and Pepe-Altarelli, M. and Picchi, P. and Colrain, P. and ten Have, I. and Lynch, J. G. and Maitland, W. and Morton, W. T. and Raine, C. and Reeves, P. and Scarr, J. M. and Smith, K. and Smith, M. G. and Thompson, A. S. and Turnbull, R. M. and Brandl, B. and Braun, O. and Geweniger, C. and Hanke, P. and Hepp, V. and Kluge, E. E. and Maumary, Y. and Putzer, A. and Rensch, B. and Stahl, A. and Tittel, K. and Wunsch, M. and Beuselinck, R. and Binnie, D. M. and Cameron, W. and Cattaneo, M. and Colling, D. J. and Dornan, P. J. and Greene, A. M. and Hassard, J. F. and Lieske, N. M. and Moutoussi, A. and Nash, J. and Patton, S. and Payne, D. G. and Phillips, M. J. and San Martin, G. and Sedgbeer, J. K. and Wright, A. G. and Girtler, P. and Kuhn, D. and Rudolph, G. and Vogl, R. and Bowdery, C. K. and Brodbeck, T. J. and Finch, A. J. and Foster, F. and Hughes, G. and Jackson, D. and Keemer, N. R. and Nuttall, M. and Patel, A. and Sloan, T. and Snow, S. W. and Whelan, E. P. and Kleinknecht, K. and Raab, J. and Renk, B. and Sander, H. -G. and Schmidt, H. and Steeg, F. and Walther, S. M. and Wanke, R. and Wolf, B. and Bencheikh, A. M. and Benchouk, C. and Bonissent, A. and Carr, J. and Coyle, P. and Drinkard, J. and Etienne, F. and Nicod, D. and Papalexiou, S. and Payre, P. and Roos, L. and Rousseau, D. and Schwemling, P. and Talby, M. and Adlung, S. and Assmann, R. and Bauer, C. and Blum, W. and Brown, D. and Cattaneo, P. and Dehning, B. and Dietl, H. and Dydak, F. and Frank, M. and Halley, A. W. and Jakobs, K. and Lauber, J. and Lütjens, G. and Lutz, G. and Männer, W. and Richter, R. and Schröder, J. and Schwarz, A. S. and Settles, R. and Seywerd, H. and Stierlin, U. and Stiegler, U. and St. Denis, R. and Wolf, G. and Alemany, R. and Boucrot, J. and Callot, O. and Cordier, A. and Davier, M. and Duflot, L. and Grivaz, J. -F. and Heusse, Ph. and Jaffe, D. E. and Janot, P. and Kim, D. W. and Le Diberder, F. and Lefrançois, J. and Lutz, A. -M. and Schune, M. -H. and Veillet, J. -J. and Videau, I. and Zhang, Z. and Abbaneo, D. and Bagliesi, G. and Batignani, G. and Bottigli, U. and Bozzi, C. and Calderini, G. and Carpinelli, M. and Ciocci, M. A. and Dell'Orso, R. and Ferrante, I. and Fidecaro, F. and Foà, L. and Forti, F. and Giassi, A. and Giorgi, M. A. and Gregorio, A. and Ligabue, F. and Lusiani, A. and Mannelli, E. B. and Marrocchesi, P. S. and Messineo, A. and Palla, F. and Rizzo, G. and Sanguinetti, G. and Spagnolo, P. and Steinberger, J. and Tenchini, R. and Tonelli, G. and Triggiani, G. and Vannini, C. and Venturi, A. and Verdini, P. G. and Walsh, J. and Betteridge, A. P. and Gao, Y. and Green, M. G. and March, P. V. and Mir, Ll. M. and Medcalf, T. and Quazi, I. S. and Strong, J. A. and West, L. R. and Botterill, D. R. and Clifft, R. W. and Edgecock, T. R. and Haywood, S. and Norton, P. R. and Thompson, J. C. and Bloch-Devaux, B. and Colas, P. and Duarte, H. and Emery, S. and Kozanecki, W. and Lançon, E. and Lemaire, M. C. and Locci, E. and Marx, B. and Perez, P. and Rander, J. and Renardy, J. -F. and Rosowsky, A. and Roussarie, A. and Schuller, J. -P. and Schwindling, J. and Si Mohand, D. and Vallage, B. and Johnson, R. P. and Litke, A. M. and Taylor, G. and Wear, J. and Ashman, J. G. and Babbage, W. and Booth, C. N. and Buttar, C. and Cartwright, S. and Combley, F. and Dawson, I. and Thompson, L. F. and Barberio, E. and Böhrer, A. and Brandt, S. and Cowan, G. and Grupen, C. and Lutters, G. and Rivera, F. and Schäfer, U. and Smolik, L. and Bosisio, L. and Marina, R. Della and Giannini, G. and Gobbo, B. and Ragusa, F. and Bellantoni, L. and Chen, W. and Conway, J. S. and Feng, Z. and Ferguson, D. P. S. and Gao, Y. S. and Grahl, J. and Harton, J. L. and Hayes, O. J. and Nachtman, J. M. and Pan, Y. B. and Saadi, Y. and Schmitt, M. and Scott, I. and Sharma, V. and Shi, Z. H. and Turk, J. D. and Walsh, A. M. and Weber, F. V. and {Sau Lan Wu} and Wu, X. and Zheng, M. and Zobernig, G.},
  options = {useprefix=true},
  file = {/Users/michelsen/Zotero/storage/HGS39TS6/Buskulic et al. - 1993 - A precise measurement of ΓZ→bbΓZ→hadrons.pdf;/Users/michelsen/Zotero/storage/AGLGU46S/037026939390028G.html}
}

@article{buckleyGeneralpurposeEventGenerators2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1101.2599},
  title = {General-Purpose Event Generators for {{LHC}} Physics},
  volume = {504},
  issn = {03701573},
  url = {http://arxiv.org/abs/1101.2599},
  doi = {10.1016/j.physrep.2011.03.005},
  abstract = {We review the physics basis, main features and use of general-purpose Monte Carlo event generators for the simulation of proton-proton collisions at the Large Hadron Collider. Topics included are: the generation of hard-scattering matrix elements for processes of interest, at both leading and next-to-leading QCD perturbative order; their matching to approximate treatments of higher orders based on the showering approximation; the parton and dipole shower formulations; parton distribution functions for event generators; non-perturbative aspects such as soft QCD collisions, the underlying event and diffractive processes; the string and cluster models for hadron formation; the treatment of hadron and tau decays; the inclusion of QED radiation and beyond-Standard-Model processes. We describe the principal features of the ARIADNE, Herwig++, PYTHIA 8 and SHERPA generators, together with the Rivet and Professor validation and tuning tools, and discuss the physics philosophy behind the proper use of these generators and tools. This review is aimed at phenomenologists wishing to understand better how parton-level predictions are translated into hadron-level events as well as experimentalists wanting a deeper insight into the tools available for signal and background simulation at the LHC.},
  number = {5},
  journaltitle = {Physics Reports},
  shortjournal = {Physics Reports},
  urldate = {2019-11-12},
  date = {2011-07},
  pages = {145-233},
  keywords = {High Energy Physics - Experiment,High Energy Physics - Phenomenology,hadronization,quarks,rope},
  author = {Buckley, Andy and Butterworth, Jonathan and Gieseke, Stefan and Grellscheid, David and Hoche, Stefan and Hoeth, Hendrik and Krauss, Frank and Lonnblad, Leif and Nurse, Emily and Richardson, Peter and Schumann, Steffen and Seymour, Michael H. and Sjostrand, Torbjorn and Skands, Peter and Webber, Bryan},
  file = {/Users/michelsen/Zotero/storage/U6SQ3M8Z/Buckley et al. - 2011 - General-purpose event generators for LHC physics.pdf;/Users/michelsen/Zotero/storage/FGKRHKVA/1101.html}
}

@article{badeaMeasurementsTwoparticleCorrelations2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.00489},
  primaryClass = {hep-ex},
  title = {Measurements of Two-Particle Correlations in \$e\^+e\^-\$ Collisions at 91 {{GeV}} with {{ALEPH}} Archived Data},
  url = {http://arxiv.org/abs/1906.00489},
  abstract = {Measurements of two-particle angular correlations of charged particles emitted in hadronic \$Z\$ decays are presented. The archived \$e\^+e\^-\$ annihilation data at a center-of-mass energy of 91 GeV were collected with the ALEPH detector at LEP between 1992 and 1995. The correlation functions are measured over a broad range of pseudorapidity and full azimuth as a function of charged particle multiplicity. No significant long-range correlation is observed in either the lab coordinate analysis or the thrust coordinate analysis, where the latter is sensitive to a medium expanding transverse to the color string between the outgoing \$q\textbackslash{}bar\{q\}\$ pair from \$Z\$ boson decays. The associated yield distributions in both analyses are in better agreement with the prediction from PYTHIA v6.1 event generator than from HERWIG v7.1.5. They provide new insights to showering and hadronization modeling. These results serve as an important reference to the observed long-range correlation in proton-proton, proton-nucleus, and nucleus-nucleus collisions.},
  urldate = {2019-11-12},
  date = {2019-10-09},
  keywords = {High Energy Physics - Experiment,e+e-,LEP},
  author = {Badea, Anthony and Baty, Austin and Chang, Paoti and Innocenti, Gian Michele and Maggi, Marcello and McGinn, Christopher and Peters, Michael and Sheng, Tzu-An and Thaler, Jesse and Lee, Yen-Jie},
  file = {/Users/michelsen/Zotero/storage/XFDI7KG3/Badea et al. - 2019 - Measurements of two-particle correlations in $e^+e.pdf;/Users/michelsen/Zotero/storage/QQFSHUUM/1906.html}
}

@article{fisherUseMultipleMeasurements1936,
  langid = {english},
  title = {The {{Use}} of {{Multiple Measurements}} in {{Taxonomic Problems}}},
  volume = {7},
  issn = {2050-1439},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x},
  doi = {10.1111/j.1469-1809.1936.tb02137.x},
  abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
  number = {2},
  journaltitle = {Annals of Eugenics},
  urldate = {2019-11-18},
  date = {1936},
  pages = {179-188},
  author = {Fisher, R. A.},
  file = {/Users/michelsen/Zotero/storage/ANYLFAY6/Fisher - 1936 - The Use of Multiple Measurements in Taxonomic Prob.pdf;/Users/michelsen/Zotero/storage/9SQVZIHA/j.1469-1809.1936.tb02137.html}
}

@article{andersonSpeciesProblemIris1936a,
  title = {The {{Species Problem}} in {{Iris}}},
  volume = {23},
  issn = {00266493},
  url = {www.jstor.org/stable/2394164},
  doi = {10.2307/2394164},
  number = {3},
  journaltitle = {Annals of the Missouri Botanical Garden},
  date = {1936},
  pages = {457-509},
  author = {Anderson, Edgar}
}

@online{LargeElectronPositronCollider,
  title = {The {{Large Electron}}-{{Positron Collider}} | {{CERN}}},
  url = {https://home.cern/science/accelerators/large-electron-positron-collider},
  urldate = {2019-11-18},
  keywords = {LEP_Cern},
  file = {/Users/michelsen/Zotero/storage/D3SNT8XC/large-electron-positron-collider.html}
}

@online{CERN,
  title = {{{CERN}}},
  url = {https://home.cern/},
  urldate = {2019-11-18},
  file = {/Users/michelsen/Zotero/storage/PJHBW63Y/home.cern.html}
}

@book{abu-mostafaLearningData2012,
  title = {Learning {{From Data}}},
  isbn = {978-1-60049-006-4},
  abstract = {Machine learning allows computational systems to adaptively improve their performance with experience accumulated from the observed data. Its techniques are widely applied in engineering, science, finance, and commerce. This book is designed for a short course on machine learning. It is a short course, not a hurried course. From over a decade of teaching this material, we have distilled what we believe to be the core topics that every student of the subject should know. We chose the title `learning from data' that faithfully describes what the subject is about, and made it a point to cover the topics in a story-like fashion. Our hope is that the reader can learn all the fundamentals of the subject by reading the book cover to cover. ---- Learning from data has distinct theoretical and practical tracks. In this book, we balance the theoretical and the practical, the mathematical and the heuristic. Our criterion for inclusion is relevance. Theory that establishes the conceptual framework for learning is included, and so are heuristics that impact the performance of real learning systems. ---- Learning from data is a very dynamic field. Some of the hot techniques and theories at times become just fads, and others gain traction and become part of the field. What we have emphasized in this book are the necessary fundamentals that give any student of learning from data a solid foundation, and enable him or her to venture out and explore further techniques and theories, or perhaps to contribute their own. ---- The authors are professors at California Institute of Technology (Caltech), Rensselaer Polytechnic Institute (RPI), and National Taiwan University (NTU), where this book is the main text for their popular courses on machine learning. The authors also consult extensively with financial and commercial companies on machine learning applications, and have led winning teams in machine learning competitions.},
  publisher = {{AMLBook}},
  date = {2012},
  keywords = {Learning From Data},
  author = {Abu-Mostafa, Yaser S. and Magdon-Ismail, Malik and Lin, Hsuan-Tien}
}

@online{AdvancedTopicsMachine,
  title = {Advanced {{Topics}} in {{Machine Learning}} ({{ATML}})},
  url = {https://kurser.ku.dk/course/ndak15014u},
  urldate = {2019-11-19},
  file = {/Users/michelsen/Zotero/storage/V7Z7NJUT/ndak15014u.html}
}

@inproceedings{vapnikPrinciplesRiskMinimization1991,
  location = {{San Francisco, CA, USA}},
  title = {Principles of {{Risk Minimization}} for {{Learning Theory}}},
  isbn = {978-1-55860-222-9},
  url = {http://dl.acm.org/citation.cfm?id=2986916.2987018},
  abstract = {Learning is posed as a problem of function estimation, for which two principles of solution are considered: empirical risk minimization and structural risk minimization. These two principles are applied to two different statements of the function estimation problem: global and local. Systematic improvements in prediction power are illustrated in application to zip-code recognition.},
  booktitle = {Proceedings of the 4th {{International Conference}} on {{Neural Information Processing Systems}}},
  series = {{{NIPS}}'91},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  urldate = {2019-11-19},
  date = {1991},
  pages = {831--838},
  keywords = {RiskMinimization},
  author = {Vapnik, V.},
  venue = {Denver, Colorado}
}

@book{tikhonovStabilityInverseProblems1943,
  title = {On the Stability of Inverse Problems},
  volume = {vol. 39},
  pagetotal = {195-198},
  number = {no. 5},
  series = {Doklady {{Akademii Nauk SSSR}}},
  date = {1943},
  author = {Tikhonov, A.N.}
}

@article{tibshiraniRegressionShrinkageSelection1996,
  title = {Regression {{Shrinkage}} and {{Selection}} via the {{Lasso}}},
  volume = {58},
  issn = {0035-9246},
  url = {www.jstor.org/stable/2346178},
  abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
  number = {1},
  journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
  urldate = {2019-11-21},
  date = {1996},
  pages = {267-288},
  author = {Tibshirani, Robert}
}

@online{AllstateClaimsSeverity,
  langid = {english},
  title = {Allstate {{Claims Severity}} - {{Fair Loss}}},
  url = {https://kaggle.com/c/allstate-claims-severity},
  abstract = {How severe is an insurance claim?},
  urldate = {2019-11-25},
  keywords = {fair,fair loss},
  file = {/Users/michelsen/Zotero/storage/VQ5K53RJ/24520.html}
}

@online{MachineLearningXgboostHow,
  title = {Machine Learning - {{Xgboost}}-{{How}} to Use "Mae" as Objective Function?},
  url = {https://stackoverflow.com/questions/45006341/xgboost-how-to-use-mae-as-objective-function/45370500},
  journaltitle = {Stack Overflow},
  urldate = {2019-11-25},
  keywords = {fair,loss function},
  file = {/Users/michelsen/Zotero/storage/2SMJSBK6/45370500.html}
}

@article{breimanRandomForests2001,
  langid = {english},
  title = {Random {{Forests}}},
  volume = {45},
  issn = {1573-0565},
  url = {https://doi.org/10.1023/A:1010933404324},
  doi = {10.1023/A:1010933404324},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  number = {1},
  journaltitle = {Machine Learning},
  shortjournal = {Machine Learning},
  urldate = {2019-11-26},
  date = {2001-10-01},
  pages = {5-32},
  keywords = {classification,ensemble,regression},
  author = {Breiman, Leo},
  file = {/Users/michelsen/Zotero/storage/69BGH4EU/Breiman - 2001 - Random Forests.pdf}
}

@article{JSSv059i10,
  title = {Tidy Data},
  volume = {59},
  issn = {1548-7660},
  url = {https://www.jstatsoft.org/v059/i10},
  doi = {10.18637/jss.v059.i10},
  abstract = {A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.},
  number = {10},
  journaltitle = {Journal of Statistical Software, Articles},
  date = {2014},
  pages = {1-23},
  author = {Wickham, Hadley},
  file = {/Users/michelsen/Zotero/storage/BRN5DP8V/tidy-data.pdf}
}

@online{HEPMeetsML,
  langid = {american},
  title = {{{HEP}} Meets {{ML}} Award | {{The Higgs Machine Learning Challenge}}},
  url = {https://higgsml.lal.in2p3.fr/prizes-and-award/award/},
  urldate = {2019-11-26},
  keywords = {xgboost,higgs},
  file = {/Users/michelsen/Zotero/storage/YQGISWGI/award.html}
}

@online{DmlcXgboost,
  langid = {english},
  title = {Dmlc/Xgboost},
  url = {https://github.com/dmlc/xgboost},
  abstract = {Scalable, Portable and Distributed Gradient Boosting (GBDT, GBRT or GBM) Library,  for Python, R, Java, Scala, C++ and more. Runs on single machine, Hadoop, Spark, Flink and DataFlow - dmlc/xgboost},
  journaltitle = {GitHub},
  urldate = {2019-11-26},
  keywords = {xgboost,awards},
  file = {/Users/michelsen/Zotero/storage/I7UGKIII/demo.html}
}

@article{liGentleIntroductionGradient,
  langid = {english},
  title = {A {{Gentle Introduction}} to {{Gradient Boosting}}},
  pages = {80},
  author = {Li, Cheng},
  file = {/Users/michelsen/Zotero/storage/IFFFQ3TK/Li - A Gentle Introduction to Gradient Boosting.pdf}
}

@inproceedings{freundDesiciontheoreticGeneralizationOnline1995,
  location = {{Berlin, Heidelberg}},
  title = {A Desicion-Theoretic Generalization of on-Line Learning and an Application to Boosting},
  isbn = {978-3-540-49195-8},
  abstract = {We consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update rule of Littlestone and Warmuth [10] can be adapted to this mode yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games and prediction of points in ℝn. We also show how the weight-update rule can be used to derive a new boosting algorithm which does not require prior knowledge about the performance of the weak learning algorithm.},
  booktitle = {Computational Learning Theory},
  publisher = {{Springer Berlin Heidelberg}},
  date = {1995},
  pages = {23-37},
  keywords = {AdaBoost,adaboost},
  author = {Freund, Yoav and Schapire, Robert E.},
  editor = {Vitányi, Paul},
  note = {AdaBoost}
}

@article{bergstraRandomSearchHyperparameter2012,
  title = {Random {{Search}} for {{Hyper}}-Parameter {{Optimization}}},
  volume = {13},
  issn = {1532-4435},
  url = {http://dl.acm.org/citation.cfm?id=2188385.2188395},
  abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success--they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
  journaltitle = {J. Mach. Learn. Res.},
  urldate = {2019-11-27},
  date = {2012-02},
  pages = {281--305},
  keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling},
  author = {Bergstra, James and Bengio, Yoshua},
  file = {/Users/michelsen/Zotero/storage/RX4UEZZQ/Bergstra and Bengio - 2012 - Random Search for Hyper-parameter Optimization.pdf}
}

@article{scikit-learn,
  title = {Scikit-Learn: Machine Learning in {{Python}}},
  volume = {12},
  journaltitle = {Journal of Machine Learning Research},
  date = {2011},
  pages = {2825-2830},
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.}
}

@inproceedings{Lundberg:2017,
  location = {{USA}},
  title = {A Unified Approach to Interpreting Model Predictions},
  isbn = {978-1-5108-6096-4},
  url = {http://dl.acm.org/citation.cfm?id=3295222.3295230},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  series = {{{NIPS}}'17},
  publisher = {{Curran Associates Inc.}},
  date = {2017},
  pages = {4768-4777},
  author = {Lundberg, Scott M. and Lee, Su-In},
  venue = {Long Beach, California, USA},
  acmid = {3295230},
  numpages = {10}
}

@incollection{Shapley1953,
  title = {A Value for N-Person Games},
  volume = {28},
  doi = {10.1017/CBO9780511528446.003},
  number = {28},
  booktitle = {The {{Shapley Value}}},
  series = {Annals of {{Math Studies}}},
  date = {1953-01},
  pages = {307-317},
  author = {Shapley, Lloyd}
}

@article{holmgaardUsikkerhedVedOpgorelse2010,
  langid = {danish},
  title = {Usikkerhed ved opgørelse af udviklingen i boligpriser},
  date = {2010},
  pages = {7},
  author = {Holmgaard, Af Jakob},
  file = {/Users/michelsen/Zotero/storage/45LSS4PZ/Holmgaard - 2010 - Usikkerhed ved opgørelse af udviklingen i boligpri.pdf}
}

@online{dstPriceIndexEJ14,
  langid = {english},
  title = {Price {{Index}} ({{EJ14}}) - {{Statistics Denmark}}},
  url = {https://www.dst.dk/en/Statistik/emner/priser-og-forbrug/ejendomme},
  shorttitle = {Statistics {{Denmark}} - {{Real Property}}},
  abstract = {The real property statistics illustrate the development in prices and number of properties sold incl. forced sales.},
  journaltitle = {Statistics Denmark - Real Property ((EJ14)},
  urldate = {2019-11-28},
  keywords = {danmarks statistik,dst},
  author = {DST, Statistics Denmark},
  file = {/Users/michelsen/Zotero/storage/A2HV37YH/ejendomme.html}
}

@article{virtanenSciPyFundamentalAlgorithms2019,
  langid = {english},
  title = {{{SciPy}} 1.0--{{Fundamental Algorithms}} for {{Scientific Computing}} in {{Python}}},
  url = {https://ui.adsabs.harvard.edu/abs/2019arXiv190710121V/abstract},
  abstract = {SciPy is an open source scientific computing library for the Python programming language. SciPy 1.0 was released in late 2017, about 16 years after the original version 0.1 release. SciPy has become a de facto standard for leveraging scientific algorithms in the Python programming language, with more than 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories, and millions of downloads per year. This includes usage of SciPy in almost half of all machine learning projects on GitHub, and usage by high profile projects including LIGO gravitational wave analysis and creation of the first-ever image of a black hole (M87). The library includes functionality spanning clustering, Fourier transforms, integration, interpolation, file I/O, linear algebra, image processing, orthogonal distance regression, minimization algorithms, signal processing, sparse matrix handling, computational geometry, and statistics. In this work, we provide an overview of the capabilities and development practices of the SciPy library and highlight some recent technical developments.},
  journaltitle = {arXiv},
  urldate = {2019-11-28},
  date = {2019-07},
  pages = {arXiv:1907.10121},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, Stéfan J. and Brett, Matthew and Wilson, Joshua and Jarrod Millman, K. and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, Ilhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Antônio H. and Pedregosa, Fabian and van Mulbregt, Paul and Contributors, SciPy 1 0},
  options = {useprefix=true},
  file = {/Users/michelsen/Zotero/storage/W3NFM7EJ/Virtanen et al. - 2019 - SciPy 1.0--Fundamental Algorithms for Scientific C.pdf;/Users/michelsen/Zotero/storage/8ZWMN6MQ/abstract.html}
}

@online{oisWwwOISDk,
  title = {Www.{{OIS}}.Dk - {{Din}} Genvej Til Ejendomsdata},
  url = {https://www.ois.dk/},
  urldate = {2019-12-02},
  author = {, OIS},
  file = {/Users/michelsen/Zotero/storage/LDW5BZSL/www.ois.dk.html}
}

@article{albanesePracticalToolMaximal2018a,
  title = {A Practical Tool for Maximal Information Coefficient Analysis},
  volume = {7},
  issn = {2047-217X},
  url = {https://doi.org/10.1093/gigascience/giy032},
  doi = {10.1093/gigascience/giy032},
  abstract = {The ability of finding complex associations in large omics datasets, assessing their significance, and prioritizing them according to their strength can be of great help in the data exploration phase. Mutual information-based measures of association are particularly promising, in particular after the recent introduction of the TICe and MICe estimators, which combine computational efficiency with superior bias/variance properties. An open-source software implementation of these two measures providing a complete procedure to test their significance would be extremely useful.Here, we present MICtools, a comprehensive and effective pipeline that combines TICe and MICe into a multistep procedure that allows the identification of relationships of various degrees of complexity. MICtools calculates their strength assessing statistical significance using a permutation-based strategy. The performances of the proposed approach are assessed by an extensive investigation in synthetic datasets and an example of a potential application on a metagenomic dataset is also illustrated.We show that MICtools, combining TICe and MICe, is able to highlight associations that would not be captured by conventional strategies.},
  issue = {giy032},
  journaltitle = {GigaScience},
  shortjournal = {GigaScience},
  urldate = {2019-12-03},
  date = {2018-04-02},
  keywords = {MIC},
  author = {Albanese, Davide and Riccadonna, Samantha and Donati, Claudio and Franceschi, Pietro}
}

@inproceedings{harveyEstimationProceduresStructural1990,
  title = {Estimation {{Procedures}} for {{Structural Time Series Models}}},
  doi = {10.1002/for.3980090203},
  abstract = {A univariate structural time series model based on the traditional decomposition into trend, seasonal and irregular components is defined. A number of methods of computing maximum likelihood estimators are then considered. These include direct maximization of various time domain likelihood function. The asymptotic properties of the estimators are given and a comparison between the various methods in terms of computational efficiency and accuracy is made. The methods are then extended to models with explanatory variables. Ktv WORDS Structural time series model Forecasting Kalman filter Stochastic trend Unobserved components model EM algorithm},
  date = {1990},
  keywords = {prophet,Computation,Expectation–maximization algorithm,explanation,Kalman filter,Projections and Predictions,Time series,Facebook},
  author = {Harvey, Andrew Edward and Peters, Stanley}
}

@article{hastieGeneralizedAdditiveModels1987,
  title = {Generalized {{Additive Models}}: {{Some Applications}}},
  volume = {82},
  issn = {01621459},
  url = {www.jstor.org/stable/2289439},
  doi = {10.2307/2289439},
  abstract = {[Generalized additive models have the form η( x) = α + ∑ f\textsubscript{j}(x\textsubscript{j}), where η might be the regression function in a multiple regression or the logistic transformation of the posterior probability {$<$}tex-math{$>\$\backslash$}Pr(y = 1 \textbackslash{}mid \textbackslash{}mathbf x)\${$<$}/tex-math{$>$} in a logistic regression. In fact, these models generalize the whole family of generalized linear models η( x) = β' x, where η( x) = g(μ( x)) is some transformation of the regression function. We use the local scoring algorithm to estimate the functions f\textsubscript{j}(x\textsubscript{j}) nonparametrically, using a scatterplot smoother as a building block. We demonstrate the models in two different analyses: a nonparametric analysis of covariance and a logistic regression. The procedure can be used as a diagnostic tool for identifying parametric transformations of the covariates in a standard linear analysis. A variety of inferential tools have been developed to aid the analyst in assessing the relevance and significance of the estimated functions: these include confidence curves, degrees of freedom estimates, and approximate hypothesis tests. The local scoring algorithm is analogous to the iterative reweighted least squares algorithm for solving likelihood and nonlinear regression equations. At each iteration, an adjusted dependent variable is formed and an additive regression model is fit using the backfitting algorithm. The backfitting algorithm cycles through the variables and estimates each coordinated function by smoothing the partial residuals.]},
  number = {398},
  journaltitle = {Journal of the American Statistical Association},
  date = {1987},
  pages = {371-386},
  keywords = {GAM},
  author = {Hastie, Trevor and Tibshirani, Robert}
}

@software{bednarDatashaderRevealingStructure2019,
  title = {Datashader: {{Revealing}} the {{Structure}} of {{Genuinely Big Data}}},
  url = {https://github.com/holoviz/datashader},
  abstract = {Reveal everything even in your largest datasets, by turning them into images.},
  urldate = {2019-12-03},
  date = {2019-12-03T02:54:58Z},
  keywords = {datashader,holoviz},
  editora = {Bednar, James A. and Crist, Jim and Cottam, Joseph and Wang, Peter},
  editoratype = {collaborator},
  origdate = {2015-12-23T18:02:20Z}
}

@book{huber2011robust,
  title = {Robust Statistics},
  isbn = {978-1-118-21033-8},
  url = {https://books.google.dk/books?id=j1OhquR_j88C},
  abstract = {A new edition of the classic, groundbreaking book on robust  statistics   Over twenty-five years after the publication of its predecessor,  Robust Statistics, Second Edition continues to provide an  authoritative and systematic treatment of the topic. This new  edition has been thoroughly updated and expanded to reflect the  latest advances in the field while also outlining the established  theory and applications for building a solid foundation in robust  statistics for both the theoretical and the applied  statistician.  A comprehensive introduction and discussion on the formal  mathematical background behind qualitative and quantitative  robustness is provided, and subsequent chapters delve into basic  types of scale estimates, asymptotic minimax theory, regression,  robust covariance, and robust design. In addition to an extended  treatment of robust regression, the Second Edition features four  new chapters covering:      Robust Tests      Small Sample Asymptotics      Breakdown Point      Bayesian Robustness      An expanded treatment of robust regression and pseudo-values is  also featured, and concepts, rather than mathematical completeness,  are stressed in every discussion. Selected numerical algorithms for  computing robust estimates and convergence proofs are provided  throughout the book, along with quantitative robustness information  for a variety of estimates. A General Remarks section appears at  the beginning of each chapter and provides readers with ample  motivation for working with the presented methods and  techniques.  Robust Statistics, Second Edition is an ideal book for  graduate-level courses on the topic. It also serves as a valuable  reference for researchers and practitioners who wish to study the  statistical research associated with robust statistics.},
  series = {Wiley Series in Probability and Statistics},
  publisher = {{Wiley}},
  date = {2011},
  author = {Huber, P.J. and Ronchetti, E.M.}
}

@article{leysDetectingOutliersNot2013,
  langid = {english},
  title = {Detecting Outliers: {{Do}} Not Use Standard Deviation around the Mean, Use Absolute Deviation around the Median},
  volume = {49},
  issn = {0022-1031},
  url = {http://www.sciencedirect.com/science/article/pii/S0022103113000668},
  doi = {10.1016/j.jesp.2013.03.013},
  shorttitle = {Detecting Outliers},
  abstract = {A survey revealed that researchers still seem to encounter difficulties to cope with outliers. Detecting outliers by determining an interval spanning over the mean plus/minus three standard deviations remains a common practice. However, since both the mean and the standard deviation are particularly sensitive to outliers, this method is problematic. We highlight the disadvantages of this method and present the median absolute deviation, an alternative and more robust measure of dispersion that is easy to implement. We also explain the procedures for calculating this indicator in SPSS and R software.},
  number = {4},
  journaltitle = {Journal of Experimental Social Psychology},
  shortjournal = {Journal of Experimental Social Psychology},
  urldate = {2019-12-05},
  date = {2013-07-01},
  pages = {764-766},
  keywords = {MAD,Median absolute deviation,Outlier},
  author = {Leys, Christophe and Ley, Christophe and Klein, Olivier and Bernard, Philippe and Licata, Laurent},
  file = {/Users/michelsen/Zotero/storage/BXE8IQ9Y/Leys et al. - 2013 - Detecting outliers Do not use standard deviation .pdf;/Users/michelsen/Zotero/storage/CKSDGTFA/S0022103113000668.html}
}

@article{rousseeuwAlternativesMedianAbsolute1993,
  title = {Alternatives to the {{Median Absolute Deviation}}},
  volume = {88},
  issn = {0162-1459},
  url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476408},
  doi = {10.1080/01621459.1993.10476408},
  abstract = {In robust estimation one frequently needs an initial or auxiliary estimate of scale. For this one usually takes the median absolute deviation MAD n = 1.4826 med, |xi − med j x j |, because it has a simple explicit formula, needs little computation time, and is very robust as witnessed by its bounded influence function and its 50\% breakdown point. But there is still room for improvement in two areas: the fact that MAD n is aimed at symmetric distributions and its low (37\%) Gaussian efficiency. In this article we set out to construct explicit and 50\% breakdown scale estimators that are more efficient. We consider the estimator Sn = 1.1926 med, med j | xi − xj | and the estimator Qn given by the .25 quantile of the distances |xi − x j |; i {$<$} j. Note that Sn and Qn do not need any location estimate. Both Sn and Qn can be computed using O(n log n) time and O(n) storage. The Gaussian efficiency of Sn is 58\%, whereas Qn attains 82\%. We study Sn and Qn by means of their influence functions, their bias curves (for implosion as well as explosion), and their finite-sample performance. Their behavior is also compared at non-Gaussian models, including the negative exponential model where Sn has a lower gross-error sensitivity than the MAD.},
  number = {424},
  journaltitle = {Journal of the American Statistical Association},
  urldate = {2019-12-05},
  date = {1993-12-01},
  pages = {1273-1283},
  keywords = {MAD,Bias curve,Breakdown point,Influence function,Robustness,Scale estimation},
  author = {Rousseeuw, Peter J. and Croux, Christophe},
  file = {/Users/michelsen/Zotero/storage/XC44VIQR/01621459.1993.html}
}

@book{Barlow:0471922951,
  title = {Statistics: {{A}} Guide to the Use of Statistical Methods in the Physical Sciences (Manchester Physics Series)},
  edition = {Reprint},
  isbn = {0-471-92295-1},
  url = {http://www.amazon.co.uk/Statistics-Statistical-Physical-Sciences-Manchester/dp/0471922951%3FSubscriptionId%3D13CT5CVB80YFWJEPWS02%26tag%3Dws%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D0471922951},
  publisher = {{WileyBlackwell}},
  date = {1989},
  keywords = {imported textbook},
  author = {Barlow, R. J.},
  added-at = {2009-01-21T15:25:53.000+0100},
  asin = {0471922951},
  biburl = {https://www.bibsonomy.org/bibtex/2a3a53b359a17963dc26d760338cf6da9/clange},
  description = {Statistics: A Guide to the Use of Statistical Methods in the Physical Sciences Manchester Physics Series: Amazon.co.uk: R. J. Barlow: Books},
  dewey = {530.1595},
  ean = {9780471922957},
  interhash = {58cb0c0714f3bedbce6d985bba5f01ba},
  intrahash = {a3a53b359a17963dc26d760338cf6da9},
  timestamp = {2009-01-21T15:25:53.000+0100}
}

@online{www.bolighed.dkBolighedUsikkerhedDatavurderingen,
  langid = {danish},
  title = {Bolighed - Usikkerhed i data-vurderingen},
  url = {https://bolighed.dk/om-bolighed/spoergsmaal-og-svar/#boligvaerdi},
  abstract = {Statistisk rammer data-vurderingen inden for 5\% af handelsprisen på næsten halvdelen af alle almindelige handler. Og inden for 20\% på op mod 9 ud af 10 handler. I de fleste tilfælde giver den altså et godt bud på boligens værdi.},
  journaltitle = {Bolighed},
  urldate = {2019-12-09},
  keywords = {Bolighed},
  author = {, www.bolighed.dk},
  file = {/Users/michelsen/Zotero/storage/LIA4XWL7/spoergsmaal-og-svar.html}
}


