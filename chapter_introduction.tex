
\chapter{Introduction}
\label{ch:introduction}
\epigraph{\textit{``Begin at the beginning,'' the King said, gravely, ``and go on till you
come to an end; then stop.''}}{--- Lewis Carroll, \textit{Alice in Wonderland}}


\newthought{Not only is the title} of this master's thesis fairly broad, so are the subjects covered. The overall goal of this project is to apply machine learning to different datasets and see how well these comparatively new tools might improve classical statistical methods. The project have dealt with two (seemingly) very different datasets: Danish housing prices and quark gluon discrimination in particle physics. The aim of this section is to provide an initial overview of the scope and relationship of the two subprojects.

\hyperref[part1]{Part I} of the thesis deals with the problem of estimating housing prices as precisely and accurately as possible. As the first of the two projects, this worked as an initial introduction to the application of machine learning to real life datasets. The housing prices dataset thus became the playground in which the subtleties of these new modern tools were examined. In this project the difference between real life datasets with all its quirks, outliers and bad formatting, and curated toy datasets that works out of the box (such as the famous Iris dataset \citep{andersonSpeciesProblemIris1936a,fisherUseMultipleMeasurements1936}) were experienced first hand. 

Since the project started, the dataset changed due to a new collaboration with the Danish housing agency \href{www.boligsiden.dk}{Boligsiden}. Boligsiden is a natural collaborator since they are the biggest on the market\sidenote{Boligsiden is owned by the The Danish Association of Chartered Estate Agents, Dansk Ejendomsm√¶glerforening.} and have been very helpful in the continuos process of providing data. It should also be noted that they have had no say on the results presented in this thesis. 

During this initial stage, the author sparred with Simon Gudiksen\sidenote{Who afterwards went on to get a job at Boligsiden.} who also worked on the same dataset, however, both projects were done independently. Where Gudiksen focussed on the prediction of the time evolution of the housing prices using Recurrent Neural Networks (RNN), my work was mostly on the different levels and methods of hyperparameter optimization with some smaller detours into Natural Language Processing (NLP) as an example. 

\clearpage


\hyperref[part2]{Part II} contains the main part of the thesis: quark gluon discrimination in particle physics. Not only was most of the time focussed on this project, it was also the work that generated the highest academic output; an article based on this is in the making. This part dealt with data from the Large Electron Positron collider (LEP) which was an underground particle accelerator at CERN. It was built in \num{1989} and was in use until it was discontinued in \num{2000}. The first phase, LEP1, from \num{1989}-\num{1995}, is the sole source of data in this project. As the name suggests, it collided electrons and positrons in what is still the largest electron-positron accelerator ever built \citep{LargeElectronPositronCollider}. 
During LEP1 it was primarily the decay channels of the $Z$-boson that were probed where especially the $Z\rightarrow q\bar{q}g$ and $Z \rightarrow q\bar{q}gg$ are examined in this thesis. Here $q$ refers to quarks, $\bar{q}$ to antiquarks, and $g$ to gluons. The former of the two decay modes is a so-called 3-jet event and the latter a 4-jet event. An event consists thus of a different number of jets depending on the physical interaction between the particles in that event. 

The distributions of the gluon jets, and the difference between Data\sidenote{Where \q{Data} with capital D refers to the actual, measured data and \q{data} refers to any arbitrary selection of data.} and Monte-Carlo (MC), are of interests to the theoreticians that develop the MC-models. At first an improved $b$-tagging algorithm was developed using both methods and code from the first part of the thesis. In addition to the jet-based $b$-tagging model, an event-based $g$-tagging model was implemented. The $g$-tagging model allows one to extract events of interest, and the gluon jets can be identified using the $b$-tagging model. 

The thesis is structured such that \autoref{ch:ML_theory} introduces the theoretical machine learning background needed for understanding the methods used throughout the thesis. This theory is applied in the analysis of the housing prices in \autoref{ch:housing_price_analysis}. These two chapters constitutes the \hyperref[part1]{Part I} of this thesis. \hyperref[part2]{Part II} contains the last two chapters which deals with particle physics. In \autoref{ch:hep:particle_physcis_LEP}, the basic physics of the standard model and the Lund string model is introduced along with an introduction to the ALEPH detector. During \autoref{ch:quark_gluon_analysis}, this theory is used in the analysis of the quark gluon discrimination. This latter analysis constitutes the main part of this thesis. 

The work presented in this thesis is split up into two parts, however, they should not be treated as two independent projects but rather as two instances of same underlying problem: teaching computers how to find patterns, automatically, in high-dimensional data. That it is the machine learning methodology which connects the two parts also highlights another key aspect of this project: that the author does not have any background in particle physics other than rudimentary knowledge stemming from an undergraduate education in general physics. 

All of the work presented in this thesis is performed by the author unless otherwise noted.




% Part \RNum{2} of this thesis deals with particle physics and the discriminatory power of machine learning for quark-gluon identification and subsequent analysis. In \autoref{ch:hep:particle_physcis_LEP} the theory of the Standard Model is introduced together with a description of the ALEPH detector. Theory is applied in \autoref{ch:quark_gluon_analysis} where the types of jets and events in each collision is analysed using machine learning to improve the understanding of how gluon jets hadronizes and splits: simply said how they look and behave. 