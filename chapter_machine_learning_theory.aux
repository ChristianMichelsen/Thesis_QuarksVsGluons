\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand{\transparent@use}[1]{}
\citation{veenNeuralNetworkZoo2016}
\citation{abu-mostafaLearningData2012}
\citation{AdvancedTopicsMachine}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Machine Learning Theory}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:ML_theory}{{2}{5}{Machine Learning Theory}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Statistical Learning Theory}{5}{section.2.1}}
\citation{vapnikPrinciplesRiskMinimization1991}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Supervised Learning}{6}{section.2.2}}
\newlabel{sec:ml:supervised_learning}{{2.2}{6}{Supervised Learning}{section.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The learning problem.}}{6}{section.2.2}}
\newlabel{fig:ml:learning_problem}{{2.1}{6}{Supervised Learning}{section.2.2}{}}
\newlabel{eq:L}{{2.1}{6}{Supervised Learning}{equation.2.2.1}{}}
\newlabel{eq:L_hat}{{2.2}{6}{Supervised Learning}{equation.2.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Generalization Bound}{7}{section.2.3}}
\newlabel{sec:generalization_bound}{{2.3}{7}{Generalization Bound}{section.2.3}{}}
\newlabel{eq:hoeffding_onesided_a}{{2.6}{7}{The one-sided Hoeffding's inequalities}{equation.2.3.6}{}}
\newlabel{eq:hoeffding_onesided_b}{{2.7}{7}{The one-sided Hoeffding's inequalities}{equation.2.3.7}{}}
\newlabel{lemma:hoeffding}{{3}{7}{The two-sided Hoeffding's inequality}{lemma.3}{}}
\newlabel{eq:hoeffding_inequality}{{2.8}{7}{The two-sided Hoeffding's inequality}{equation.2.3.8}{}}
\newlabel{eq:hoeffding_inequality_generalization_error}{{2.9}{8}{Generalization Bound}{equation.2.3.9}{}}
\newlabel{theorem:hoeffding_single}{{1}{8}{Hoeffding's inequality for a single hypothesis}{theorem.1}{}}
\newlabel{eq:hoeffding_inequality_generalization_error_delta}{{2.11}{8}{Hoeffding's inequality for a single hypothesis}{equation.2.3.11}{}}
\newlabel{eq:hoeffding_inequality_single_PAC}{{2.12}{8}{Generalization Bound}{equation.2.3.12}{}}
\newlabel{theorem:hoeffding_finite}{{2}{8}{Hoeffding's inequality for a finite set of hypotheses candidates}{theorem.2}{}}
\newlabel{eq:hoeffding_inequality_theorem_multiple}{{2.13}{8}{Hoeffding's inequality for a finite set of hypotheses candidates}{equation.2.3.13}{}}
\citation{abu-mostafaLearningData2012}
\newlabel{eq:hoeffding_inequality_multi_PAC}{{2.14}{9}{Generalization Bound}{equation.2.3.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Generalization Bound for infinite hypotheses}{9}{subsection.2.3.1}}
\newlabel{subsec:generalization_bound_infinite}{{2.3.1}{9}{Generalization Bound for infinite hypotheses}{subsection.2.3.1}{}}
\citation{tikhonovStabilityInverseProblems1943}
\newlabel{theorem:VC_generalization_bound}{{3}{10}{VC Generalization Bound}{theorem.3}{}}
\newlabel{eq:VC_bound}{{2.15}{10}{VC Generalization Bound}{equation.2.3.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Approximation-Estimation Tradeoff}}{10}{equation.2.3.16}}
\newlabel{fig:ml:empirical_risk}{{2.2}{10}{Generalization Bound for infinite hypotheses}{equation.2.3.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Avoiding overfitting}{10}{section.2.4}}
\newlabel{sec:ml:overfitting}{{2.4}{10}{Avoiding overfitting}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Model Regularization}{10}{subsection.2.4.1}}
\newlabel{subsec:regularization}{{2.4.1}{10}{Model Regularization}{subsection.2.4.1}{}}
\citation{hastieElementsStatisticalLearning2009}
\newlabel{eq:l2_norm}{{2.20}{11}{Model Regularization}{equation.2.4.20}{}}
\newlabel{eq:l2_norm_linear}{{2.21}{11}{Model Regularization}{equation.2.4.21}{}}
\newlabel{eq:l2_norm_non_lagrangian}{{2.22}{11}{Model Regularization}{equation.2.4.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Regularization Strength}}{11}{equation.2.4.22}}
\newlabel{fig:ml:regularization_ridge}{{2.3}{11}{Model Regularization}{equation.2.4.22}{}}
\citation{tibshiraniRegressionShrinkageSelection1996}
\citation{hastieElementsStatisticalLearning2009}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Regularization Effect of $L_2$ }}{12}{equation.2.4.22}}
\newlabel{fig:ml:regularization_effect_ridge}{{2.4}{12}{Model Regularization}{equation.2.4.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Regularization Effect of $L_1$}}{12}{equation.2.4.23}}
\newlabel{fig:ml:regularization_effect_lasso}{{2.5}{12}{Model Regularization}{equation.2.4.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Cross Validation}{12}{subsection.2.4.2}}
\newlabel{subsec:cross_validation}{{2.4.2}{12}{Cross Validation}{subsection.2.4.2}{}}
\citation{hastieElementsStatisticalLearning2009}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces $k$-Fold Cross Validation}}{13}{subsection.2.4.2}}
\newlabel{fig:ml:cross_val_kfold}{{2.6}{13}{Cross Validation}{subsection.2.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces $k$-Fold Cross Validation for Time Series Data}}{13}{subsection.2.4.2}}
\newlabel{fig:ml:cross_val_kfold_time}{{2.7}{13}{Cross Validation}{subsection.2.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Early Stopping}{14}{subsection.2.4.3}}
\newlabel{subsec:early_stopping}{{2.4.3}{14}{Early Stopping}{subsection.2.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Loss functions}{14}{section.2.5}}
\newlabel{sec:ml:loss_function}{{2.5}{14}{Loss functions}{section.2.5}{}}
\citation{barronGeneralAdaptiveRobust2017}
\citation{barronGeneralAdaptiveRobust2017}
\citation{barronGeneralAdaptiveRobust2017}
\citation{AllstateClaimsSeverity}
\newlabel{eq:ml:loss_functions}{{2.27}{15}{Loss functions}{equation.2.5.27}{}}
\oddpage@label{1}{15}
\citation{hastieElementsStatisticalLearning2009}
\gincltex@bb{figures/decision_tree/decision_tree.tex}{0}{0}{167.85095}{97.0583}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Objective Functions.}}{16}{equation.2.5.27}}
\newlabel{fig:ml:objective_funcs}{{2.8}{16}{Loss functions}{equation.2.5.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Objective Functions Zoom In.}}{16}{equation.2.5.27}}
\newlabel{fig:ml:objective_funcs_zoom}{{2.9}{16}{Loss functions}{equation.2.5.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Evaluation Function}{16}{subsection.2.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Decision Trees}{16}{section.2.6}}
\newlabel{sec:ml:decision_trees}{{2.6}{16}{Decision Trees}{section.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Decision Tree Cuts In Feature Space}}{16}{section.2.6}}
\newlabel{fig:ml:decision_tree_feature_space}{{2.10}{16}{Decision Trees}{section.2.6}{}}
\oddpage@label{2}{16}
\citation{hastieElementsStatisticalLearning2009}
\citation{breimanRandomForests2001}
\citation{hastieElementsStatisticalLearning2009}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Decision Tree}}{17}{section.2.6}}
\newlabel{fig:ml:decision_tree}{{2.11}{17}{Decision Trees}{section.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Ensembles of Decision Trees}{17}{subsection.2.6.1}}
\newlabel{subsec:ml:multiple_decision_trees}{{2.6.1}{17}{Ensembles of Decision Trees}{subsection.2.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Random Forests}{17}{subsubsection*.6}}
\newlabel{subsubsec:ml:random_forest}{{2.6.1}{17}{Random Forests}{subsubsection*.6}{}}
\citation{hastieElementsStatisticalLearning2009}
\citation{JSSv059i10}
\citation{chenXGBoostScalableTree2016}
\citation{DmlcXgboost}
\citation{HEPMeetsML}
\citation{hastieElementsStatisticalLearning2009}
\newlabel{eq:ml:variance_of_correlated_variables}{{2.30}{18}{Variance of average of correlated i.d. variables}{equation.2.6.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{Boosted Decision Trees}{18}{subsubsection*.7}}
\newlabel{subsubsec:ml:boosted_decision_trees}{{2.6.1}{18}{Boosted Decision Trees}{subsubsection*.7}{}}
\citation{freundDesiciontheoreticGeneralizationOnline1995}
\citation{chenXGBoostScalableTree2016}
\newlabel{eq:ml:boosted_decision_trees_residual}{{2.32}{19}{Boosted Decision Trees}{equation.2.6.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Hyperparamater Optimization}{19}{section.2.7}}
\newlabel{sec:ml:hyperparameter_optimization}{{2.7}{19}{Hyperparamater Optimization}{section.2.7}{}}
\citation{bergstraRandomSearchHyperparameter2012}
\citation{bergstraRandomSearchHyperparameter2012}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}Grid Search}{20}{subsection.2.7.1}}
\newlabel{subsec:ml:grid_search}{{2.7.1}{20}{Grid Search}{subsection.2.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Grid Search}}{20}{subsection.2.7.1}}
\newlabel{fig:ml:gridsearch}{{2.12}{20}{Grid Search}{subsection.2.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}Random Search}{20}{subsection.2.7.2}}
\newlabel{subsec:ml:random_search}{{2.7.2}{20}{Random Search}{subsection.2.7.2}{}}
\newlabel{eq:ml:random_search}{{2.37}{20}{Random Search}{equation.2.7.37}{}}
\citation{bergstraRandomSearchHyperparameter2012}
\citation{bergstraRandomSearchHyperparameter2012}
\citation{bergstraRandomSearchHyperparameter2012}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Random Search}}{21}{equation.2.7.38}}
\newlabel{fig:ml:random_search}{{2.13}{21}{Random Search}{equation.2.7.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.3}Bayesian Optimization}{21}{subsection.2.7.3}}
\newlabel{subsec:ml:bayesian_optimization}{{2.7.3}{21}{Bayesian Optimization}{subsection.2.7.3}{}}
\citation{brochuTutorialBayesianOptimization2010}
\citation{brochuTutorialBayesianOptimization2010}
\citation{brochuTutorialBayesianOptimization2010}
\citation{scikit-learn}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Bayesian Optimization}}{22}{subsection.2.7.3}}
\newlabel{fig:ml:bayesian_optimization}{{2.14}{22}{Bayesian Optimization}{subsection.2.7.3}{}}
\citation{Lundberg:2017}
\citation{Lundberg:2017}
\citation{lundbergConsistentIndividualizedFeature2019}
\citation{Shapley1953}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Feature Importance}{23}{section.2.8}}
\newlabel{sec:ml:feature_importance}{{2.8}{23}{Feature Importance}{section.2.8}{}}
\newlabel{eq:ml:additive_feature_attribution_method}{{2.40}{23}{Feature Importance}{equation.2.8.40}{}}
\citation{Lundberg:2017}
\citation{lundbergConsistentIndividualizedFeature2019}
\newlabel{axiom:ml:shap_consistency}{{3}{24}{Consistency}{axiom.3}{}}
\newlabel{eq:ml:shap_feature_importance}{{2.43}{24}{Feature Importance}{equation.2.8.43}{}}
\newlabel{eq:ml:shap_feature_importance_simplification}{{2.44}{24}{Feature Importance}{equation.2.8.44}{}}
\citation{lundbergConsistentIndividualizedFeature2019}
\citation{Lundberg:2017}
\newlabel{axiom:ml:shapley_symmetry}{{4}{25}{Symmetry}{axiom.4}{}}
\@setckpt{chapter_machine_learning_theory}{
\setcounter{page}{26}
\setcounter{equation}{47}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{42}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{8}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{14}
\setcounter{table}{0}
\setcounter{Item}{0}
\setcounter{bookmark@seq@number}{13}
\setcounter{cp@cntr}{0}
\setcounter{@tufte@num@bibkeys}{0}
\setcounter{NAT@ctr}{0}
\setcounter{lips@count}{0}
\setcounter{parentequation}{0}
\setcounter{currfiledepth}{0}
\setcounter{KVtest}{2}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{FancyVerbLine}{0}
\setcounter{lemma}{3}
\setcounter{sublemma}{0}
\setcounter{corollary}{0}
\setcounter{theorem}{4}
\setcounter{axiom}{4}
\setcounter{AM@survey}{0}
\setcounter{section@level}{0}
}
