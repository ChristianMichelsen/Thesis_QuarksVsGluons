\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand{\transparent@use}[1]{}
\citation{veenNeuralNetworkZoo2016}
\citation{abu-mostafaLearningData2012}
\citation{AdvancedTopicsMachine}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Machine Learning Theory}{7}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:ML_theory}{{3}{7}{Machine Learning Theory}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Statistical Learning Theory}{7}{section.3.1}}
\citation{vapnikPrinciplesRiskMinimization1991}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Supervised Learning}{8}{section.3.2}}
\newlabel{sec:ml:supervised_learning}{{3.2}{8}{Supervised Learning}{section.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Overview of the learning problem.}}{8}{section.3.2}}
\newlabel{fig:ml:learning_problem}{{3.1}{8}{Supervised Learning}{section.3.2}{}}
\newlabel{eq:L}{{3.1}{8}{Supervised Learning}{equation.3.2.1}{}}
\newlabel{eq:L_hat}{{3.2}{8}{Supervised Learning}{equation.3.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Generalization Bound}{9}{section.3.3}}
\newlabel{sec:generalization_bound}{{3.3}{9}{Generalization Bound}{section.3.3}{}}
\newlabel{eq:hoeffding_onesided_a}{{3.6}{9}{The one-sided Hoeffding's inequalities}{equation.3.3.6}{}}
\newlabel{eq:hoeffding_onesided_b}{{3.7}{9}{The one-sided Hoeffding's inequalities}{equation.3.3.7}{}}
\newlabel{lemma:hoeffding}{{3}{9}{The two-sided Hoeffding's inequality}{lemma.3}{}}
\newlabel{eq:hoeffding_inequality}{{3.8}{9}{The two-sided Hoeffding's inequality}{equation.3.3.8}{}}
\newlabel{eq:hoeffding_inequality_generalization_error}{{3.9}{10}{Generalization Bound}{equation.3.3.9}{}}
\newlabel{theorem:hoeffding_single}{{1}{10}{Hoeffding's inequality for a single hypothesis}{theorem.1}{}}
\newlabel{eq:hoeffding_inequality_generalization_error_delta}{{3.11}{10}{Hoeffding's inequality for a single hypothesis}{equation.3.3.11}{}}
\newlabel{eq:hoeffding_inequality_single_PAC}{{3.12}{10}{Generalization Bound}{equation.3.3.12}{}}
\newlabel{theorem:hoeffding_finite}{{2}{10}{Hoeffding's inequality for a finite set of hypotheses candidates}{theorem.2}{}}
\newlabel{eq:hoeffding_inequality_theorem_multiple}{{3.13}{10}{Hoeffding's inequality for a finite set of hypotheses candidates}{equation.3.3.13}{}}
\citation{abu-mostafaLearningData2012}
\newlabel{eq:hoeffding_inequality_multi_PAC}{{3.14}{11}{Generalization Bound}{equation.3.3.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Generalization Bound for infinite hypotheses}{11}{subsection.3.3.1}}
\newlabel{subsec:generalization_bound_infinite}{{3.3.1}{11}{Generalization Bound for infinite hypotheses}{subsection.3.3.1}{}}
\citation{tikhonovStabilityInverseProblems1943}
\newlabel{theorem:VC_generalization_bound}{{3}{12}{VC Generalization Bound}{theorem.3}{}}
\newlabel{eq:VC_bound}{{3.15}{12}{VC Generalization Bound}{equation.3.3.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Approximation-Estimation tradeoff}}{12}{equation.3.3.16}}
\newlabel{fig:ml:empirical_risk}{{3.2}{12}{Generalization Bound for infinite hypotheses}{equation.3.3.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Avoiding overfitting}{12}{section.3.4}}
\newlabel{sec:ml:overfitting}{{3.4}{12}{Avoiding overfitting}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Model Regularization}{12}{subsection.3.4.1}}
\newlabel{subsec:regularization}{{3.4.1}{12}{Model Regularization}{subsection.3.4.1}{}}
\citation{hastieElementsStatisticalLearning2009}
\newlabel{eq:l2_norm}{{3.20}{13}{Model Regularization}{equation.3.4.20}{}}
\newlabel{eq:l2_norm_linear}{{3.21}{13}{Model Regularization}{equation.3.4.21}{}}
\newlabel{eq:l2_norm_non_lagrangian}{{3.22}{13}{Model Regularization}{equation.3.4.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Regularization Effect}}{13}{equation.3.4.22}}
\newlabel{fig:ml:regularization_ridge}{{3.3}{13}{Model Regularization}{equation.3.4.22}{}}
\citation{tibshiraniRegressionShrinkageSelection1996}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Regularization Effect of $L_2$ }}{14}{equation.3.4.22}}
\newlabel{fig:ml:regularization_effect_ridge}{{3.4}{14}{Model Regularization}{equation.3.4.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Regularization Effect of $L_1$}}{14}{equation.3.4.22}}
\newlabel{fig:ml:regularization_effect_lasso}{{3.5}{14}{Model Regularization}{equation.3.4.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Cross Validation}{14}{subsection.3.4.2}}
\newlabel{subsec:cross_validation}{{3.4.2}{14}{Cross Validation}{subsection.3.4.2}{}}
\citation{hastieElementsStatisticalLearning2009}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces $k$-Fold Cross Validation}}{15}{subsection.3.4.2}}
\newlabel{fig:ml:cross_val_kfold}{{3.6}{15}{Cross Validation}{subsection.3.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces $k$-Fold Cross Validation for Time Series Data}}{15}{subsection.3.4.2}}
\newlabel{fig:ml:cross_val_kfold_time}{{3.7}{15}{Cross Validation}{subsection.3.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Early Stopping}{16}{subsection.3.4.3}}
\newlabel{subsec:early_stopping}{{3.4.3}{16}{Early Stopping}{subsection.3.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Loss functions}{16}{section.3.5}}
\newlabel{sec:ml:loss_function}{{3.5}{16}{Loss functions}{section.3.5}{}}
\citation{barronGeneralAdaptiveRobust2017}
\citation{barronGeneralAdaptiveRobust2017}
\citation{barronGeneralAdaptiveRobust2017}
\citation{AllstateClaimsSeverity}
\oddpage@label{1}{17}
\citation{hastieElementsStatisticalLearning2009}
\gincltex@bb{figures/decision_tree/decision_tree.tex}{0}{0}{167.85095}{97.0583}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Comparison of different objective functions.}}{18}{equation.3.5.27}}
\newlabel{fig:ml:objective_funcs}{{3.8}{18}{Loss functions}{equation.3.5.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Comparison of different objective functions zoom in.}}{18}{equation.3.5.27}}
\newlabel{fig:ml:objective_funcs_zoom}{{3.9}{18}{Loss functions}{equation.3.5.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Evaluation Function}{18}{subsection.3.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Decision Trees}{18}{section.3.6}}
\newlabel{sec:ml:decision_trees}{{3.6}{18}{Decision Trees}{section.3.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Decision Tree Cuts In Feature Space}}{18}{section.3.6}}
\newlabel{fig:ml:decision_tree_feature_space}{{3.10}{18}{Decision Trees}{section.3.6}{}}
\oddpage@label{2}{18}
\citation{hastieElementsStatisticalLearning2009}
\citation{breimanRandomForests2001}
\citation{hastieElementsStatisticalLearning2009}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Decision Tree}}{19}{section.3.6}}
\newlabel{fig:ml:decision_tree}{{3.11}{19}{Decision Trees}{section.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Ensembles of Decision Trees}{19}{subsection.3.6.1}}
\newlabel{subsec:ml:multiple_decision_trees}{{3.6.1}{19}{Ensembles of Decision Trees}{subsection.3.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Random Forests}{19}{subsubsection*.5}}
\newlabel{subsubsec:ml:random_forest}{{3.6.1}{19}{Random Forests}{subsubsection*.5}{}}
\citation{hastieElementsStatisticalLearning2009}
\citation{JSSv059i10}
\citation{chenXGBoostScalableTree2016}
\citation{DmlcXgboost}
\citation{HEPMeetsML}
\citation{hastieElementsStatisticalLearning2009}
\newlabel{eq:ml:variance_of_correlated_variables}{{3.30}{20}{Variance of average of correlated i.d. variables}{equation.3.6.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{Boosted Decision Trees}{20}{subsubsection*.6}}
\newlabel{subsubsec:ml:boosted_decision_trees}{{3.6.1}{20}{Boosted Decision Trees}{subsubsection*.6}{}}
\citation{freundDesiciontheoreticGeneralizationOnline1995}
\citation{chenXGBoostScalableTree2016}
\newlabel{eq:ml:boosted_decision_trees_residual}{{3.32}{21}{Boosted Decision Trees}{equation.3.6.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Hyperparamater Optimization}{21}{section.3.7}}
\newlabel{sec:ml:hyperparameter_optimization}{{3.7}{21}{Hyperparamater Optimization}{section.3.7}{}}
\citation{bergstraRandomSearchHyperparameter2012}
\citation{bergstraRandomSearchHyperparameter2012}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Grid Search}{22}{subsection.3.7.1}}
\newlabel{subsec:ml:grid_search}{{3.7.1}{22}{Grid Search}{subsection.3.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Grid Search}}{22}{subsection.3.7.1}}
\newlabel{fig:ml:gridsearch}{{3.12}{22}{Grid Search}{subsection.3.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Random Search}{22}{subsection.3.7.2}}
\newlabel{subsec:ml:random_search}{{3.7.2}{22}{Random Search}{subsection.3.7.2}{}}
\newlabel{eq:ml:random_search}{{3.37}{22}{Random Search}{equation.3.7.37}{}}
\citation{bergstraRandomSearchHyperparameter2012}
\citation{bergstraRandomSearchHyperparameter2012}
\citation{bergstraRandomSearchHyperparameter2012}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Random Search}}{23}{equation.3.7.38}}
\newlabel{fig:ml:random_search}{{3.13}{23}{Random Search}{equation.3.7.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.3}Bayesian Optimization}{23}{subsection.3.7.3}}
\newlabel{subsec:ml:bayesian_optimization}{{3.7.3}{23}{Bayesian Optimization}{subsection.3.7.3}{}}
\citation{brochuTutorialBayesianOptimization2010}
\citation{brochuTutorialBayesianOptimization2010}
\citation{brochuTutorialBayesianOptimization2010}
\citation{scikit-learn}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Bayesian Optimization}}{24}{subsection.3.7.3}}
\newlabel{fig:ml:bayesian_optimization}{{3.14}{24}{Bayesian Optimization}{subsection.3.7.3}{}}
\citation{Lundberg:2017}
\citation{Lundberg:2017}
\citation{lundbergConsistentIndividualizedFeature2019}
\citation{Shapley1953}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Feature Importance}{25}{section.3.8}}
\newlabel{sec:ml:feature_importance}{{3.8}{25}{Feature Importance}{section.3.8}{}}
\newlabel{eq:ml:additive_feature_attribution_method}{{3.40}{25}{Feature Importance}{equation.3.8.40}{}}
\citation{Lundberg:2017}
\citation{lundbergConsistentIndividualizedFeature2019}
\newlabel{axiom:ml:shap_consistency}{{3}{26}{Consistency}{axiom.3}{}}
\newlabel{eq:ml:shap_feature_importance}{{3.43}{26}{Feature Importance}{equation.3.8.43}{}}
\newlabel{eq:ml:shap_feature_importance_simplification}{{3.44}{26}{Feature Importance}{equation.3.8.44}{}}
\citation{lundbergConsistentIndividualizedFeature2019}
\citation{Lundberg:2017}
\newlabel{axiom:ml:shapley_symmetry}{{4}{27}{Symmetry}{axiom.4}{}}
\@setckpt{chapter_machine_learning_theory}{
\setcounter{page}{28}
\setcounter{equation}{47}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{43}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{8}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{14}
\setcounter{table}{0}
\setcounter{Item}{0}
\setcounter{bookmark@seq@number}{11}
\setcounter{cp@cntr}{0}
\setcounter{@tufte@num@bibkeys}{0}
\setcounter{NAT@ctr}{0}
\setcounter{lips@count}{0}
\setcounter{parentequation}{0}
\setcounter{currfiledepth}{0}
\setcounter{KVtest}{2}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{FancyVerbLine}{0}
\setcounter{lemma}{3}
\setcounter{sublemma}{0}
\setcounter{corollary}{0}
\setcounter{theorem}{4}
\setcounter{axiom}{4}
\setcounter{AM@survey}{0}
\setcounter{section@level}{0}
}
